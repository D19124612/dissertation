{"cells":[{"cell_type":"code","source":["import os \n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"pXLAvBclVgn_","executionInfo":{"status":"ok","timestamp":1644778239922,"user_tz":0,"elapsed":7,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"F-o6bXOJ18j4","executionInfo":{"status":"ok","timestamp":1644778249158,"user_tz":0,"elapsed":9241,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["\n","!pip install -qq transformers\n","!pip install -qq optuna\n","!pip install -qq datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Rz6wNlu92ge_","executionInfo":{"status":"ok","timestamp":1644778252853,"user_tz":0,"elapsed":3701,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["import transformers\n","import datasets\n","from transformers import AutoTokenizer,AutoModelForQuestionAnswering, AutoModelForSequenceClassification,AdamW, get_linear_schedule_with_warmup,Trainer, TrainingArguments\n","from transformers import DataCollator, DataCollatorForLanguageModeling,default_data_collator\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","import random\n","from textwrap import wrap\n","from datetime import datetime\n","from datasets import load_from_disk\n","from datasets import load_dataset\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","from torch import nn"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"T7IFr4-3TKaA","executionInfo":{"status":"ok","timestamp":1644778252853,"user_tz":0,"elapsed":9,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["# the model we gonna train, base uncased BERT\n","# check text classification models here: https://huggingface.co/models?filter=text-classification\n","MODEL_NAME = \"distilbert-base-uncased\"\n","# max sequence length for each document/sentence sample\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE= 6.58e-5\n","WEIGHT_DECAY = 0.289\n","WARMUP_STEPS = 464\n","RANDOM_SEED=22\n","\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"YoKXcvyo_X47","executionInfo":{"status":"ok","timestamp":1644778252854,"user_tz":0,"elapsed":9,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n","       When seed is None, disables deterministic mode.\n","    :param seed: an integer to your choosing\n","    \"\"\"\n","    if seed is not None:\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        np.random.seed(seed)\n","        random.seed(seed)\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","  acc = accuracy_score(labels, preds)\n","  confusion_matrix = classification_report(labels, preds, digits=4,output_dict=True)\n","  return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'hate_f1': confusion_matrix[\"0\"][\"f1-score\"],\n","        'hate_recall': confusion_matrix[\"0\"][\"recall\"],\n","        'hate_precision': confusion_matrix[\"0\"][\"precision\"],\n","        'offensive_f1': confusion_matrix[\"1\"][\"f1-score\"],\n","        'offensive_recall': confusion_matrix[\"1\"][\"recall\"],\n","        'offensive_precision': confusion_matrix[\"1\"][\"precision\"],\n","        'normal_f1': confusion_matrix[\"2\"][\"f1-score\"],\n","        'normal_recall': confusion_matrix[\"2\"][\"recall\"],\n","        'normal_precision': confusion_matrix[\"2\"][\"precision\"],    \n","  }\n","\n","\n","\n","\n","def model_init_two_layer_reinit():\n","  temp_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=3).to(device)\n","  temp_model = reinit_autoencoder_model(temp_model,2)\n","  return temp_model\n","\n","\n","\n","def timestamp():\n","    dateTimeObj = datetime.now()\n","    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n","    print(timestampStr)\n","\n","\n","def reinit_autoencoder_model(model, reinit_num_layers=0):\n","    \"\"\"reinitialize autoencoder model layers\"\"\"\n","\n","    if reinit_num_layers:\n","        for layer in model.distilbert.transformer.layer[-reinit_num_layers:]:\n","            for module in layer.modules():\n","                if isinstance(module, nn.Embedding):\n","                  if module.weight.requires_grad:\n","                    module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                if isinstance(module, nn.Linear):\n","                  module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n","                elif isinstance(module, nn.LayerNorm):\n","                  module.bias.data.zero_()\n","                  module.weight.data.fill_(1.0)\n","                if isinstance(module, nn.Linear) and module.bias is not None:\n","                  module.bias.data.zero_()\n","\n","    return model\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"lqKiS7jbkC4x","executionInfo":{"status":"ok","timestamp":1644778252855,"user_tz":0,"elapsed":9,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["set_seed(RANDOM_SEED)"]},{"cell_type":"code","source":["dataset_dfs = load_from_disk('/content/drive/MyDrive/Dissertation/datasets/hatetwit_'+str(1))"],"metadata":{"id":"0DtMt9ngFcUt","executionInfo":{"status":"ok","timestamp":1644778252855,"user_tz":0,"elapsed":9,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results',          # output directory\n","    num_train_epochs=EPOCHS,              # total number of training epochs\n","    save_strategy =\"epoch\" ,\n","    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n","    weight_decay= WEIGHT_DECAY,               # strength of weight decay\n","    learning_rate= LEARNING_RATE, \n","    logging_dir='./disbert_hate_task/hyper/logs',     # directory for storing logs\n","    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","    evaluation_strategy=\"epoch\",\n","    #eval_steps = 500     # evaluate each `logging_steps`\n",")"],"metadata":{"id":"qO86KFanHLcD","executionInfo":{"status":"ok","timestamp":1644778252856,"user_tz":0,"elapsed":9,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"jo4n81tx6lbr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644778258534,"user_tz":0,"elapsed":5687,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"4c909079-36b7-41ca-9fb2-48fe1c37e279"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["hyper_trainer = Trainer(\n","    model_init=model_init_two_layer_reinit,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=dataset_dfs['train'],         # training dataset\n","    eval_dataset=dataset_dfs['validation'],          # evaluation dataset\n","    compute_metrics=compute_metrics     # the callback that computes metrics of interest\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"khwJ-nkJrtka","executionInfo":{"status":"ok","timestamp":1644778258535,"user_tz":0,"elapsed":7,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["def hp_space_optuna(trial) :\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 3),\n","        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n","        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.3),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [ 16, 32, 64]),\n","    }"]},{"cell_type":"code","source":["best_trial = hyper_trainer.hyperparameter_search(n_trials=40, direction=\"maximize\", backend=\"optuna\", hp_space=hp_space_optuna)"],"metadata":{"id":"PON0aGtf78Aw","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7c082975-71dc-4759-dd76-889f41585857","executionInfo":{"status":"ok","timestamp":1644787515929,"user_tz":0,"elapsed":9257400,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-13 18:50:58,475]\u001b[0m A new study created in memory with name: no-name-3b742680-6be4-406d-9109-d5ecf6ec790b\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3495' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3495/3495 07:02, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.586600</td>\n","      <td>0.544256</td>\n","      <td>0.780425</td>\n","      <td>0.735791</td>\n","      <td>0.732764</td>\n","      <td>0.740217</td>\n","      <td>0.717090</td>\n","      <td>0.753521</td>\n","      <td>0.684018</td>\n","      <td>0.857196</td>\n","      <td>0.847834</td>\n","      <td>0.866768</td>\n","      <td>0.633086</td>\n","      <td>0.619295</td>\n","      <td>0.647505</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.497300</td>\n","      <td>0.518703</td>\n","      <td>0.793733</td>\n","      <td>0.739827</td>\n","      <td>0.759422</td>\n","      <td>0.728901</td>\n","      <td>0.730712</td>\n","      <td>0.738431</td>\n","      <td>0.723153</td>\n","      <td>0.868852</td>\n","      <td>0.902629</td>\n","      <td>0.837513</td>\n","      <td>0.619918</td>\n","      <td>0.545643</td>\n","      <td>0.717599</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.428900</td>\n","      <td>0.510194</td>\n","      <td>0.793518</td>\n","      <td>0.748525</td>\n","      <td>0.749122</td>\n","      <td>0.750395</td>\n","      <td>0.737345</td>\n","      <td>0.776660</td>\n","      <td>0.701818</td>\n","      <td>0.867050</td>\n","      <td>0.865605</td>\n","      <td>0.868499</td>\n","      <td>0.641180</td>\n","      <td>0.608921</td>\n","      <td>0.677047</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495 (score: 0.5101936459541321).\n","\u001b[32m[I 2022-02-13 18:58:03,757]\u001b[0m Trial 0 finished with value: 9.78568530720917 and parameters: {'learning_rate': 1.801553090660746e-05, 'num_train_epochs': 3, 'seed': 35, 'warmup_steps': 85, 'weight_decay': 0.19808075449785992, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 9.78568530720917.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 10:13, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.632100</td>\n","      <td>0.596592</td>\n","      <td>0.757029</td>\n","      <td>0.695887</td>\n","      <td>0.713462</td>\n","      <td>0.688712</td>\n","      <td>0.689689</td>\n","      <td>0.713280</td>\n","      <td>0.667608</td>\n","      <td>0.841616</td>\n","      <td>0.871529</td>\n","      <td>0.813688</td>\n","      <td>0.556355</td>\n","      <td>0.481328</td>\n","      <td>0.659091</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.575600</td>\n","      <td>0.566918</td>\n","      <td>0.767976</td>\n","      <td>0.713900</td>\n","      <td>0.722245</td>\n","      <td>0.709366</td>\n","      <td>0.703922</td>\n","      <td>0.722334</td>\n","      <td>0.686424</td>\n","      <td>0.849211</td>\n","      <td>0.866346</td>\n","      <td>0.832740</td>\n","      <td>0.588568</td>\n","      <td>0.539419</td>\n","      <td>0.647572</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.564800</td>\n","      <td>0.563479</td>\n","      <td>0.769693</td>\n","      <td>0.712281</td>\n","      <td>0.725260</td>\n","      <td>0.707900</td>\n","      <td>0.709832</td>\n","      <td>0.744467</td>\n","      <td>0.678277</td>\n","      <td>0.851879</td>\n","      <td>0.873010</td>\n","      <td>0.831746</td>\n","      <td>0.575133</td>\n","      <td>0.506224</td>\n","      <td>0.665757</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990 (score: 0.5634791254997253).\n","\u001b[32m[I 2022-02-13 19:08:19,213]\u001b[0m Trial 1 finished with value: 9.351458669552894 and parameters: {'learning_rate': 3.9079225062003695e-06, 'num_train_epochs': 3, 'seed': 31, 'warmup_steps': 237, 'weight_decay': 0.10496835925612016, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 9.78568530720917.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 10:05, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.723700</td>\n","      <td>0.673845</td>\n","      <td>0.713672</td>\n","      <td>0.635917</td>\n","      <td>0.661961</td>\n","      <td>0.623476</td>\n","      <td>0.630052</td>\n","      <td>0.611670</td>\n","      <td>0.649573</td>\n","      <td>0.811524</td>\n","      <td>0.865605</td>\n","      <td>0.763803</td>\n","      <td>0.466175</td>\n","      <td>0.393154</td>\n","      <td>0.572508</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.661400</td>\n","      <td>0.635858</td>\n","      <td>0.732775</td>\n","      <td>0.674877</td>\n","      <td>0.681626</td>\n","      <td>0.669622</td>\n","      <td>0.655207</td>\n","      <td>0.651911</td>\n","      <td>0.658537</td>\n","      <td>0.821675</td>\n","      <td>0.839319</td>\n","      <td>0.804757</td>\n","      <td>0.547750</td>\n","      <td>0.517635</td>\n","      <td>0.581585</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.647800</td>\n","      <td>0.629892</td>\n","      <td>0.739429</td>\n","      <td>0.674907</td>\n","      <td>0.693123</td>\n","      <td>0.665286</td>\n","      <td>0.664995</td>\n","      <td>0.667002</td>\n","      <td>0.663000</td>\n","      <td>0.827660</td>\n","      <td>0.864124</td>\n","      <td>0.794148</td>\n","      <td>0.532067</td>\n","      <td>0.464730</td>\n","      <td>0.622222</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990 (score: 0.6298924684524536).\n","\u001b[32m[I 2022-02-13 19:18:26,698]\u001b[0m Trial 2 finished with value: 8.872692608875564 and parameters: {'learning_rate': 1.2962701816412703e-06, 'num_train_epochs': 3, 'seed': 39, 'warmup_steps': 95, 'weight_decay': 0.25945566695545663, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 9.78568530720917.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 06:27, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.615000</td>\n","      <td>0.565204</td>\n","      <td>0.765400</td>\n","      <td>0.703035</td>\n","      <td>0.727186</td>\n","      <td>0.691395</td>\n","      <td>0.698952</td>\n","      <td>0.704225</td>\n","      <td>0.693756</td>\n","      <td>0.847918</td>\n","      <td>0.889670</td>\n","      <td>0.809909</td>\n","      <td>0.562234</td>\n","      <td>0.480290</td>\n","      <td>0.677892</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.543700</td>\n","      <td>0.544345</td>\n","      <td>0.776132</td>\n","      <td>0.719904</td>\n","      <td>0.735355</td>\n","      <td>0.713371</td>\n","      <td>0.719068</td>\n","      <td>0.745473</td>\n","      <td>0.694470</td>\n","      <td>0.855192</td>\n","      <td>0.881155</td>\n","      <td>0.830716</td>\n","      <td>0.585452</td>\n","      <td>0.513485</td>\n","      <td>0.680880</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-4660 (score: 0.5443453788757324).\n","\u001b[32m[I 2022-02-13 19:24:55,782]\u001b[0m Trial 3 finished with value: 9.450655989656928 and parameters: {'learning_rate': 7.243656485409401e-06, 'num_train_epochs': 2, 'seed': 8, 'warmup_steps': 34, 'weight_decay': 0.17428944806598892, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 9.78568530720917.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3495' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3495/3495 06:35, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.712200</td>\n","      <td>0.652449</td>\n","      <td>0.728053</td>\n","      <td>0.657611</td>\n","      <td>0.679004</td>\n","      <td>0.646331</td>\n","      <td>0.652352</td>\n","      <td>0.641851</td>\n","      <td>0.663202</td>\n","      <td>0.820783</td>\n","      <td>0.865605</td>\n","      <td>0.780374</td>\n","      <td>0.499700</td>\n","      <td>0.431535</td>\n","      <td>0.593438</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.647100</td>\n","      <td>0.617230</td>\n","      <td>0.748659</td>\n","      <td>0.691866</td>\n","      <td>0.700161</td>\n","      <td>0.687975</td>\n","      <td>0.684544</td>\n","      <td>0.706237</td>\n","      <td>0.664144</td>\n","      <td>0.833152</td>\n","      <td>0.850426</td>\n","      <td>0.816566</td>\n","      <td>0.557901</td>\n","      <td>0.507261</td>\n","      <td>0.619772</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.612000</td>\n","      <td>0.606696</td>\n","      <td>0.753595</td>\n","      <td>0.695234</td>\n","      <td>0.707138</td>\n","      <td>0.690466</td>\n","      <td>0.691525</td>\n","      <td>0.718310</td>\n","      <td>0.666667</td>\n","      <td>0.837453</td>\n","      <td>0.859311</td>\n","      <td>0.816678</td>\n","      <td>0.556725</td>\n","      <td>0.493776</td>\n","      <td>0.638070</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-3495\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-3495/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-3495/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-3495 (score: 0.606696367263794).\n","\u001b[32m[I 2022-02-13 19:31:32,353]\u001b[0m Trial 4 finished with value: 9.124948710878593 and parameters: {'learning_rate': 2.390329089604453e-06, 'num_train_epochs': 3, 'seed': 14, 'warmup_steps': 314, 'weight_decay': 0.08189835936628033, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 9.78568530720917.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 06:31, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.586500</td>\n","      <td>0.527713</td>\n","      <td>0.773127</td>\n","      <td>0.715119</td>\n","      <td>0.737460</td>\n","      <td>0.724599</td>\n","      <td>0.709053</td>\n","      <td>0.847082</td>\n","      <td>0.609703</td>\n","      <td>0.859649</td>\n","      <td>0.852647</td>\n","      <td>0.866767</td>\n","      <td>0.576656</td>\n","      <td>0.474066</td>\n","      <td>0.735910</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.402000</td>\n","      <td>0.498605</td>\n","      <td>0.798884</td>\n","      <td>0.755246</td>\n","      <td>0.754615</td>\n","      <td>0.757528</td>\n","      <td>0.759750</td>\n","      <td>0.793763</td>\n","      <td>0.728532</td>\n","      <td>0.869920</td>\n","      <td>0.867827</td>\n","      <td>0.872024</td>\n","      <td>0.636069</td>\n","      <td>0.610996</td>\n","      <td>0.663288</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-4660 (score: 0.498604953289032).\n","\u001b[32m[I 2022-02-13 19:38:04,984]\u001b[0m Trial 5 finished with value: 9.868441309844021 and parameters: {'learning_rate': 5.7198607800777105e-05, 'num_train_epochs': 2, 'seed': 4, 'warmup_steps': 487, 'weight_decay': 0.24513152502569516, 'per_device_train_batch_size': 16}. Best is trial 5 with value: 9.868441309844021.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 05:01, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.695200</td>\n","      <td>0.557981</td>\n","      <td>0.775488</td>\n","      <td>0.717371</td>\n","      <td>0.740639</td>\n","      <td>0.700966</td>\n","      <td>0.698465</td>\n","      <td>0.640845</td>\n","      <td>0.767470</td>\n","      <td>0.857294</td>\n","      <td>0.901888</td>\n","      <td>0.816901</td>\n","      <td>0.596356</td>\n","      <td>0.560166</td>\n","      <td>0.637544</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.518200</td>\n","      <td>0.495086</td>\n","      <td>0.798669</td>\n","      <td>0.746477</td>\n","      <td>0.761751</td>\n","      <td>0.737906</td>\n","      <td>0.753839</td>\n","      <td>0.765594</td>\n","      <td>0.742439</td>\n","      <td>0.871997</td>\n","      <td>0.900407</td>\n","      <td>0.845325</td>\n","      <td>0.613597</td>\n","      <td>0.547718</td>\n","      <td>0.697490</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.365000</td>\n","      <td>0.514563</td>\n","      <td>0.806396</td>\n","      <td>0.762272</td>\n","      <td>0.762826</td>\n","      <td>0.764669</td>\n","      <td>0.778837</td>\n","      <td>0.821932</td>\n","      <td>0.740036</td>\n","      <td>0.875764</td>\n","      <td>0.875602</td>\n","      <td>0.875926</td>\n","      <td>0.632216</td>\n","      <td>0.596473</td>\n","      <td>0.672515</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-1166 (score: 0.49508553743362427).\n","\u001b[32m[I 2022-02-13 19:43:08,527]\u001b[0m Trial 6 finished with value: 9.96546190688163 and parameters: {'learning_rate': 6.779448121064052e-05, 'num_train_epochs': 3, 'seed': 31, 'warmup_steps': 491, 'weight_decay': 0.05951904479543706, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 03:17, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.663900</td>\n","      <td>0.532085</td>\n","      <td>0.778493</td>\n","      <td>0.725774</td>\n","      <td>0.731904</td>\n","      <td>0.726486</td>\n","      <td>0.715622</td>\n","      <td>0.769618</td>\n","      <td>0.668706</td>\n","      <td>0.859982</td>\n","      <td>0.865235</td>\n","      <td>0.854792</td>\n","      <td>0.601719</td>\n","      <td>0.544606</td>\n","      <td>0.672215</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.471200</td>\n","      <td>0.489350</td>\n","      <td>0.800386</td>\n","      <td>0.755627</td>\n","      <td>0.756508</td>\n","      <td>0.757663</td>\n","      <td>0.757504</td>\n","      <td>0.799799</td>\n","      <td>0.719457</td>\n","      <td>0.872013</td>\n","      <td>0.871529</td>\n","      <td>0.872498</td>\n","      <td>0.637363</td>\n","      <td>0.601660</td>\n","      <td>0.677570</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-1166 (score: 0.48935019969940186).\n","\u001b[32m[I 2022-02-13 19:46:27,957]\u001b[0m Trial 7 finished with value: 9.87957622527845 and parameters: {'learning_rate': 9.997363474330555e-05, 'num_train_epochs': 2, 'seed': 12, 'warmup_steps': 270, 'weight_decay': 0.14419108066651837, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2330/2330 04:11, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.607400</td>\n","      <td>0.545197</td>\n","      <td>0.775703</td>\n","      <td>0.724697</td>\n","      <td>0.731086</td>\n","      <td>0.719949</td>\n","      <td>0.703593</td>\n","      <td>0.709256</td>\n","      <td>0.698020</td>\n","      <td>0.855115</td>\n","      <td>0.869678</td>\n","      <td>0.841031</td>\n","      <td>0.615385</td>\n","      <td>0.580913</td>\n","      <td>0.654206</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.524500</td>\n","      <td>0.527529</td>\n","      <td>0.780210</td>\n","      <td>0.729147</td>\n","      <td>0.736018</td>\n","      <td>0.728808</td>\n","      <td>0.720339</td>\n","      <td>0.769618</td>\n","      <td>0.676991</td>\n","      <td>0.858506</td>\n","      <td>0.865976</td>\n","      <td>0.851164</td>\n","      <td>0.608596</td>\n","      <td>0.550830</td>\n","      <td>0.679898</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-8/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-8/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-8/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 19:50:47,302]\u001b[0m Trial 8 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/4660 03:03 < 03:03, 12.69 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.617700</td>\n","      <td>0.571502</td>\n","      <td>0.760249</td>\n","      <td>0.704891</td>\n","      <td>0.714231</td>\n","      <td>0.707324</td>\n","      <td>0.696404</td>\n","      <td>0.769618</td>\n","      <td>0.635910</td>\n","      <td>0.844141</td>\n","      <td>0.848204</td>\n","      <td>0.840117</td>\n","      <td>0.574129</td>\n","      <td>0.504149</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 19:53:58,243]\u001b[0m Trial 9 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 05:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.728400</td>\n","      <td>0.550346</td>\n","      <td>0.771196</td>\n","      <td>0.715597</td>\n","      <td>0.732297</td>\n","      <td>0.714031</td>\n","      <td>0.707182</td>\n","      <td>0.772636</td>\n","      <td>0.651952</td>\n","      <td>0.851373</td>\n","      <td>0.866346</td>\n","      <td>0.836910</td>\n","      <td>0.588235</td>\n","      <td>0.503112</td>\n","      <td>0.708029</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.531900</td>\n","      <td>0.510128</td>\n","      <td>0.791801</td>\n","      <td>0.737764</td>\n","      <td>0.768823</td>\n","      <td>0.720174</td>\n","      <td>0.739063</td>\n","      <td>0.722334</td>\n","      <td>0.756586</td>\n","      <td>0.861538</td>\n","      <td>0.912255</td>\n","      <td>0.816164</td>\n","      <td>0.612689</td>\n","      <td>0.525934</td>\n","      <td>0.733719</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.435300</td>\n","      <td>0.500947</td>\n","      <td>0.798669</td>\n","      <td>0.755375</td>\n","      <td>0.756174</td>\n","      <td>0.757141</td>\n","      <td>0.758489</td>\n","      <td>0.797787</td>\n","      <td>0.722881</td>\n","      <td>0.867987</td>\n","      <td>0.867827</td>\n","      <td>0.868148</td>\n","      <td>0.639650</td>\n","      <td>0.605809</td>\n","      <td>0.677494</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749 (score: 0.5009468197822571).\n","\u001b[32m[I 2022-02-13 19:59:18,320]\u001b[0m Trial 10 finished with value: 9.873430827797346 and parameters: {'learning_rate': 3.347362108844184e-05, 'num_train_epochs': 3, 'seed': 23, 'warmup_steps': 468, 'weight_decay': 0.017310141151868824, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 03:30, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.676400</td>\n","      <td>0.559727</td>\n","      <td>0.777205</td>\n","      <td>0.714056</td>\n","      <td>0.762579</td>\n","      <td>0.690522</td>\n","      <td>0.706568</td>\n","      <td>0.671026</td>\n","      <td>0.746085</td>\n","      <td>0.852213</td>\n","      <td>0.923362</td>\n","      <td>0.791244</td>\n","      <td>0.583386</td>\n","      <td>0.477178</td>\n","      <td>0.750408</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.484800</td>\n","      <td>0.486583</td>\n","      <td>0.801889</td>\n","      <td>0.757426</td>\n","      <td>0.758178</td>\n","      <td>0.761028</td>\n","      <td>0.760788</td>\n","      <td>0.815895</td>\n","      <td>0.712654</td>\n","      <td>0.872910</td>\n","      <td>0.869678</td>\n","      <td>0.876166</td>\n","      <td>0.638581</td>\n","      <td>0.597510</td>\n","      <td>0.685714</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166 (score: 0.48658254742622375).\n","\u001b[32m[I 2022-02-13 20:02:50,454]\u001b[0m Trial 11 finished with value: 9.908416736847574 and parameters: {'learning_rate': 9.306904223288078e-05, 'num_train_epochs': 2, 'seed': 18, 'warmup_steps': 352, 'weight_decay': 0.0015799292608630194, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 03:33, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.688800</td>\n","      <td>0.530558</td>\n","      <td>0.782571</td>\n","      <td>0.738391</td>\n","      <td>0.733502</td>\n","      <td>0.744001</td>\n","      <td>0.718522</td>\n","      <td>0.743461</td>\n","      <td>0.695202</td>\n","      <td>0.861174</td>\n","      <td>0.847464</td>\n","      <td>0.875335</td>\n","      <td>0.635476</td>\n","      <td>0.641079</td>\n","      <td>0.629969</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.489200</td>\n","      <td>0.480710</td>\n","      <td>0.799957</td>\n","      <td>0.753216</td>\n","      <td>0.760681</td>\n","      <td>0.752694</td>\n","      <td>0.753296</td>\n","      <td>0.804829</td>\n","      <td>0.707965</td>\n","      <td>0.870666</td>\n","      <td>0.878563</td>\n","      <td>0.862909</td>\n","      <td>0.635686</td>\n","      <td>0.574689</td>\n","      <td>0.711168</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166 (score: 0.4807102084159851).\n","\u001b[32m[I 2022-02-13 20:06:25,967]\u001b[0m Trial 12 finished with value: 9.866317503619673 and parameters: {'learning_rate': 8.618663011005413e-05, 'num_train_epochs': 2, 'seed': 20, 'warmup_steps': 385, 'weight_decay': 0.001966014493177245, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 05:01, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.709200</td>\n","      <td>0.550370</td>\n","      <td>0.768405</td>\n","      <td>0.729948</td>\n","      <td>0.721575</td>\n","      <td>0.741572</td>\n","      <td>0.712598</td>\n","      <td>0.728370</td>\n","      <td>0.697495</td>\n","      <td>0.844248</td>\n","      <td>0.813773</td>\n","      <td>0.877095</td>\n","      <td>0.632997</td>\n","      <td>0.682573</td>\n","      <td>0.590135</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.511300</td>\n","      <td>0.501610</td>\n","      <td>0.799528</td>\n","      <td>0.750274</td>\n","      <td>0.769357</td>\n","      <td>0.739512</td>\n","      <td>0.757214</td>\n","      <td>0.765594</td>\n","      <td>0.749016</td>\n","      <td>0.867285</td>\n","      <td>0.900037</td>\n","      <td>0.836833</td>\n","      <td>0.626322</td>\n","      <td>0.552905</td>\n","      <td>0.722222</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.409900</td>\n","      <td>0.505449</td>\n","      <td>0.803177</td>\n","      <td>0.759712</td>\n","      <td>0.762221</td>\n","      <td>0.760125</td>\n","      <td>0.765835</td>\n","      <td>0.802817</td>\n","      <td>0.732110</td>\n","      <td>0.871313</td>\n","      <td>0.874861</td>\n","      <td>0.867793</td>\n","      <td>0.641989</td>\n","      <td>0.602697</td>\n","      <td>0.686761</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-13/checkpoint-1166 (score: 0.5016103982925415).\n","\u001b[32m[I 2022-02-13 20:11:29,139]\u001b[0m Trial 13 finished with value: 9.931411211885951 and parameters: {'learning_rate': 4.1118213285063136e-05, 'num_train_epochs': 3, 'seed': 21, 'warmup_steps': 380, 'weight_decay': 0.033760580983478575, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 05:02, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.737500</td>\n","      <td>0.546564</td>\n","      <td>0.777205</td>\n","      <td>0.721302</td>\n","      <td>0.739264</td>\n","      <td>0.712376</td>\n","      <td>0.712881</td>\n","      <td>0.729376</td>\n","      <td>0.697115</td>\n","      <td>0.855407</td>\n","      <td>0.885968</td>\n","      <td>0.826883</td>\n","      <td>0.595619</td>\n","      <td>0.521784</td>\n","      <td>0.693793</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.539000</td>\n","      <td>0.509528</td>\n","      <td>0.793303</td>\n","      <td>0.743749</td>\n","      <td>0.759651</td>\n","      <td>0.736739</td>\n","      <td>0.744794</td>\n","      <td>0.773642</td>\n","      <td>0.718021</td>\n","      <td>0.863366</td>\n","      <td>0.887819</td>\n","      <td>0.840224</td>\n","      <td>0.623086</td>\n","      <td>0.548755</td>\n","      <td>0.720708</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.451600</td>\n","      <td>0.505035</td>\n","      <td>0.801245</td>\n","      <td>0.756276</td>\n","      <td>0.763046</td>\n","      <td>0.753824</td>\n","      <td>0.754935</td>\n","      <td>0.788732</td>\n","      <td>0.723915</td>\n","      <td>0.869788</td>\n","      <td>0.880415</td>\n","      <td>0.859415</td>\n","      <td>0.644106</td>\n","      <td>0.592324</td>\n","      <td>0.705810</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-14/checkpoint-1749 (score: 0.5050345659255981).\n","\u001b[32m[I 2022-02-13 20:16:32,811]\u001b[0m Trial 14 finished with value: 9.893829997992066 and parameters: {'learning_rate': 2.821204161566471e-05, 'num_train_epochs': 3, 'seed': 25, 'warmup_steps': 456, 'weight_decay': 0.04454809078766979, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:30 < 03:01, 6.40 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.735100</td>\n","      <td>0.551976</td>\n","      <td>0.766903</td>\n","      <td>0.715514</td>\n","      <td>0.720853</td>\n","      <td>0.710734</td>\n","      <td>0.692938</td>\n","      <td>0.681087</td>\n","      <td>0.705208</td>\n","      <td>0.848706</td>\n","      <td>0.861903</td>\n","      <td>0.835907</td>\n","      <td>0.604899</td>\n","      <td>0.589212</td>\n","      <td>0.621444</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:18:11,282]\u001b[0m Trial 15 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:30 < 03:00, 6.46 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.718700</td>\n","      <td>0.552829</td>\n","      <td>0.768405</td>\n","      <td>0.707392</td>\n","      <td>0.740770</td>\n","      <td>0.696125</td>\n","      <td>0.709990</td>\n","      <td>0.736419</td>\n","      <td>0.685393</td>\n","      <td>0.845477</td>\n","      <td>0.889300</td>\n","      <td>0.805770</td>\n","      <td>0.566709</td>\n","      <td>0.462656</td>\n","      <td>0.731148</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:19:48,907]\u001b[0m Trial 16 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:30 < 03:01, 6.44 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.738900</td>\n","      <td>0.569620</td>\n","      <td>0.764327</td>\n","      <td>0.711341</td>\n","      <td>0.727043</td>\n","      <td>0.699757</td>\n","      <td>0.685590</td>\n","      <td>0.631791</td>\n","      <td>0.749403</td>\n","      <td>0.844421</td>\n","      <td>0.874121</td>\n","      <td>0.816672</td>\n","      <td>0.604013</td>\n","      <td>0.593361</td>\n","      <td>0.615054</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:21:26,830]\u001b[0m Trial 17 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 04:57, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.674900</td>\n","      <td>0.540338</td>\n","      <td>0.779137</td>\n","      <td>0.721418</td>\n","      <td>0.741733</td>\n","      <td>0.713501</td>\n","      <td>0.721200</td>\n","      <td>0.749497</td>\n","      <td>0.694963</td>\n","      <td>0.857806</td>\n","      <td>0.888930</td>\n","      <td>0.828788</td>\n","      <td>0.585248</td>\n","      <td>0.502075</td>\n","      <td>0.701449</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.487600</td>\n","      <td>0.494034</td>\n","      <td>0.802318</td>\n","      <td>0.755904</td>\n","      <td>0.766058</td>\n","      <td>0.747985</td>\n","      <td>0.752154</td>\n","      <td>0.746479</td>\n","      <td>0.757916</td>\n","      <td>0.872110</td>\n","      <td>0.893743</td>\n","      <td>0.851499</td>\n","      <td>0.643449</td>\n","      <td>0.603734</td>\n","      <td>0.688757</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.376900</td>\n","      <td>0.513377</td>\n","      <td>0.802318</td>\n","      <td>0.757906</td>\n","      <td>0.757931</td>\n","      <td>0.759058</td>\n","      <td>0.767510</td>\n","      <td>0.793763</td>\n","      <td>0.742938</td>\n","      <td>0.874006</td>\n","      <td>0.874491</td>\n","      <td>0.873521</td>\n","      <td>0.632202</td>\n","      <td>0.608921</td>\n","      <td>0.657335</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-18/checkpoint-1166 (score: 0.4940342605113983).\n","\u001b[32m[I 2022-02-13 20:26:25,549]\u001b[0m Trial 18 finished with value: 9.901899175764317 and parameters: {'learning_rate': 5.621994300251687e-05, 'num_train_epochs': 3, 'seed': 16, 'warmup_steps': 230, 'weight_decay': 0.13256807206210164, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:30 < 03:01, 6.40 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.797500</td>\n","      <td>0.594526</td>\n","      <td>0.751878</td>\n","      <td>0.699716</td>\n","      <td>0.700359</td>\n","      <td>0.699534</td>\n","      <td>0.679171</td>\n","      <td>0.692153</td>\n","      <td>0.666667</td>\n","      <td>0.837888</td>\n","      <td>0.840059</td>\n","      <td>0.835727</td>\n","      <td>0.582090</td>\n","      <td>0.566390</td>\n","      <td>0.598684</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:28:04,203]\u001b[0m Trial 19 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/3495 02:03 < 04:06, 9.45 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.584800</td>\n","      <td>0.561304</td>\n","      <td>0.759605</td>\n","      <td>0.717303</td>\n","      <td>0.708849</td>\n","      <td>0.735264</td>\n","      <td>0.702016</td>\n","      <td>0.805835</td>\n","      <td>0.621894</td>\n","      <td>0.841060</td>\n","      <td>0.799334</td>\n","      <td>0.887382</td>\n","      <td>0.608833</td>\n","      <td>0.600622</td>\n","      <td>0.617271</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:30:15,075]\u001b[0m Trial 20 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 03:19, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.687200</td>\n","      <td>0.551859</td>\n","      <td>0.772483</td>\n","      <td>0.721416</td>\n","      <td>0.735200</td>\n","      <td>0.732296</td>\n","      <td>0.704297</td>\n","      <td>0.841046</td>\n","      <td>0.605797</td>\n","      <td>0.855469</td>\n","      <td>0.838208</td>\n","      <td>0.873457</td>\n","      <td>0.604482</td>\n","      <td>0.517635</td>\n","      <td>0.726346</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.482600</td>\n","      <td>0.486198</td>\n","      <td>0.795664</td>\n","      <td>0.749545</td>\n","      <td>0.752375</td>\n","      <td>0.750319</td>\n","      <td>0.754411</td>\n","      <td>0.795775</td>\n","      <td>0.717135</td>\n","      <td>0.867625</td>\n","      <td>0.871159</td>\n","      <td>0.864120</td>\n","      <td>0.626600</td>\n","      <td>0.584025</td>\n","      <td>0.675870</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-1166 (score: 0.48619794845581055).\n","\u001b[32m[I 2022-02-13 20:33:36,343]\u001b[0m Trial 21 finished with value: 9.804625085275404 and parameters: {'learning_rate': 7.590838134788001e-05, 'num_train_epochs': 2, 'seed': 17, 'warmup_steps': 355, 'weight_decay': 0.021842854857437366, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 03:20, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.684000</td>\n","      <td>0.523623</td>\n","      <td>0.783645</td>\n","      <td>0.735760</td>\n","      <td>0.740087</td>\n","      <td>0.732173</td>\n","      <td>0.718797</td>\n","      <td>0.721328</td>\n","      <td>0.716284</td>\n","      <td>0.859912</td>\n","      <td>0.870418</td>\n","      <td>0.849657</td>\n","      <td>0.628571</td>\n","      <td>0.604772</td>\n","      <td>0.654321</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.485300</td>\n","      <td>0.486389</td>\n","      <td>0.799313</td>\n","      <td>0.751958</td>\n","      <td>0.760061</td>\n","      <td>0.750586</td>\n","      <td>0.753788</td>\n","      <td>0.800805</td>\n","      <td>0.711986</td>\n","      <td>0.870265</td>\n","      <td>0.880415</td>\n","      <td>0.860347</td>\n","      <td>0.631821</td>\n","      <td>0.570539</td>\n","      <td>0.707851</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-1166 (score: 0.4863889217376709).\n","\u001b[32m[I 2022-02-13 20:36:58,743]\u001b[0m Trial 22 finished with value: 9.84973532376825 and parameters: {'learning_rate': 6.868925160114426e-05, 'num_train_epochs': 2, 'seed': 20, 'warmup_steps': 306, 'weight_decay': 0.003352479898824043, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:32 < 03:05, 6.29 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.710800</td>\n","      <td>0.548927</td>\n","      <td>0.765400</td>\n","      <td>0.705672</td>\n","      <td>0.727023</td>\n","      <td>0.692873</td>\n","      <td>0.676157</td>\n","      <td>0.669014</td>\n","      <td>0.683453</td>\n","      <td>0.848399</td>\n","      <td>0.887819</td>\n","      <td>0.812331</td>\n","      <td>0.592462</td>\n","      <td>0.521784</td>\n","      <td>0.685286</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:38:39,060]\u001b[0m Trial 23 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1166 01:31 < 01:31, 6.33 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.688500</td>\n","      <td>0.564875</td>\n","      <td>0.767761</td>\n","      <td>0.692344</td>\n","      <td>0.767268</td>\n","      <td>0.663463</td>\n","      <td>0.691928</td>\n","      <td>0.633803</td>\n","      <td>0.761790</td>\n","      <td>0.847266</td>\n","      <td>0.943725</td>\n","      <td>0.768697</td>\n","      <td>0.537838</td>\n","      <td>0.412863</td>\n","      <td>0.771318</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:40:18,810]\u001b[0m Trial 24 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:31 < 03:03, 6.36 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.695400</td>\n","      <td>0.551189</td>\n","      <td>0.776132</td>\n","      <td>0.715433</td>\n","      <td>0.747619</td>\n","      <td>0.698157</td>\n","      <td>0.707581</td>\n","      <td>0.690141</td>\n","      <td>0.725926</td>\n","      <td>0.853858</td>\n","      <td>0.907442</td>\n","      <td>0.806250</td>\n","      <td>0.584860</td>\n","      <td>0.496888</td>\n","      <td>0.710682</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:41:58,257]\u001b[0m Trial 25 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1166 01:31 < 01:31, 6.35 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.684500</td>\n","      <td>0.549180</td>\n","      <td>0.767332</td>\n","      <td>0.707358</td>\n","      <td>0.734704</td>\n","      <td>0.706094</td>\n","      <td>0.706679</td>\n","      <td>0.787726</td>\n","      <td>0.640753</td>\n","      <td>0.848518</td>\n","      <td>0.868937</td>\n","      <td>0.829036</td>\n","      <td>0.566879</td>\n","      <td>0.461618</td>\n","      <td>0.734323</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:43:37,588]\u001b[0m Trial 26 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:31 < 03:03, 6.36 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.709800</td>\n","      <td>0.550914</td>\n","      <td>0.771625</td>\n","      <td>0.710924</td>\n","      <td>0.740652</td>\n","      <td>0.712176</td>\n","      <td>0.703036</td>\n","      <td>0.803823</td>\n","      <td>0.624707</td>\n","      <td>0.856102</td>\n","      <td>0.870048</td>\n","      <td>0.842596</td>\n","      <td>0.573633</td>\n","      <td>0.462656</td>\n","      <td>0.754653</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:45:16,816]\u001b[0m Trial 27 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 03:19, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.680900</td>\n","      <td>0.522598</td>\n","      <td>0.785147</td>\n","      <td>0.733690</td>\n","      <td>0.748480</td>\n","      <td>0.722783</td>\n","      <td>0.716265</td>\n","      <td>0.702213</td>\n","      <td>0.730890</td>\n","      <td>0.860157</td>\n","      <td>0.890411</td>\n","      <td>0.831892</td>\n","      <td>0.624648</td>\n","      <td>0.575726</td>\n","      <td>0.682657</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.479400</td>\n","      <td>0.489708</td>\n","      <td>0.799742</td>\n","      <td>0.757167</td>\n","      <td>0.757832</td>\n","      <td>0.758689</td>\n","      <td>0.757561</td>\n","      <td>0.793763</td>\n","      <td>0.724518</td>\n","      <td>0.868358</td>\n","      <td>0.868197</td>\n","      <td>0.868519</td>\n","      <td>0.645583</td>\n","      <td>0.614108</td>\n","      <td>0.680460</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-28/checkpoint-1166 (score: 0.4897080361843109).\n","\u001b[32m[I 2022-02-13 20:48:37,613]\u001b[0m Trial 28 finished with value: 9.894497052485265 and parameters: {'learning_rate': 8.129101124040969e-05, 'num_train_epochs': 2, 'seed': 6, 'warmup_steps': 337, 'weight_decay': 0.05998525049510678, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/3495 02:03 < 04:06, 9.45 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.603900</td>\n","      <td>0.549214</td>\n","      <td>0.772269</td>\n","      <td>0.718629</td>\n","      <td>0.723872</td>\n","      <td>0.718606</td>\n","      <td>0.711511</td>\n","      <td>0.755533</td>\n","      <td>0.672337</td>\n","      <td>0.854939</td>\n","      <td>0.861903</td>\n","      <td>0.848087</td>\n","      <td>0.589438</td>\n","      <td>0.538382</td>\n","      <td>0.651192</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:50:48,382]\u001b[0m Trial 29 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/3495 04:19 < 02:09, 8.98 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.603200</td>\n","      <td>0.543644</td>\n","      <td>0.777849</td>\n","      <td>0.730032</td>\n","      <td>0.732635</td>\n","      <td>0.729681</td>\n","      <td>0.717874</td>\n","      <td>0.747485</td>\n","      <td>0.690520</td>\n","      <td>0.854301</td>\n","      <td>0.858571</td>\n","      <td>0.850073</td>\n","      <td>0.617922</td>\n","      <td>0.582988</td>\n","      <td>0.657310</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.507800</td>\n","      <td>0.510588</td>\n","      <td>0.791157</td>\n","      <td>0.746114</td>\n","      <td>0.747850</td>\n","      <td>0.744754</td>\n","      <td>0.731343</td>\n","      <td>0.739437</td>\n","      <td>0.723425</td>\n","      <td>0.864825</td>\n","      <td>0.869308</td>\n","      <td>0.860388</td>\n","      <td>0.642173</td>\n","      <td>0.625519</td>\n","      <td>0.659737</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-30/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-30/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-30/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:55:15,569]\u001b[0m Trial 30 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1167' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1167/1749 03:12 < 01:36, 6.06 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.685300</td>\n","      <td>0.538637</td>\n","      <td>0.776991</td>\n","      <td>0.728284</td>\n","      <td>0.729968</td>\n","      <td>0.728764</td>\n","      <td>0.713529</td>\n","      <td>0.745473</td>\n","      <td>0.684211</td>\n","      <td>0.855771</td>\n","      <td>0.857830</td>\n","      <td>0.853721</td>\n","      <td>0.615553</td>\n","      <td>0.582988</td>\n","      <td>0.651972</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.484000</td>\n","      <td>0.496615</td>\n","      <td>0.799099</td>\n","      <td>0.744026</td>\n","      <td>0.764432</td>\n","      <td>0.740682</td>\n","      <td>0.762483</td>\n","      <td>0.821932</td>\n","      <td>0.711053</td>\n","      <td>0.872544</td>\n","      <td>0.895964</td>\n","      <td>0.850316</td>\n","      <td>0.597052</td>\n","      <td>0.504149</td>\n","      <td>0.731928</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 20:58:35,681]\u001b[0m Trial 31 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1167' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1167/1749 03:11 < 01:35, 6.09 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.675300</td>\n","      <td>0.543865</td>\n","      <td>0.773986</td>\n","      <td>0.723239</td>\n","      <td>0.730734</td>\n","      <td>0.733911</td>\n","      <td>0.711646</td>\n","      <td>0.832998</td>\n","      <td>0.621155</td>\n","      <td>0.856657</td>\n","      <td>0.839689</td>\n","      <td>0.874325</td>\n","      <td>0.601415</td>\n","      <td>0.529046</td>\n","      <td>0.696721</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.490300</td>\n","      <td>0.495102</td>\n","      <td>0.795879</td>\n","      <td>0.743799</td>\n","      <td>0.753047</td>\n","      <td>0.746794</td>\n","      <td>0.747383</td>\n","      <td>0.825956</td>\n","      <td>0.682461</td>\n","      <td>0.873525</td>\n","      <td>0.877083</td>\n","      <td>0.869996</td>\n","      <td>0.610489</td>\n","      <td>0.537344</td>\n","      <td>0.706685</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-32/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-32/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-32/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:01:54,600]\u001b[0m Trial 32 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:31 < 03:03, 6.35 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.703800</td>\n","      <td>0.547603</td>\n","      <td>0.772483</td>\n","      <td>0.718243</td>\n","      <td>0.727882</td>\n","      <td>0.710989</td>\n","      <td>0.690248</td>\n","      <td>0.687123</td>\n","      <td>0.693401</td>\n","      <td>0.854821</td>\n","      <td>0.876342</td>\n","      <td>0.834332</td>\n","      <td>0.609661</td>\n","      <td>0.569502</td>\n","      <td>0.655914</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:03:34,148]\u001b[0m Trial 33 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 03:10 < 06:21, 12.21 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.571300</td>\n","      <td>0.548472</td>\n","      <td>0.776561</td>\n","      <td>0.716645</td>\n","      <td>0.748453</td>\n","      <td>0.706863</td>\n","      <td>0.717433</td>\n","      <td>0.753521</td>\n","      <td>0.684644</td>\n","      <td>0.853654</td>\n","      <td>0.893003</td>\n","      <td>0.817627</td>\n","      <td>0.578847</td>\n","      <td>0.474066</td>\n","      <td>0.743089</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:06:52,383]\u001b[0m Trial 34 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:31 < 03:03, 6.36 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.941500</td>\n","      <td>0.780163</td>\n","      <td>0.641339</td>\n","      <td>0.477315</td>\n","      <td>0.535952</td>\n","      <td>0.486075</td>\n","      <td>0.503589</td>\n","      <td>0.458753</td>\n","      <td>0.558140</td>\n","      <td>0.775270</td>\n","      <td>0.902999</td>\n","      <td>0.679198</td>\n","      <td>0.153086</td>\n","      <td>0.096473</td>\n","      <td>0.370518</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:08:31,649]\u001b[0m Trial 35 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 03:10 < 06:21, 12.23 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.668200</td>\n","      <td>0.629426</td>\n","      <td>0.738571</td>\n","      <td>0.667368</td>\n","      <td>0.698294</td>\n","      <td>0.650967</td>\n","      <td>0.657534</td>\n","      <td>0.627767</td>\n","      <td>0.690265</td>\n","      <td>0.828087</td>\n","      <td>0.886338</td>\n","      <td>0.777020</td>\n","      <td>0.516484</td>\n","      <td>0.438797</td>\n","      <td>0.627596</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:11:49,796]\u001b[0m Trial 36 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:31 < 03:03, 6.35 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.775200</td>\n","      <td>0.617222</td>\n","      <td>0.745010</td>\n","      <td>0.690320</td>\n","      <td>0.693475</td>\n","      <td>0.688712</td>\n","      <td>0.672549</td>\n","      <td>0.690141</td>\n","      <td>0.655832</td>\n","      <td>0.831836</td>\n","      <td>0.839689</td>\n","      <td>0.824128</td>\n","      <td>0.566575</td>\n","      <td>0.536307</td>\n","      <td>0.600465</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:13:29,069]\u001b[0m Trial 37 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 05:01, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.634200</td>\n","      <td>0.541557</td>\n","      <td>0.774415</td>\n","      <td>0.736718</td>\n","      <td>0.725973</td>\n","      <td>0.751451</td>\n","      <td>0.723424</td>\n","      <td>0.773642</td>\n","      <td>0.679329</td>\n","      <td>0.848473</td>\n","      <td>0.812662</td>\n","      <td>0.887586</td>\n","      <td>0.638256</td>\n","      <td>0.668050</td>\n","      <td>0.611006</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.472600</td>\n","      <td>0.498773</td>\n","      <td>0.803391</td>\n","      <td>0.752557</td>\n","      <td>0.771716</td>\n","      <td>0.742720</td>\n","      <td>0.762327</td>\n","      <td>0.777666</td>\n","      <td>0.747582</td>\n","      <td>0.872857</td>\n","      <td>0.904850</td>\n","      <td>0.843049</td>\n","      <td>0.622485</td>\n","      <td>0.545643</td>\n","      <td>0.724518</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.359900</td>\n","      <td>0.519240</td>\n","      <td>0.803821</td>\n","      <td>0.761829</td>\n","      <td>0.760504</td>\n","      <td>0.765092</td>\n","      <td>0.770704</td>\n","      <td>0.809859</td>\n","      <td>0.735160</td>\n","      <td>0.872234</td>\n","      <td>0.868197</td>\n","      <td>0.876308</td>\n","      <td>0.642549</td>\n","      <td>0.617220</td>\n","      <td>0.670045</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-38/checkpoint-1166 (score: 0.4987732768058777).\n","\u001b[32m[I 2022-02-13 21:18:32,319]\u001b[0m Trial 38 finished with value: 9.953520149401607 and parameters: {'learning_rate': 5.684200071168151e-05, 'num_train_epochs': 3, 'seed': 21, 'warmup_steps': 26, 'weight_decay': 0.1486409293347265, 'per_device_train_batch_size': 64}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, token_type_ids_bert, input_ids_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 06:41, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.551700</td>\n","      <td>0.521829</td>\n","      <td>0.785362</td>\n","      <td>0.744971</td>\n","      <td>0.738410</td>\n","      <td>0.758186</td>\n","      <td>0.729888</td>\n","      <td>0.816901</td>\n","      <td>0.659626</td>\n","      <td>0.859004</td>\n","      <td>0.830063</td>\n","      <td>0.890036</td>\n","      <td>0.646022</td>\n","      <td>0.627593</td>\n","      <td>0.665567</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.418300</td>\n","      <td>0.496331</td>\n","      <td>0.798884</td>\n","      <td>0.752729</td>\n","      <td>0.754442</td>\n","      <td>0.756197</td>\n","      <td>0.756656</td>\n","      <td>0.814889</td>\n","      <td>0.706190</td>\n","      <td>0.871776</td>\n","      <td>0.869678</td>\n","      <td>0.873884</td>\n","      <td>0.629754</td>\n","      <td>0.584025</td>\n","      <td>0.683252</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, token_type_ids_bert, __index_level_0__, input_ids_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-39/checkpoint-4660 (score: 0.49633103609085083).\n","\u001b[32m[I 2022-02-13 21:25:15,404]\u001b[0m Trial 39 finished with value: 9.852355972842762 and parameters: {'learning_rate': 3.8191793370453705e-05, 'num_train_epochs': 2, 'seed': 21, 'warmup_steps': 68, 'weight_decay': 0.15005004651360174, 'per_device_train_batch_size': 16}. Best is trial 6 with value: 9.96546190688163.\u001b[0m\n"]}]},{"cell_type":"code","source":["best_trial"],"metadata":{"id":"KXAQOU-_79fl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644787515929,"user_tz":0,"elapsed":25,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"92cec94d-a1b9-4766-9875-3a7ddb90543f"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='6', objective=9.96546190688163, hyperparameters={'learning_rate': 6.779448121064052e-05, 'num_train_epochs': 3, 'seed': 31, 'warmup_steps': 491, 'weight_decay': 0.05951904479543706, 'per_device_train_batch_size': 64})"]},"metadata":{},"execution_count":12}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Distilbert Experiment HateTwit Reinit HyperParam Search.ipynb","provenance":[{"file_id":"1rRPr2DENFy0pCRMIFdatZW-MljEG_50P","timestamp":1644757020448},{"file_id":"11zAh-a_KPmflomNvD1V-XdXzenQf9Q16","timestamp":1644755707832},{"file_id":"1a2My2hhUmHWnDPSOkajnyi9ZeBYHGBAE","timestamp":1644482917164},{"file_id":"1P3VgwZUmhtAsU0t-TfIzNCI19KZV3I_q","timestamp":1643491583972},{"file_id":"1-rZqxPTOtm21puGsZKK_FiKXeyCg7py7","timestamp":1643483337240},{"file_id":"15wylWRQJ1vxlP58OhBHUnY5fdagThMvJ","timestamp":1643406846637},{"file_id":"1B35hajBJY3fRQV7LSjqoqQ3xjdcyCzE3","timestamp":1642365829686},{"file_id":"1b0lCW2Cj6AULiE-Axh2OeZwURHR6pfcD","timestamp":1642333651961},{"file_id":"1RAjFaq-k9CLJQP5eO668UXLc0q-wjuve","timestamp":1642280213414},{"file_id":"1LzZr7GRN1SwVrNyjVX5t5PpCvio8w_kE","timestamp":1642109182548},{"file_id":"1t-qzEyxZtGn_Cnlfg2WtN-0dlgUmxfzx","timestamp":1641764210714},{"file_id":"14bEU8hCGPF8cVUA_BonJQA6Brv89DKUm","timestamp":1641252935713},{"file_id":"18Il3CpGf89tF1Z10iYX8OdfeHxxAX1Ui","timestamp":1639243141142},{"file_id":"1SHCgoRQVGtl9OiWEJGK3JgKkuxxSPy_5","timestamp":1639231968300},{"file_id":"1D-3yvF0nGnaWEZGxQj21R6fsGnyv5a5g","timestamp":1637526185003},{"file_id":"1glXr2JglKPUpN_3AGyk3GjfCtZSDNfsZ","timestamp":1637408594851},{"file_id":"1rc5hX8PBIv7GKvlxLQ35Vwf2jxLsv9f4","timestamp":1634501666668}],"mount_file_id":"1Otjeoys-uPPK6UK7Hr-3r4U4GrydolVA","authorship_tag":"ABX9TyPb5ukja6jmSjCVMVDIvEtm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}