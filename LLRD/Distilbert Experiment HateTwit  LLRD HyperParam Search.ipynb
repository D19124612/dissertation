{"cells":[{"cell_type":"code","source":["import os "],"metadata":{"id":"pXLAvBclVgn_","executionInfo":{"status":"ok","timestamp":1644787755050,"user_tz":0,"elapsed":8,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"F-o6bXOJ18j4","executionInfo":{"status":"ok","timestamp":1644787764598,"user_tz":0,"elapsed":9556,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["\n","!pip install -qq transformers\n","!pip install -qq optuna\n","!pip install -qq datasets"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Rz6wNlu92ge_","executionInfo":{"status":"ok","timestamp":1644787768404,"user_tz":0,"elapsed":3812,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["import transformers\n","import datasets\n","from transformers import AutoTokenizer,AutoModelForQuestionAnswering, AutoModelForSequenceClassification,AdamW, get_linear_schedule_with_warmup,Trainer, TrainingArguments\n","from transformers import DataCollator, DataCollatorForLanguageModeling,default_data_collator\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","import random\n","from textwrap import wrap\n","from datetime import datetime\n","from datasets import load_from_disk\n","from datasets import load_dataset\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support\n","from torch import nn"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"T7IFr4-3TKaA","executionInfo":{"status":"ok","timestamp":1644787768405,"user_tz":0,"elapsed":11,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["# the model we gonna train, base uncased BERT\n","# check text classification models here: https://huggingface.co/models?filter=text-classification\n","MODEL_NAME = \"distilbert-base-uncased\"\n","# max sequence length for each document/sentence sample\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE= 6.58e-5\n","WEIGHT_DECAY = 0.289\n","WARMUP_STEPS = 464\n","RANDOM_SEED=22\n","LEARNING_RATE_DECAY_MULTIPLIER = 0.95\n","REINIT_LAYERS = 2\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"YoKXcvyo_X47","executionInfo":{"status":"ok","timestamp":1644787906815,"user_tz":0,"elapsed":328,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["\n","class LLRDTrainer(Trainer):\n","\n","    def create_optimizer_and_scheduler(self, num_training_steps: int):\n","        \"\"\"\n","        Setup the optimizer and the learning rate scheduler.\n","        We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n","        Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n","        `create_scheduler`) in a subclass.\n","        \"\"\"\n","        self.create_optimizer()\n","        parameters = get_optimizer_parameters_with_llrd(self.model, LEARNING_RATE, 0.95)\n","        self.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n","        self.create_scheduler(num_training_steps=num_training_steps, optimizer=self.optimizer)\n","\n","\n","def set_seed(seed):\n","    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n","       When seed is None, disables deterministic mode.\n","    :param seed: an integer to your choosing\n","    \"\"\"\n","    if seed is not None:\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        np.random.seed(seed)\n","        random.seed(seed)\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","  acc = accuracy_score(labels, preds)\n","  confusion_matrix = classification_report(labels, preds, digits=4,output_dict=True)\n","  return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'hate_f1': confusion_matrix[\"0\"][\"f1-score\"],\n","        'hate_recall': confusion_matrix[\"0\"][\"recall\"],\n","        'hate_precision': confusion_matrix[\"0\"][\"precision\"],\n","        'offensive_f1': confusion_matrix[\"1\"][\"f1-score\"],\n","        'offensive_recall': confusion_matrix[\"1\"][\"recall\"],\n","        'offensive_precision': confusion_matrix[\"1\"][\"precision\"],\n","        'normal_f1': confusion_matrix[\"2\"][\"f1-score\"],\n","        'normal_recall': confusion_matrix[\"2\"][\"recall\"],\n","        'normal_precision': confusion_matrix[\"2\"][\"precision\"],    \n","  }\n","\n","def model_init():\n","  return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=3).to(device)\n","\n","def timestamp():\n","    dateTimeObj = datetime.now()\n","    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n","    print(timestampStr)\n","\n","\n","def get_optimizer_parameters_with_llrd(model, peak_lr, multiplicative_factor):\n","    num_encoder_layers = len(model.distilbert.transformer.layer)\n","    # Task specific layer gets the peak_lr\n","    tsl_parameters = [\n","        {\n","            \"params\": [param for name, param in model.named_parameters() if 'distilbert' not in name],\n","            \"param_names\": [name for name, param in model.named_parameters() if 'distilbert' not in name],\n","            \"lr\": peak_lr,\n","            \"name\": \"tsl\",\n","        }\n","    ]\n","\n","    # Starting from the last encoder layer each encoder layers get a lr defined by\n","    # current_layer_lr = prev_layer_lr * multiplicative_factor\n","    # the last encoder layer lr = peak_lr * multiplicative_factor\n","    encoder_parameters = [\n","        {\n","            \"params\": [param for name, param in model.named_parameters() if f\"distilbert.transformer.layer.{layer_num}\" in name],\n","            \"param_names\": [name for name, param in model.named_parameters() if f\"distilbert.transformer.layer.{layer_num}\" in name],\n","            \"lr\": peak_lr * (multiplicative_factor ** (num_encoder_layers - layer_num)),\n","            \"name\": f\"layer_{layer_num}\",\n","        }\n","        for layer_num, layer in enumerate(model.distilbert.transformer.layer)\n","    ]\n","\n","    # Embedding layer gets embedding layer lr = first encoder layer lr * multiplicative_factor\n","    embedding_parameters = [\n","        {\n","            \"params\": [param for name, param in model.named_parameters() if 'embeddings' in name],\n","            \"param_names\": [name for name, param in model.named_parameters() if 'embeddings' in name],\n","            \"lr\": peak_lr * (multiplicative_factor ** (num_encoder_layers + 1)),\n","            \"name\": \"embedding\",\n","        }\n","    ]\n","    return tsl_parameters + encoder_parameters + embedding_parameters\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"lqKiS7jbkC4x","executionInfo":{"status":"ok","timestamp":1644787768406,"user_tz":0,"elapsed":8,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["set_seed(RANDOM_SEED)"]},{"cell_type":"code","source":["dataset_dfs = load_from_disk('/content/drive/MyDrive/Dissertation/datasets/hatetwit_'+str(1))"],"metadata":{"id":"0DtMt9ngFcUt","executionInfo":{"status":"ok","timestamp":1644787849404,"user_tz":0,"elapsed":6585,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results',          # output directory\n","    num_train_epochs=EPOCHS,              # total number of training epochs\n","    save_strategy =\"epoch\" ,\n","    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n","    weight_decay= WEIGHT_DECAY,               # strength of weight decay\n","    learning_rate= LEARNING_RATE, \n","    logging_dir='./disbert_hate_llrd/hyper/logs',     # directory for storing logs\n","    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","    evaluation_strategy=\"epoch\",\n","    #eval_steps = 500     # evaluate each `logging_steps`\n",")"],"metadata":{"id":"qO86KFanHLcD","executionInfo":{"status":"ok","timestamp":1644787849405,"user_tz":0,"elapsed":25,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"id":"jo4n81tx6lbr","colab":{"base_uri":"https://localhost:8080/","height":917,"referenced_widgets":["54ec6bc361f345ee94f2349bad8c6496","98c24086d9fb4a8da4277727e77d21f8","ee66566a36f84abaaf5931632e5a9fc1","9f4f32242b4241cbbf90590588368636","607e5cdd924f478186bb9c79c8ac79e0","dfe9d27046e349cb8f6f01887a09909a","3a5e66e6f4e047ae95a5dacbc6764007","9654aa98437b42b1876f6b750b4af9f3","cb74caea6e9e4ee987d3d16c529d7931","8c1dcb9c42ec4ae083347901acac0566","420bbb97e8fc46e0a0077384f3a0eba3","4d7ce8d1b1084bdc9bc59e65fc5122fc","893112085c064234a626caf811b766bc","6c6b4b79e0b741a683a8193dfcf9edcf","2f33725ba43c4bf694958f57b7843188","a6987f4969154e4fab7bd8065809e65f","e1ac130fec0b4ee582c837faf9604c37","8c737547eee54521b4e46d9c2f679bc3","35150d7a41464b7b8192aa61ebdb83de","2c159dc9026a4a21a2942aae3633582a","cbc510aca3de4c0986711a564ff0865d","ad7385706bb748038ea14c67b312ed29"]},"executionInfo":{"status":"ok","timestamp":1644787935321,"user_tz":0,"elapsed":19500,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"a2ef090e-6b94-4482-946c-9a24f284240b"},"outputs":[{"output_type":"stream","name":"stderr","text":["https://huggingface.co/distilbert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoagdoj79\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"54ec6bc361f345ee94f2349bad8c6496","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/distilbert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","creating metadata file for /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpq3a9ey30\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d7ce8d1b1084bdc9bc59e65fc5122fc","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["hyper_trainer = LLRDTrainer(\n","    model_init=model_init,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=dataset_dfs['train'],         # training dataset\n","    eval_dataset=dataset_dfs['validation'],          # evaluation dataset\n","    compute_metrics=compute_metrics     # the callback that computes metrics of interest\n",")"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"khwJ-nkJrtka","executionInfo":{"status":"ok","timestamp":1644787935322,"user_tz":0,"elapsed":6,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["def hp_space_optuna(trial) :\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 3),\n","        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n","        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.3),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [ 16, 32]),\n","    }"]},{"cell_type":"code","source":["best_trial = hyper_trainer.hyperparameter_search(n_trials=40, direction=\"maximize\", backend=\"optuna\", hp_space=hp_space_optuna)"],"metadata":{"id":"PON0aGtf78Aw","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"409ea63a-8fd6-4aa2-a481-f89efc63e3e9","executionInfo":{"status":"ok","timestamp":1644795024228,"user_tz":0,"elapsed":7088911,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-13 21:32:15,668]\u001b[0m A new study created in memory with name: no-name-e8548c78-ab27-41f9-8116-83ecf8a9f26c\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:23, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.581500</td>\n","      <td>0.531052</td>\n","      <td>0.776991</td>\n","      <td>0.719508</td>\n","      <td>0.739714</td>\n","      <td>0.719972</td>\n","      <td>0.716539</td>\n","      <td>0.799799</td>\n","      <td>0.648980</td>\n","      <td>0.858341</td>\n","      <td>0.871529</td>\n","      <td>0.845546</td>\n","      <td>0.583643</td>\n","      <td>0.488589</td>\n","      <td>0.724615</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.432900</td>\n","      <td>0.493823</td>\n","      <td>0.805323</td>\n","      <td>0.763135</td>\n","      <td>0.760917</td>\n","      <td>0.767693</td>\n","      <td>0.768281</td>\n","      <td>0.813883</td>\n","      <td>0.727518</td>\n","      <td>0.874953</td>\n","      <td>0.867827</td>\n","      <td>0.882198</td>\n","      <td>0.646170</td>\n","      <td>0.621369</td>\n","      <td>0.673034</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.291100</td>\n","      <td>0.551367</td>\n","      <td>0.806182</td>\n","      <td>0.762200</td>\n","      <td>0.762712</td>\n","      <td>0.763177</td>\n","      <td>0.775352</td>\n","      <td>0.803823</td>\n","      <td>0.748828</td>\n","      <td>0.876363</td>\n","      <td>0.877823</td>\n","      <td>0.874908</td>\n","      <td>0.634886</td>\n","      <td>0.607884</td>\n","      <td>0.664399</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-0/checkpoint-4660 (score: 0.4938226044178009).\n","\u001b[32m[I 2022-02-13 21:37:41,962]\u001b[0m Trial 0 finished with value: 9.95853637408766 and parameters: {'learning_rate': 1.2114084305808536e-05, 'num_train_epochs': 3, 'seed': 13, 'warmup_steps': 398, 'weight_decay': 0.13172373821630495, 'per_device_train_batch_size': 16}. Best is trial 0 with value: 9.95853637408766.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.564800</td>\n","      <td>0.524745</td>\n","      <td>0.790298</td>\n","      <td>0.741399</td>\n","      <td>0.752521</td>\n","      <td>0.732149</td>\n","      <td>0.717976</td>\n","      <td>0.685111</td>\n","      <td>0.754153</td>\n","      <td>0.865537</td>\n","      <td>0.888930</td>\n","      <td>0.843344</td>\n","      <td>0.640683</td>\n","      <td>0.622407</td>\n","      <td>0.660066</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.425200</td>\n","      <td>0.490465</td>\n","      <td>0.801674</td>\n","      <td>0.767268</td>\n","      <td>0.758053</td>\n","      <td>0.779612</td>\n","      <td>0.771947</td>\n","      <td>0.791751</td>\n","      <td>0.753110</td>\n","      <td>0.869315</td>\n","      <td>0.838578</td>\n","      <td>0.902390</td>\n","      <td>0.660542</td>\n","      <td>0.708506</td>\n","      <td>0.618659</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.279500</td>\n","      <td>0.559296</td>\n","      <td>0.808328</td>\n","      <td>0.768571</td>\n","      <td>0.766243</td>\n","      <td>0.771155</td>\n","      <td>0.780296</td>\n","      <td>0.796781</td>\n","      <td>0.764479</td>\n","      <td>0.875442</td>\n","      <td>0.870418</td>\n","      <td>0.880524</td>\n","      <td>0.649974</td>\n","      <td>0.646266</td>\n","      <td>0.653725</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-1/checkpoint-4660 (score: 0.49046453833580017).\n","\u001b[32m[I 2022-02-13 21:43:04,071]\u001b[0m Trial 1 finished with value: 10.032200592981592 and parameters: {'learning_rate': 2.1221137870433124e-05, 'num_train_epochs': 3, 'seed': 7, 'warmup_steps': 164, 'weight_decay': 0.1290288922573804, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 10.032200592981592.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.563800</td>\n","      <td>0.516093</td>\n","      <td>0.792874</td>\n","      <td>0.745223</td>\n","      <td>0.751703</td>\n","      <td>0.740214</td>\n","      <td>0.732733</td>\n","      <td>0.736419</td>\n","      <td>0.729084</td>\n","      <td>0.866922</td>\n","      <td>0.881525</td>\n","      <td>0.852794</td>\n","      <td>0.636015</td>\n","      <td>0.602697</td>\n","      <td>0.673233</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.419900</td>\n","      <td>0.496215</td>\n","      <td>0.799313</td>\n","      <td>0.763789</td>\n","      <td>0.752609</td>\n","      <td>0.778980</td>\n","      <td>0.770093</td>\n","      <td>0.828974</td>\n","      <td>0.719023</td>\n","      <td>0.867001</td>\n","      <td>0.832655</td>\n","      <td>0.904302</td>\n","      <td>0.654271</td>\n","      <td>0.675311</td>\n","      <td>0.634503</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.264800</td>\n","      <td>0.582264</td>\n","      <td>0.804464</td>\n","      <td>0.764938</td>\n","      <td>0.758799</td>\n","      <td>0.772293</td>\n","      <td>0.779808</td>\n","      <td>0.815895</td>\n","      <td>0.746777</td>\n","      <td>0.873516</td>\n","      <td>0.857830</td>\n","      <td>0.889785</td>\n","      <td>0.641490</td>\n","      <td>0.643154</td>\n","      <td>0.639835</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-2/checkpoint-4660 (score: 0.49621501564979553).\n","\u001b[32m[I 2022-02-13 21:48:25,831]\u001b[0m Trial 2 finished with value: 9.98858378344516 and parameters: {'learning_rate': 9.283905626202353e-05, 'num_train_epochs': 3, 'seed': 1, 'warmup_steps': 145, 'weight_decay': 0.09669081100497436, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 10.032200592981592.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3495' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3495/3495 04:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.587900</td>\n","      <td>0.542904</td>\n","      <td>0.775703</td>\n","      <td>0.715361</td>\n","      <td>0.742043</td>\n","      <td>0.706356</td>\n","      <td>0.697608</td>\n","      <td>0.733400</td>\n","      <td>0.665146</td>\n","      <td>0.857906</td>\n","      <td>0.891892</td>\n","      <td>0.826415</td>\n","      <td>0.590571</td>\n","      <td>0.493776</td>\n","      <td>0.734568</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.460000</td>\n","      <td>0.500993</td>\n","      <td>0.799957</td>\n","      <td>0.747230</td>\n","      <td>0.767579</td>\n","      <td>0.746335</td>\n","      <td>0.762340</td>\n","      <td>0.839034</td>\n","      <td>0.698492</td>\n","      <td>0.871460</td>\n","      <td>0.888560</td>\n","      <td>0.855005</td>\n","      <td>0.607891</td>\n","      <td>0.511411</td>\n","      <td>0.749240</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.316700</td>\n","      <td>0.536322</td>\n","      <td>0.803606</td>\n","      <td>0.759273</td>\n","      <td>0.760096</td>\n","      <td>0.761230</td>\n","      <td>0.764678</td>\n","      <td>0.805835</td>\n","      <td>0.727520</td>\n","      <td>0.874283</td>\n","      <td>0.874121</td>\n","      <td>0.874444</td>\n","      <td>0.638858</td>\n","      <td>0.603734</td>\n","      <td>0.678322</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-3495\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-3495/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-3495/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-3/checkpoint-2330 (score: 0.5009927153587341).\n","\u001b[32m[I 2022-02-13 21:52:45,531]\u001b[0m Trial 3 finished with value: 9.925999852815512 and parameters: {'learning_rate': 1.3445908333563748e-06, 'num_train_epochs': 3, 'seed': 10, 'warmup_steps': 378, 'weight_decay': 0.17335133397735178, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 10.032200592981592.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 03:34, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.572000</td>\n","      <td>0.530558</td>\n","      <td>0.785362</td>\n","      <td>0.733358</td>\n","      <td>0.746368</td>\n","      <td>0.723081</td>\n","      <td>0.707395</td>\n","      <td>0.663984</td>\n","      <td>0.756881</td>\n","      <td>0.864894</td>\n","      <td>0.891151</td>\n","      <td>0.840140</td>\n","      <td>0.627784</td>\n","      <td>0.614108</td>\n","      <td>0.642082</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.419800</td>\n","      <td>0.486153</td>\n","      <td>0.804894</td>\n","      <td>0.761261</td>\n","      <td>0.764362</td>\n","      <td>0.760762</td>\n","      <td>0.761583</td>\n","      <td>0.793763</td>\n","      <td>0.731911</td>\n","      <td>0.873551</td>\n","      <td>0.878563</td>\n","      <td>0.868594</td>\n","      <td>0.648649</td>\n","      <td>0.609959</td>\n","      <td>0.692580</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-4/checkpoint-4660 (score: 0.4861527979373932).\n","\u001b[32m[I 2022-02-13 21:56:21,371]\u001b[0m Trial 4 finished with value: 9.950429286410818 and parameters: {'learning_rate': 2.784637073092407e-05, 'num_train_epochs': 2, 'seed': 2, 'warmup_steps': 426, 'weight_decay': 0.17647662017291058, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 10.032200592981592.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/4660 01:40 < 01:40, 23.15 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.579800</td>\n","      <td>0.532373</td>\n","      <td>0.776776</td>\n","      <td>0.713318</td>\n","      <td>0.758497</td>\n","      <td>0.691227</td>\n","      <td>0.692872</td>\n","      <td>0.664990</td>\n","      <td>0.723195</td>\n","      <td>0.854396</td>\n","      <td>0.921140</td>\n","      <td>0.796670</td>\n","      <td>0.592686</td>\n","      <td>0.487552</td>\n","      <td>0.755627</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 21:58:07,149]\u001b[0m Trial 5 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2330/2330 02:44, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.570200</td>\n","      <td>0.522648</td>\n","      <td>0.786220</td>\n","      <td>0.735714</td>\n","      <td>0.742111</td>\n","      <td>0.730051</td>\n","      <td>0.718686</td>\n","      <td>0.704225</td>\n","      <td>0.733753</td>\n","      <td>0.865927</td>\n","      <td>0.881155</td>\n","      <td>0.851216</td>\n","      <td>0.622531</td>\n","      <td>0.604772</td>\n","      <td>0.641364</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.425100</td>\n","      <td>0.492832</td>\n","      <td>0.797381</td>\n","      <td>0.749335</td>\n","      <td>0.751541</td>\n","      <td>0.751043</td>\n","      <td>0.753912</td>\n","      <td>0.799799</td>\n","      <td>0.713004</td>\n","      <td>0.872714</td>\n","      <td>0.874491</td>\n","      <td>0.870944</td>\n","      <td>0.621381</td>\n","      <td>0.578838</td>\n","      <td>0.670673</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-6/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-6/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-6/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:00:56,736]\u001b[0m Trial 6 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:40 < 03:20, 23.24 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.553400</td>\n","      <td>0.528150</td>\n","      <td>0.780640</td>\n","      <td>0.729512</td>\n","      <td>0.739217</td>\n","      <td>0.722471</td>\n","      <td>0.717588</td>\n","      <td>0.718310</td>\n","      <td>0.716867</td>\n","      <td>0.857453</td>\n","      <td>0.878563</td>\n","      <td>0.837332</td>\n","      <td>0.613497</td>\n","      <td>0.570539</td>\n","      <td>0.663450</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:02:42,076]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2330/2330 02:48, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.576600</td>\n","      <td>0.521536</td>\n","      <td>0.785791</td>\n","      <td>0.735093</td>\n","      <td>0.744503</td>\n","      <td>0.728633</td>\n","      <td>0.725187</td>\n","      <td>0.731388</td>\n","      <td>0.719090</td>\n","      <td>0.861950</td>\n","      <td>0.881896</td>\n","      <td>0.842887</td>\n","      <td>0.618141</td>\n","      <td>0.572614</td>\n","      <td>0.671533</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.439500</td>\n","      <td>0.482733</td>\n","      <td>0.803391</td>\n","      <td>0.757132</td>\n","      <td>0.762157</td>\n","      <td>0.757444</td>\n","      <td>0.768356</td>\n","      <td>0.815895</td>\n","      <td>0.726052</td>\n","      <td>0.873369</td>\n","      <td>0.879674</td>\n","      <td>0.867153</td>\n","      <td>0.629672</td>\n","      <td>0.576763</td>\n","      <td>0.693267</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-8/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-8/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-8/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:05:35,884]\u001b[0m Trial 8 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2330/2330 02:46, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.560000</td>\n","      <td>0.506658</td>\n","      <td>0.790513</td>\n","      <td>0.738983</td>\n","      <td>0.749088</td>\n","      <td>0.732779</td>\n","      <td>0.730598</td>\n","      <td>0.743461</td>\n","      <td>0.718173</td>\n","      <td>0.867221</td>\n","      <td>0.887449</td>\n","      <td>0.847895</td>\n","      <td>0.619128</td>\n","      <td>0.567427</td>\n","      <td>0.681196</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.421100</td>\n","      <td>0.483437</td>\n","      <td>0.805323</td>\n","      <td>0.761889</td>\n","      <td>0.764331</td>\n","      <td>0.761262</td>\n","      <td>0.764591</td>\n","      <td>0.790744</td>\n","      <td>0.740113</td>\n","      <td>0.874080</td>\n","      <td>0.878934</td>\n","      <td>0.869279</td>\n","      <td>0.646995</td>\n","      <td>0.614108</td>\n","      <td>0.683603</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-9/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-9/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-9/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:08:27,996]\u001b[0m Trial 9 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:28 < 01:44, 22.39 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.569400</td>\n","      <td>0.523761</td>\n","      <td>0.788581</td>\n","      <td>0.735694</td>\n","      <td>0.751232</td>\n","      <td>0.727061</td>\n","      <td>0.723593</td>\n","      <td>0.737425</td>\n","      <td>0.710271</td>\n","      <td>0.864525</td>\n","      <td>0.891892</td>\n","      <td>0.838788</td>\n","      <td>0.618965</td>\n","      <td>0.551867</td>\n","      <td>0.704636</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.435800</td>\n","      <td>0.500461</td>\n","      <td>0.801245</td>\n","      <td>0.753829</td>\n","      <td>0.765539</td>\n","      <td>0.746451</td>\n","      <td>0.760675</td>\n","      <td>0.770624</td>\n","      <td>0.750980</td>\n","      <td>0.870130</td>\n","      <td>0.893003</td>\n","      <td>0.848400</td>\n","      <td>0.630682</td>\n","      <td>0.575726</td>\n","      <td>0.697236</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-10/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-10/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-10/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:12:01,210]\u001b[0m Trial 10 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:40 < 03:20, 23.28 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.564000</td>\n","      <td>0.542260</td>\n","      <td>0.765830</td>\n","      <td>0.707170</td>\n","      <td>0.725958</td>\n","      <td>0.693450</td>\n","      <td>0.665941</td>\n","      <td>0.615694</td>\n","      <td>0.725118</td>\n","      <td>0.850664</td>\n","      <td>0.888930</td>\n","      <td>0.815557</td>\n","      <td>0.604905</td>\n","      <td>0.575726</td>\n","      <td>0.637199</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:13:46,487]\u001b[0m Trial 11 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:26 < 01:43, 22.61 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.566900</td>\n","      <td>0.520034</td>\n","      <td>0.786435</td>\n","      <td>0.735256</td>\n","      <td>0.748202</td>\n","      <td>0.726185</td>\n","      <td>0.727273</td>\n","      <td>0.724346</td>\n","      <td>0.730223</td>\n","      <td>0.861196</td>\n","      <td>0.887819</td>\n","      <td>0.836123</td>\n","      <td>0.617298</td>\n","      <td>0.566390</td>\n","      <td>0.678261</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.424500</td>\n","      <td>0.490748</td>\n","      <td>0.796523</td>\n","      <td>0.758744</td>\n","      <td>0.749742</td>\n","      <td>0.770107</td>\n","      <td>0.756654</td>\n","      <td>0.800805</td>\n","      <td>0.717117</td>\n","      <td>0.866743</td>\n","      <td>0.840429</td>\n","      <td>0.894758</td>\n","      <td>0.652834</td>\n","      <td>0.669087</td>\n","      <td>0.637352</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-12/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-12/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-12/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:17:17,696]\u001b[0m Trial 12 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:26 < 01:43, 22.58 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.558400</td>\n","      <td>0.540601</td>\n","      <td>0.789440</td>\n","      <td>0.735819</td>\n","      <td>0.762872</td>\n","      <td>0.720415</td>\n","      <td>0.725610</td>\n","      <td>0.718310</td>\n","      <td>0.733060</td>\n","      <td>0.861116</td>\n","      <td>0.905591</td>\n","      <td>0.820805</td>\n","      <td>0.620731</td>\n","      <td>0.537344</td>\n","      <td>0.734752</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.438000</td>\n","      <td>0.507018</td>\n","      <td>0.804035</td>\n","      <td>0.755615</td>\n","      <td>0.770127</td>\n","      <td>0.747082</td>\n","      <td>0.767857</td>\n","      <td>0.778672</td>\n","      <td>0.757339</td>\n","      <td>0.872329</td>\n","      <td>0.899297</td>\n","      <td>0.846932</td>\n","      <td>0.626659</td>\n","      <td>0.563278</td>\n","      <td>0.706112</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-13/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-13/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-13/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:20:49,153]\u001b[0m Trial 13 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:25 < 01:42, 22.63 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.568700</td>\n","      <td>0.537998</td>\n","      <td>0.781713</td>\n","      <td>0.735096</td>\n","      <td>0.737360</td>\n","      <td>0.739226</td>\n","      <td>0.726015</td>\n","      <td>0.791751</td>\n","      <td>0.670358</td>\n","      <td>0.856877</td>\n","      <td>0.852277</td>\n","      <td>0.861527</td>\n","      <td>0.622397</td>\n","      <td>0.573651</td>\n","      <td>0.680197</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.435900</td>\n","      <td>0.496118</td>\n","      <td>0.803821</td>\n","      <td>0.753755</td>\n","      <td>0.769399</td>\n","      <td>0.742411</td>\n","      <td>0.759610</td>\n","      <td>0.745473</td>\n","      <td>0.774295</td>\n","      <td>0.875313</td>\n","      <td>0.907071</td>\n","      <td>0.845702</td>\n","      <td>0.626343</td>\n","      <td>0.574689</td>\n","      <td>0.688199</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-14/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-14/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-14/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:24:20,141]\u001b[0m Trial 14 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:18, 23.43 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.560600</td>\n","      <td>0.534236</td>\n","      <td>0.775918</td>\n","      <td>0.732524</td>\n","      <td>0.729684</td>\n","      <td>0.742673</td>\n","      <td>0.716981</td>\n","      <td>0.802817</td>\n","      <td>0.647727</td>\n","      <td>0.851613</td>\n","      <td>0.830803</td>\n","      <td>0.873492</td>\n","      <td>0.628979</td>\n","      <td>0.594398</td>\n","      <td>0.667832</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:26:04,657]\u001b[0m Trial 15 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:25 < 01:42, 22.68 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.574700</td>\n","      <td>0.524974</td>\n","      <td>0.791586</td>\n","      <td>0.737178</td>\n","      <td>0.761377</td>\n","      <td>0.723334</td>\n","      <td>0.731041</td>\n","      <td>0.727364</td>\n","      <td>0.734756</td>\n","      <td>0.865018</td>\n","      <td>0.906331</td>\n","      <td>0.827307</td>\n","      <td>0.615476</td>\n","      <td>0.536307</td>\n","      <td>0.722067</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.414700</td>\n","      <td>0.510304</td>\n","      <td>0.798240</td>\n","      <td>0.758908</td>\n","      <td>0.752712</td>\n","      <td>0.766254</td>\n","      <td>0.762042</td>\n","      <td>0.795775</td>\n","      <td>0.731054</td>\n","      <td>0.867925</td>\n","      <td>0.851536</td>\n","      <td>0.884956</td>\n","      <td>0.646756</td>\n","      <td>0.651452</td>\n","      <td>0.642127</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-16/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-16/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-16/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:29:35,136]\u001b[0m Trial 16 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:25 < 01:42, 22.68 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.561200</td>\n","      <td>0.540166</td>\n","      <td>0.779137</td>\n","      <td>0.732704</td>\n","      <td>0.733242</td>\n","      <td>0.745695</td>\n","      <td>0.723884</td>\n","      <td>0.840040</td>\n","      <td>0.635948</td>\n","      <td>0.858232</td>\n","      <td>0.833765</td>\n","      <td>0.884177</td>\n","      <td>0.615995</td>\n","      <td>0.563278</td>\n","      <td>0.679599</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.431200</td>\n","      <td>0.506772</td>\n","      <td>0.797381</td>\n","      <td>0.753240</td>\n","      <td>0.751457</td>\n","      <td>0.761553</td>\n","      <td>0.758778</td>\n","      <td>0.837022</td>\n","      <td>0.693912</td>\n","      <td>0.869712</td>\n","      <td>0.856350</td>\n","      <td>0.883499</td>\n","      <td>0.631229</td>\n","      <td>0.591286</td>\n","      <td>0.676960</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-17/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-17/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-17/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:33:05,674]\u001b[0m Trial 17 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/3495 02:43 < 01:21, 14.28 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.573100</td>\n","      <td>0.518818</td>\n","      <td>0.790298</td>\n","      <td>0.745698</td>\n","      <td>0.745525</td>\n","      <td>0.747888</td>\n","      <td>0.738860</td>\n","      <td>0.775654</td>\n","      <td>0.705398</td>\n","      <td>0.863400</td>\n","      <td>0.861163</td>\n","      <td>0.865649</td>\n","      <td>0.634835</td>\n","      <td>0.606846</td>\n","      <td>0.665529</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.443300</td>\n","      <td>0.489973</td>\n","      <td>0.800601</td>\n","      <td>0.754466</td>\n","      <td>0.763424</td>\n","      <td>0.746828</td>\n","      <td>0.758230</td>\n","      <td>0.741449</td>\n","      <td>0.775789</td>\n","      <td>0.871312</td>\n","      <td>0.891151</td>\n","      <td>0.852337</td>\n","      <td>0.633856</td>\n","      <td>0.607884</td>\n","      <td>0.662147</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-18/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-18/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-18/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:35:53,919]\u001b[0m Trial 18 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:19, 23.34 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.556500</td>\n","      <td>0.527583</td>\n","      <td>0.779137</td>\n","      <td>0.719162</td>\n","      <td>0.749271</td>\n","      <td>0.724104</td>\n","      <td>0.721241</td>\n","      <td>0.842052</td>\n","      <td>0.630746</td>\n","      <td>0.861860</td>\n","      <td>0.869678</td>\n","      <td>0.854182</td>\n","      <td>0.574386</td>\n","      <td>0.460581</td>\n","      <td>0.762887</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:37:38,875]\u001b[0m Trial 19 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 03:25, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.568000</td>\n","      <td>0.520935</td>\n","      <td>0.789869</td>\n","      <td>0.742334</td>\n","      <td>0.746107</td>\n","      <td>0.743767</td>\n","      <td>0.732738</td>\n","      <td>0.784708</td>\n","      <td>0.687225</td>\n","      <td>0.865115</td>\n","      <td>0.866716</td>\n","      <td>0.863519</td>\n","      <td>0.629150</td>\n","      <td>0.579876</td>\n","      <td>0.687577</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.426000</td>\n","      <td>0.496946</td>\n","      <td>0.796308</td>\n","      <td>0.754982</td>\n","      <td>0.751977</td>\n","      <td>0.759828</td>\n","      <td>0.751787</td>\n","      <td>0.793763</td>\n","      <td>0.714027</td>\n","      <td>0.866068</td>\n","      <td>0.857090</td>\n","      <td>0.875236</td>\n","      <td>0.647090</td>\n","      <td>0.628631</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-20/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-20/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-20/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:41:09,591]\u001b[0m Trial 20 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:18, 23.50 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.573200</td>\n","      <td>0.554680</td>\n","      <td>0.777849</td>\n","      <td>0.731604</td>\n","      <td>0.740291</td>\n","      <td>0.724306</td>\n","      <td>0.720942</td>\n","      <td>0.708249</td>\n","      <td>0.734098</td>\n","      <td>0.848869</td>\n","      <td>0.868197</td>\n","      <td>0.830382</td>\n","      <td>0.625000</td>\n","      <td>0.596473</td>\n","      <td>0.656393</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:42:53,776]\u001b[0m Trial 21 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:24 < 01:42, 22.73 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.569400</td>\n","      <td>0.518625</td>\n","      <td>0.790298</td>\n","      <td>0.741575</td>\n","      <td>0.749095</td>\n","      <td>0.743593</td>\n","      <td>0.734375</td>\n","      <td>0.803823</td>\n","      <td>0.675973</td>\n","      <td>0.865424</td>\n","      <td>0.867827</td>\n","      <td>0.863034</td>\n","      <td>0.624928</td>\n","      <td>0.559129</td>\n","      <td>0.708279</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.429700</td>\n","      <td>0.481130</td>\n","      <td>0.802533</td>\n","      <td>0.755314</td>\n","      <td>0.765859</td>\n","      <td>0.747879</td>\n","      <td>0.766299</td>\n","      <td>0.768612</td>\n","      <td>0.764000</td>\n","      <td>0.871841</td>\n","      <td>0.894113</td>\n","      <td>0.850652</td>\n","      <td>0.627803</td>\n","      <td>0.580913</td>\n","      <td>0.682927</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-22/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-22/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-22/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:46:23,765]\u001b[0m Trial 22 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:18, 23.45 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.577400</td>\n","      <td>0.528847</td>\n","      <td>0.782571</td>\n","      <td>0.723346</td>\n","      <td>0.747798</td>\n","      <td>0.721635</td>\n","      <td>0.722475</td>\n","      <td>0.798793</td>\n","      <td>0.659468</td>\n","      <td>0.863760</td>\n","      <td>0.883747</td>\n","      <td>0.844657</td>\n","      <td>0.583804</td>\n","      <td>0.482365</td>\n","      <td>0.739269</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:48:08,269]\u001b[0m Trial 23 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:25 < 01:42, 22.72 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.576100</td>\n","      <td>0.528316</td>\n","      <td>0.787293</td>\n","      <td>0.729156</td>\n","      <td>0.763986</td>\n","      <td>0.711620</td>\n","      <td>0.728571</td>\n","      <td>0.718310</td>\n","      <td>0.739130</td>\n","      <td>0.860627</td>\n","      <td>0.914476</td>\n","      <td>0.812767</td>\n","      <td>0.598269</td>\n","      <td>0.502075</td>\n","      <td>0.740061</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.431700</td>\n","      <td>0.488036</td>\n","      <td>0.797596</td>\n","      <td>0.759332</td>\n","      <td>0.750140</td>\n","      <td>0.773378</td>\n","      <td>0.769583</td>\n","      <td>0.845070</td>\n","      <td>0.706476</td>\n","      <td>0.866450</td>\n","      <td>0.837097</td>\n","      <td>0.897935</td>\n","      <td>0.641962</td>\n","      <td>0.637967</td>\n","      <td>0.646008</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-24/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-24/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-24/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:51:38,457]\u001b[0m Trial 24 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:18, 23.45 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.584800</td>\n","      <td>0.529258</td>\n","      <td>0.779566</td>\n","      <td>0.724198</td>\n","      <td>0.744875</td>\n","      <td>0.727672</td>\n","      <td>0.717926</td>\n","      <td>0.821932</td>\n","      <td>0.637285</td>\n","      <td>0.860140</td>\n","      <td>0.865235</td>\n","      <td>0.855104</td>\n","      <td>0.594527</td>\n","      <td>0.495851</td>\n","      <td>0.742236</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:53:22,850]\u001b[0m Trial 25 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/3495 02:42 < 01:21, 14.32 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.587500</td>\n","      <td>0.528773</td>\n","      <td>0.782786</td>\n","      <td>0.731977</td>\n","      <td>0.741164</td>\n","      <td>0.732102</td>\n","      <td>0.727695</td>\n","      <td>0.787726</td>\n","      <td>0.676166</td>\n","      <td>0.859134</td>\n","      <td>0.867086</td>\n","      <td>0.851327</td>\n","      <td>0.609102</td>\n","      <td>0.541494</td>\n","      <td>0.696000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.455600</td>\n","      <td>0.479288</td>\n","      <td>0.801889</td>\n","      <td>0.760168</td>\n","      <td>0.760579</td>\n","      <td>0.759930</td>\n","      <td>0.763564</td>\n","      <td>0.771630</td>\n","      <td>0.755665</td>\n","      <td>0.870658</td>\n","      <td>0.872270</td>\n","      <td>0.869052</td>\n","      <td>0.646284</td>\n","      <td>0.635892</td>\n","      <td>0.657020</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-26/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-26/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-26/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 22:56:10,608]\u001b[0m Trial 26 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:18, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.575000</td>\n","      <td>0.528166</td>\n","      <td>0.783859</td>\n","      <td>0.735830</td>\n","      <td>0.742964</td>\n","      <td>0.731601</td>\n","      <td>0.727898</td>\n","      <td>0.745473</td>\n","      <td>0.711132</td>\n","      <td>0.857247</td>\n","      <td>0.871529</td>\n","      <td>0.843425</td>\n","      <td>0.622346</td>\n","      <td>0.577801</td>\n","      <td>0.674334</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.431900</td>\n","      <td>0.482671</td>\n","      <td>0.809616</td>\n","      <td>0.765832</td>\n","      <td>0.774262</td>\n","      <td>0.758887</td>\n","      <td>0.771574</td>\n","      <td>0.764588</td>\n","      <td>0.778689</td>\n","      <td>0.876360</td>\n","      <td>0.894854</td>\n","      <td>0.858615</td>\n","      <td>0.649563</td>\n","      <td>0.617220</td>\n","      <td>0.685484</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.278900</td>\n","      <td>0.556421</td>\n","      <td>0.805967</td>\n","      <td>0.763095</td>\n","      <td>0.762015</td>\n","      <td>0.764714</td>\n","      <td>0.773529</td>\n","      <td>0.793763</td>\n","      <td>0.754302</td>\n","      <td>0.876646</td>\n","      <td>0.874861</td>\n","      <td>0.878439</td>\n","      <td>0.639110</td>\n","      <td>0.625519</td>\n","      <td>0.653304</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-27/checkpoint-4660 (score: 0.4826709032058716).\n","\u001b[32m[I 2022-02-13 23:01:30,874]\u001b[0m Trial 27 finished with value: 9.965264264587924 and parameters: {'learning_rate': 6.861454743716765e-05, 'num_train_epochs': 3, 'seed': 16, 'warmup_steps': 172, 'weight_decay': 0.2118491168009582, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 10.032200592981592.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:40 < 03:20, 23.25 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.561900</td>\n","      <td>0.541471</td>\n","      <td>0.785362</td>\n","      <td>0.731589</td>\n","      <td>0.753575</td>\n","      <td>0.720964</td>\n","      <td>0.722359</td>\n","      <td>0.739437</td>\n","      <td>0.706052</td>\n","      <td>0.859330</td>\n","      <td>0.893373</td>\n","      <td>0.827787</td>\n","      <td>0.613077</td>\n","      <td>0.530083</td>\n","      <td>0.726885</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:03:16,200]\u001b[0m Trial 28 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:19, 23.36 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.573700</td>\n","      <td>0.533640</td>\n","      <td>0.782786</td>\n","      <td>0.722114</td>\n","      <td>0.755239</td>\n","      <td>0.714914</td>\n","      <td>0.716418</td>\n","      <td>0.772636</td>\n","      <td>0.667826</td>\n","      <td>0.862591</td>\n","      <td>0.895964</td>\n","      <td>0.831615</td>\n","      <td>0.587332</td>\n","      <td>0.476141</td>\n","      <td>0.766277</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:05:01,037]\u001b[0m Trial 29 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:29 < 01:44, 22.28 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.566000</td>\n","      <td>0.523683</td>\n","      <td>0.790942</td>\n","      <td>0.744281</td>\n","      <td>0.749283</td>\n","      <td>0.742007</td>\n","      <td>0.730452</td>\n","      <td>0.756539</td>\n","      <td>0.706103</td>\n","      <td>0.864211</td>\n","      <td>0.873010</td>\n","      <td>0.855588</td>\n","      <td>0.638180</td>\n","      <td>0.596473</td>\n","      <td>0.686158</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.422800</td>\n","      <td>0.494058</td>\n","      <td>0.801030</td>\n","      <td>0.744452</td>\n","      <td>0.768157</td>\n","      <td>0.739569</td>\n","      <td>0.767857</td>\n","      <td>0.821932</td>\n","      <td>0.720459</td>\n","      <td>0.874194</td>\n","      <td>0.902999</td>\n","      <td>0.847169</td>\n","      <td>0.591304</td>\n","      <td>0.493776</td>\n","      <td>0.736842</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-30/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-30/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-30/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:08:35,295]\u001b[0m Trial 30 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:26 < 01:43, 22.56 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.560800</td>\n","      <td>0.519986</td>\n","      <td>0.783645</td>\n","      <td>0.735781</td>\n","      <td>0.741645</td>\n","      <td>0.730571</td>\n","      <td>0.714650</td>\n","      <td>0.704225</td>\n","      <td>0.725389</td>\n","      <td>0.859537</td>\n","      <td>0.873380</td>\n","      <td>0.846126</td>\n","      <td>0.633155</td>\n","      <td>0.614108</td>\n","      <td>0.653422</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.418300</td>\n","      <td>0.497173</td>\n","      <td>0.805752</td>\n","      <td>0.749035</td>\n","      <td>0.780837</td>\n","      <td>0.737208</td>\n","      <td>0.773060</td>\n","      <td>0.796781</td>\n","      <td>0.750711</td>\n","      <td>0.875925</td>\n","      <td>0.920030</td>\n","      <td>0.835856</td>\n","      <td>0.598119</td>\n","      <td>0.494813</td>\n","      <td>0.755943</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-31/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-31/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-31/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:12:06,977]\u001b[0m Trial 31 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:26 < 01:43, 22.56 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.564900</td>\n","      <td>0.532596</td>\n","      <td>0.784074</td>\n","      <td>0.740899</td>\n","      <td>0.740442</td>\n","      <td>0.741652</td>\n","      <td>0.719674</td>\n","      <td>0.710262</td>\n","      <td>0.729339</td>\n","      <td>0.858682</td>\n","      <td>0.855979</td>\n","      <td>0.861401</td>\n","      <td>0.644343</td>\n","      <td>0.658714</td>\n","      <td>0.630586</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.431400</td>\n","      <td>0.485098</td>\n","      <td>0.808328</td>\n","      <td>0.759171</td>\n","      <td>0.774383</td>\n","      <td>0.751628</td>\n","      <td>0.770660</td>\n","      <td>0.792757</td>\n","      <td>0.749762</td>\n","      <td>0.877180</td>\n","      <td>0.902999</td>\n","      <td>0.852797</td>\n","      <td>0.629673</td>\n","      <td>0.559129</td>\n","      <td>0.720588</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-32/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-32/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-32/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:15:38,672]\u001b[0m Trial 32 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:19, 23.30 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.579300</td>\n","      <td>0.540704</td>\n","      <td>0.774200</td>\n","      <td>0.717241</td>\n","      <td>0.734823</td>\n","      <td>0.719035</td>\n","      <td>0.713004</td>\n","      <td>0.799799</td>\n","      <td>0.643204</td>\n","      <td>0.856410</td>\n","      <td>0.865605</td>\n","      <td>0.847408</td>\n","      <td>0.582310</td>\n","      <td>0.491701</td>\n","      <td>0.713855</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:17:23,796]\u001b[0m Trial 33 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/2330 01:18 < 01:18, 14.79 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.587100</td>\n","      <td>0.536117</td>\n","      <td>0.778064</td>\n","      <td>0.722232</td>\n","      <td>0.734618</td>\n","      <td>0.720819</td>\n","      <td>0.722430</td>\n","      <td>0.777666</td>\n","      <td>0.674520</td>\n","      <td>0.858755</td>\n","      <td>0.873380</td>\n","      <td>0.844612</td>\n","      <td>0.585511</td>\n","      <td>0.511411</td>\n","      <td>0.684722</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:18:47,668]\u001b[0m Trial 34 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:39 < 03:19, 23.37 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.582000</td>\n","      <td>0.529935</td>\n","      <td>0.780640</td>\n","      <td>0.722789</td>\n","      <td>0.748302</td>\n","      <td>0.727285</td>\n","      <td>0.717194</td>\n","      <td>0.832998</td>\n","      <td>0.629658</td>\n","      <td>0.863611</td>\n","      <td>0.868567</td>\n","      <td>0.858712</td>\n","      <td>0.587563</td>\n","      <td>0.480290</td>\n","      <td>0.756536</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:20:32,469]\u001b[0m Trial 35 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:28 < 01:44, 22.34 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.567800</td>\n","      <td>0.521031</td>\n","      <td>0.790728</td>\n","      <td>0.740788</td>\n","      <td>0.750551</td>\n","      <td>0.734522</td>\n","      <td>0.725743</td>\n","      <td>0.737425</td>\n","      <td>0.714425</td>\n","      <td>0.865991</td>\n","      <td>0.885228</td>\n","      <td>0.847572</td>\n","      <td>0.630631</td>\n","      <td>0.580913</td>\n","      <td>0.689655</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.429400</td>\n","      <td>0.488016</td>\n","      <td>0.795450</td>\n","      <td>0.751436</td>\n","      <td>0.749305</td>\n","      <td>0.756530</td>\n","      <td>0.755891</td>\n","      <td>0.806841</td>\n","      <td>0.710993</td>\n","      <td>0.867925</td>\n","      <td>0.860052</td>\n","      <td>0.875943</td>\n","      <td>0.630494</td>\n","      <td>0.602697</td>\n","      <td>0.660978</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-36/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-36/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-36/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:24:06,131]\u001b[0m Trial 36 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/2330 01:18 < 01:18, 14.85 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.573900</td>\n","      <td>0.521524</td>\n","      <td>0.782142</td>\n","      <td>0.733124</td>\n","      <td>0.736594</td>\n","      <td>0.732309</td>\n","      <td>0.723322</td>\n","      <td>0.753521</td>\n","      <td>0.695450</td>\n","      <td>0.859559</td>\n","      <td>0.865605</td>\n","      <td>0.853596</td>\n","      <td>0.616491</td>\n","      <td>0.577801</td>\n","      <td>0.660735</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:25:29,631]\u001b[0m Trial 37 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:25 < 01:42, 22.69 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.576200</td>\n","      <td>0.526576</td>\n","      <td>0.790513</td>\n","      <td>0.738426</td>\n","      <td>0.754522</td>\n","      <td>0.725963</td>\n","      <td>0.725263</td>\n","      <td>0.693159</td>\n","      <td>0.760486</td>\n","      <td>0.866465</td>\n","      <td>0.899667</td>\n","      <td>0.835626</td>\n","      <td>0.623549</td>\n","      <td>0.585062</td>\n","      <td>0.667456</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.417300</td>\n","      <td>0.499708</td>\n","      <td>0.800816</td>\n","      <td>0.759643</td>\n","      <td>0.753968</td>\n","      <td>0.766773</td>\n","      <td>0.762906</td>\n","      <td>0.802817</td>\n","      <td>0.726776</td>\n","      <td>0.872645</td>\n","      <td>0.857460</td>\n","      <td>0.888377</td>\n","      <td>0.643379</td>\n","      <td>0.640041</td>\n","      <td>0.646751</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-38/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-38/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/hyper/results/run-38/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:29:00,042]\u001b[0m Trial 38 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: attention_mask_bert, sentence, input_ids_bert, token_type_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/2330 01:18 < 01:18, 14.86 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.568700</td>\n","      <td>0.527016</td>\n","      <td>0.786649</td>\n","      <td>0.728569</td>\n","      <td>0.760163</td>\n","      <td>0.711768</td>\n","      <td>0.722534</td>\n","      <td>0.711268</td>\n","      <td>0.734164</td>\n","      <td>0.861587</td>\n","      <td>0.912625</td>\n","      <td>0.815955</td>\n","      <td>0.601586</td>\n","      <td>0.511411</td>\n","      <td>0.730370</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 23:30:23,538]\u001b[0m Trial 39 pruned. \u001b[0m\n"]}]},{"cell_type":"code","source":["best_trial"],"metadata":{"id":"KXAQOU-_79fl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644795024229,"user_tz":0,"elapsed":43,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"d1ebf554-21ff-48b0-ad71-c36e9ebda192"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='1', objective=10.032200592981592, hyperparameters={'learning_rate': 2.1221137870433124e-05, 'num_train_epochs': 3, 'seed': 7, 'warmup_steps': 164, 'weight_decay': 0.1290288922573804, 'per_device_train_batch_size': 16})"]},"metadata":{},"execution_count":15}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Distilbert Experiment HateTwit  LLRD HyperParam Search.ipynb","provenance":[{"file_id":"1Otjeoys-uPPK6UK7Hr-3r4U4GrydolVA","timestamp":1644786823387},{"file_id":"1rRPr2DENFy0pCRMIFdatZW-MljEG_50P","timestamp":1644757020448},{"file_id":"11zAh-a_KPmflomNvD1V-XdXzenQf9Q16","timestamp":1644755707832},{"file_id":"1a2My2hhUmHWnDPSOkajnyi9ZeBYHGBAE","timestamp":1644482917164},{"file_id":"1P3VgwZUmhtAsU0t-TfIzNCI19KZV3I_q","timestamp":1643491583972},{"file_id":"1-rZqxPTOtm21puGsZKK_FiKXeyCg7py7","timestamp":1643483337240},{"file_id":"15wylWRQJ1vxlP58OhBHUnY5fdagThMvJ","timestamp":1643406846637},{"file_id":"1B35hajBJY3fRQV7LSjqoqQ3xjdcyCzE3","timestamp":1642365829686},{"file_id":"1b0lCW2Cj6AULiE-Axh2OeZwURHR6pfcD","timestamp":1642333651961},{"file_id":"1RAjFaq-k9CLJQP5eO668UXLc0q-wjuve","timestamp":1642280213414},{"file_id":"1LzZr7GRN1SwVrNyjVX5t5PpCvio8w_kE","timestamp":1642109182548},{"file_id":"1t-qzEyxZtGn_Cnlfg2WtN-0dlgUmxfzx","timestamp":1641764210714},{"file_id":"14bEU8hCGPF8cVUA_BonJQA6Brv89DKUm","timestamp":1641252935713},{"file_id":"18Il3CpGf89tF1Z10iYX8OdfeHxxAX1Ui","timestamp":1639243141142},{"file_id":"1SHCgoRQVGtl9OiWEJGK3JgKkuxxSPy_5","timestamp":1639231968300},{"file_id":"1D-3yvF0nGnaWEZGxQj21R6fsGnyv5a5g","timestamp":1637526185003},{"file_id":"1glXr2JglKPUpN_3AGyk3GjfCtZSDNfsZ","timestamp":1637408594851},{"file_id":"1rc5hX8PBIv7GKvlxLQ35Vwf2jxLsv9f4","timestamp":1634501666668}],"mount_file_id":"1SGMaxSJxfj1cxfEgMFzs6xCMX7pw2m3m","authorship_tag":"ABX9TyPbKmtsRUQGaG4rN2b5AGtG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"54ec6bc361f345ee94f2349bad8c6496":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_98c24086d9fb4a8da4277727e77d21f8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ee66566a36f84abaaf5931632e5a9fc1","IPY_MODEL_9f4f32242b4241cbbf90590588368636","IPY_MODEL_607e5cdd924f478186bb9c79c8ac79e0"]}},"98c24086d9fb4a8da4277727e77d21f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ee66566a36f84abaaf5931632e5a9fc1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dfe9d27046e349cb8f6f01887a09909a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3a5e66e6f4e047ae95a5dacbc6764007"}},"9f4f32242b4241cbbf90590588368636":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9654aa98437b42b1876f6b750b4af9f3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":483,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":483,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cb74caea6e9e4ee987d3d16c529d7931"}},"607e5cdd924f478186bb9c79c8ac79e0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8c1dcb9c42ec4ae083347901acac0566","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483/483 [00:00&lt;00:00, 19.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_420bbb97e8fc46e0a0077384f3a0eba3"}},"dfe9d27046e349cb8f6f01887a09909a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3a5e66e6f4e047ae95a5dacbc6764007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9654aa98437b42b1876f6b750b4af9f3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cb74caea6e9e4ee987d3d16c529d7931":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8c1dcb9c42ec4ae083347901acac0566":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"420bbb97e8fc46e0a0077384f3a0eba3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4d7ce8d1b1084bdc9bc59e65fc5122fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_893112085c064234a626caf811b766bc","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6c6b4b79e0b741a683a8193dfcf9edcf","IPY_MODEL_2f33725ba43c4bf694958f57b7843188","IPY_MODEL_a6987f4969154e4fab7bd8065809e65f"]}},"893112085c064234a626caf811b766bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6c6b4b79e0b741a683a8193dfcf9edcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e1ac130fec0b4ee582c837faf9604c37","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8c737547eee54521b4e46d9c2f679bc3"}},"2f33725ba43c4bf694958f57b7843188":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_35150d7a41464b7b8192aa61ebdb83de","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c159dc9026a4a21a2942aae3633582a"}},"a6987f4969154e4fab7bd8065809e65f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cbc510aca3de4c0986711a564ff0865d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 256M/256M [00:04&lt;00:00, 60.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ad7385706bb748038ea14c67b312ed29"}},"e1ac130fec0b4ee582c837faf9604c37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8c737547eee54521b4e46d9c2f679bc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"35150d7a41464b7b8192aa61ebdb83de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2c159dc9026a4a21a2942aae3633582a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cbc510aca3de4c0986711a564ff0865d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ad7385706bb748038ea14c67b312ed29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}