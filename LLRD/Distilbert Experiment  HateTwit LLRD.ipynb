{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17771,"status":"ok","timestamp":1644778727060,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"F-o6bXOJ18j4","outputId":"dc28ec27-1386-49b0-828c-5d1cddc723a1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.5 MB 14.5 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 6.2 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 75.8 MB/s \n","\u001b[K     |████████████████████████████████| 6.8 MB 75.6 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 75.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 14.0 MB/s \n","\u001b[K     |████████████████████████████████| 311 kB 13.5 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 73.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 75.8 MB/s \n","\u001b[K     |████████████████████████████████| 133 kB 70.0 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 67.2 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 66.4 MB/s \n","\u001b[?25h"]}],"source":["!pip install -qq transformers\n","!pip install -qq sentencepiece\n","!pip install -qq datasets\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9147,"status":"ok","timestamp":1644778736200,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"Rz6wNlu92ge_"},"outputs":[],"source":["import transformers\n","import datasets\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup,Trainer, TrainingArguments\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","import random\n","from textwrap import wrap\n","from datetime import datetime\n","from datasets import load_from_disk\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1644778736201,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"pOQkqPGp2ZTN"},"outputs":[],"source":["from torch import nn"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1644778736201,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"T7IFr4-3TKaA"},"outputs":[],"source":["# the model we gonna train, base uncased BERT\n","# check text classification models here: https://huggingface.co/models?filter=text-classification\n","MODEL_NAME = \"distilbert-base-uncased\"\n","# max sequence length for each document/sentence sample\n","MAX_LENGTH = 64\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE= 6.58e-5\n","WEIGHT_DECAY = 0.289\n","WARMUP_STEPS = 464\n","RANDOM_SEED=22\n","\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1644778736201,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"YoKXcvyo_X47"},"outputs":[],"source":["\n","\n","def set_seed(seed):\n","    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n","       When seed is None, disables deterministic mode.\n","    :param seed: an integer to your choosing\n","    \"\"\"\n","    if seed is not None:\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        np.random.seed(seed)\n","        random.seed(seed)\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","  acc = accuracy_score(labels, preds)\n","  confusion_matrix = classification_report(labels, preds, digits=4,output_dict=True)\n","  return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'hate_f1': confusion_matrix[\"0\"][\"f1-score\"],\n","        'hate_recall': confusion_matrix[\"0\"][\"recall\"],\n","        'hate_precision': confusion_matrix[\"0\"][\"precision\"],\n","        'offensive_f1': confusion_matrix[\"1\"][\"f1-score\"],\n","        'offensive_recall': confusion_matrix[\"1\"][\"recall\"],\n","        'offensive_precision': confusion_matrix[\"1\"][\"precision\"],\n","        'normal_f1': confusion_matrix[\"2\"][\"f1-score\"],\n","        'normal_recall': confusion_matrix[\"2\"][\"recall\"],\n","        'normal_precision': confusion_matrix[\"2\"][\"precision\"],    \n","  }\n","\n","\n","def model_init():\n","  temp_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=3).to(device)\n","  return temp_model\n","\n","# Code modified from Stabilizer library to handle DistilBERT architecture\n","#https://github.com/flowerpot-ai/stabilizer\n","\n","\n","def get_optimizer_parameters_with_llrd(model, peak_lr, multiplicative_factor):\n","    num_encoder_layers = len(model.distilbert.transformer.layer)\n","    # Task specific layer gets the peak_lr\n","    tsl_parameters = [\n","        {\n","            \"params\": [param for name, param in model.named_parameters() if 'distilbert' not in name],\n","            \"param_names\": [name for name, param in model.named_parameters() if 'distilbert' not in name],\n","            \"lr\": peak_lr,\n","            \"name\": \"tsl\",\n","        }\n","    ]\n","\n","    # Starting from the last encoder layer each encoder layers get a lr defined by\n","    # current_layer_lr = prev_layer_lr * multiplicative_factor\n","    # the last encoder layer lr = peak_lr * multiplicative_factor\n","    encoder_parameters = [\n","        {\n","            \"params\": [param for name, param in model.named_parameters() if f\"distilbert.transformer.layer.{layer_num}\" in name],\n","            \"param_names\": [name for name, param in model.named_parameters() if f\"distilbert.transformer.layer.{layer_num}\" in name],\n","            \"lr\": peak_lr * (multiplicative_factor ** (num_encoder_layers - layer_num)),\n","            \"name\": f\"layer_{layer_num}\",\n","        }\n","        for layer_num, layer in enumerate(model.distilbert.transformer.layer)\n","    ]\n","\n","    # Embedding layer gets embedding layer lr = first encoder layer lr * multiplicative_factor\n","    embedding_parameters = [\n","        {\n","            \"params\": [param for name, param in model.named_parameters() if 'embeddings' in name],\n","            \"param_names\": [name for name, param in model.named_parameters() if 'embeddings' in name],\n","            \"lr\": peak_lr * (multiplicative_factor ** (num_encoder_layers + 1)),\n","            \"name\": \"embedding\",\n","        }\n","    ]\n","    return tsl_parameters + encoder_parameters + embedding_parameters\n","\n","\n","def timestamp():\n","    dateTimeObj = datetime.now()\n","    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n","    print(timestampStr)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1644778736202,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"lqKiS7jbkC4x"},"outputs":[],"source":["set_seed(RANDOM_SEED)\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":7576,"status":"ok","timestamp":1644778743758,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"zRoEfiYqQIEO"},"outputs":[],"source":["hatetwit_dataset_dfs = load_from_disk('/content/drive/MyDrive/Dissertation/datasets/hatetwit_'+str(1))\n","train_dataset = hatetwit_dataset_dfs [\"train\"].remove_columns([\"input_ids_bert\",\"attention_mask_bert\",\"token_type_ids_bert\"])\n","eval_dataset = hatetwit_dataset_dfs [\"validation\"].remove_columns([\"input_ids_bert\",\"attention_mask_bert\",\"token_type_ids_bert\"])\n","test_dataset = hatetwit_dataset_dfs [\"test\"].remove_columns([\"input_ids_bert\",\"attention_mask_bert\",\"token_type_ids_bert\"])"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1644778743761,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"aum4jWZzXdgX"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Dissertation/disbert_hate_llrd/results',          # output directory\n","    num_train_epochs=EPOCHS,              # total number of training epochs\n","    save_strategy =\"epoch\" ,\n","    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n","    warmup_steps = WARMUP_STEPS,\n","    weight_decay= WEIGHT_DECAY,               # strength of weight decay\n","    learning_rate= LEARNING_RATE, \n","    logging_dir='./disbert_hate/logs',     # directory for storing logs\n","    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","    evaluation_strategy=\"epoch\",\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["8fb553b7f63344fc94dd1c9304089f8e","dc7e6178a88f4ff6bd35de44390453fd","e40c6af4815b438fbb7328353b547ecf","d195565a1dd941ee986a7a867adcd254","43fde59d4bb74d6a8e4574728574c029","a55fe51a347c4a8c8d53a9203f1844f9","e1d3c007790c4a6c8c07b3ac6a9d10a9","56de65883e904e60afa954be7643083e","27a037d5015b402eb9ce50a964571fd0","bb32e9435cc748bc804eca25d5b2b7bd","b87cc21082c646cd915c58966bc29309","8632ecb9c6f24afca89709e369d93c9e","6a37765a5b9846d5960b78a68f006202","45e76b14591d4646b1cc59c86295b3ba","9615769f26b64013be04827ff0a322bf","7d23b088dd5f428eb8c882b5dc188c8e","0dbfba224a574ec4b276c8c6b52c3fa7","45c46751417e4ba6b716944d0aae158a","607fb991c6424d00ad3060b3ac9c55ec","125bddd8dd0942f996e94cbd541533ad","5cdacb8019cd4dd99fb866e268d4f8fd","3ac400bbdea64f54924eb2fcb4a74d9c"]},"executionInfo":{"elapsed":21048,"status":"ok","timestamp":1644778764790,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"PLMUUequ6kV6","outputId":"425cf26c-2f82-43ae-f9fe-f648abd5e5ec"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fb553b7f63344fc94dd1c9304089f8e","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8632ecb9c6f24afca89709e369d93c9e","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["model = model_init()\n","trainer_one = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","    #optimizers = (optimizer,scheduler)\n",")\n","trainer_one.create_optimizer()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.9)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":395442,"status":"ok","timestamp":1644779160209,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"jEp3VQACE0Cp","outputId":"f8ea6119-7aa6-49ff-b554-46a8eb17ef12"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 06:29, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.555600</td>\n","      <td>0.529095</td>\n","      <td>0.785791</td>\n","      <td>0.742059</td>\n","      <td>0.740721</td>\n","      <td>0.751317</td>\n","      <td>0.729888</td>\n","      <td>0.816901</td>\n","      <td>0.659626</td>\n","      <td>0.860329</td>\n","      <td>0.842651</td>\n","      <td>0.878764</td>\n","      <td>0.635960</td>\n","      <td>0.594398</td>\n","      <td>0.683771</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.437900</td>\n","      <td>0.481802</td>\n","      <td>0.800816</td>\n","      <td>0.752736</td>\n","      <td>0.760185</td>\n","      <td>0.752808</td>\n","      <td>0.765977</td>\n","      <td>0.819920</td>\n","      <td>0.718695</td>\n","      <td>0.871701</td>\n","      <td>0.880415</td>\n","      <td>0.863158</td>\n","      <td>0.620531</td>\n","      <td>0.558091</td>\n","      <td>0.698701</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.307100</td>\n","      <td>0.523752</td>\n","      <td>0.807684</td>\n","      <td>0.765247</td>\n","      <td>0.765057</td>\n","      <td>0.767240</td>\n","      <td>0.782651</td>\n","      <td>0.816901</td>\n","      <td>0.751156</td>\n","      <td>0.875347</td>\n","      <td>0.874861</td>\n","      <td>0.875834</td>\n","      <td>0.637744</td>\n","      <td>0.609959</td>\n","      <td>0.668182</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660 (score: 0.4818022847175598).\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4658\n","  Batch size = 16\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [292/292 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.7917561185057965,\n"," 'eval_f1': 0.7440340099094213,\n"," 'eval_hate_f1': 0.7636022514071295,\n"," 'eval_hate_precision': 0.7146619841966637,\n"," 'eval_hate_recall': 0.8197381671701913,\n"," 'eval_loss': 0.5254986882209778,\n"," 'eval_normal_f1': 0.607536231884058,\n"," 'eval_normal_precision': 0.6894736842105263,\n"," 'eval_normal_recall': 0.5430051813471503,\n"," 'eval_offensive_f1': 0.8609635464370765,\n"," 'eval_offensive_precision': 0.8517578832910475,\n"," 'eval_offensive_recall': 0.8703703703703703,\n"," 'eval_precision': 0.7519645172327459,\n"," 'eval_recall': 0.7443712396292373,\n"," 'eval_runtime': 3.8567,\n"," 'eval_samples_per_second': 1207.783,\n"," 'eval_steps_per_second': 75.713}"]},"metadata":{},"execution_count":10}],"source":["model = model_init()\n","trainer_alpha = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","    #optimizers = (optimizer,scheduler)\n",")\n","trainer_alpha.create_optimizer()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.85)\n","trainer_alpha.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n","trainer_alpha.train()\n","trainer_alpha.evaluate(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":330017,"status":"ok","timestamp":1644774034581,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"_pNi5sX-FHF0","outputId":"7a19f388-f52e-4e4a-96e7-2b1deccc18ab"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:23, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.560600</td>\n","      <td>0.540648</td>\n","      <td>0.771625</td>\n","      <td>0.728088</td>\n","      <td>0.722939</td>\n","      <td>0.741202</td>\n","      <td>0.720071</td>\n","      <td>0.813883</td>\n","      <td>0.645650</td>\n","      <td>0.848891</td>\n","      <td>0.821548</td>\n","      <td>0.878116</td>\n","      <td>0.615301</td>\n","      <td>0.588174</td>\n","      <td>0.645051</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.458100</td>\n","      <td>0.490049</td>\n","      <td>0.799957</td>\n","      <td>0.752008</td>\n","      <td>0.759881</td>\n","      <td>0.748434</td>\n","      <td>0.759592</td>\n","      <td>0.786720</td>\n","      <td>0.734272</td>\n","      <td>0.871291</td>\n","      <td>0.885968</td>\n","      <td>0.857092</td>\n","      <td>0.625142</td>\n","      <td>0.572614</td>\n","      <td>0.688279</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.349400</td>\n","      <td>0.507668</td>\n","      <td>0.805752</td>\n","      <td>0.762921</td>\n","      <td>0.762188</td>\n","      <td>0.766818</td>\n","      <td>0.765234</td>\n","      <td>0.814889</td>\n","      <td>0.721282</td>\n","      <td>0.875442</td>\n","      <td>0.870418</td>\n","      <td>0.880524</td>\n","      <td>0.648087</td>\n","      <td>0.615145</td>\n","      <td>0.684758</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660 (score: 0.4900488257408142).\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4658\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [292/292 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.791970802919708,\n"," 'eval_f1': 0.7450416408981178,\n"," 'eval_hate_f1': 0.7542087542087542,\n"," 'eval_hate_precision': 0.7219152854511971,\n"," 'eval_hate_recall': 0.7895266868076536,\n"," 'eval_loss': 0.5253387093544006,\n"," 'eval_normal_f1': 0.6192373363688105,\n"," 'eval_normal_precision': 0.6868686868686869,\n"," 'eval_normal_recall': 0.5637305699481865,\n"," 'eval_offensive_f1': 0.8616788321167883,\n"," 'eval_offensive_precision': 0.8492805755395684,\n"," 'eval_offensive_recall': 0.8744444444444445,\n"," 'eval_precision': 0.7526881826198174,\n"," 'eval_recall': 0.7425672337334283,\n"," 'eval_runtime': 3.6689,\n"," 'eval_samples_per_second': 1269.583,\n"," 'eval_steps_per_second': 79.587}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["model = model_init()\n","trainer_beta = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","    #optimizers = (optimizer,scheduler)\n",")\n","trainer_beta.create_optimizer()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.75)\n","trainer_beta.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n","trainer_beta.train()\n","trainer_beta.evaluate(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":330564,"status":"ok","timestamp":1644774365132,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"mqYVK5SPFM8R","outputId":"b197b124-3de2-4088-931d-b0e0c423efbd"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:24, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.560100</td>\n","      <td>0.522034</td>\n","      <td>0.791157</td>\n","      <td>0.744530</td>\n","      <td>0.747597</td>\n","      <td>0.744070</td>\n","      <td>0.730250</td>\n","      <td>0.762575</td>\n","      <td>0.700555</td>\n","      <td>0.865721</td>\n","      <td>0.870048</td>\n","      <td>0.861437</td>\n","      <td>0.637617</td>\n","      <td>0.599585</td>\n","      <td>0.680801</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.430600</td>\n","      <td>0.483114</td>\n","      <td>0.809401</td>\n","      <td>0.761718</td>\n","      <td>0.773184</td>\n","      <td>0.755761</td>\n","      <td>0.774951</td>\n","      <td>0.796781</td>\n","      <td>0.754286</td>\n","      <td>0.877961</td>\n","      <td>0.898926</td>\n","      <td>0.857951</td>\n","      <td>0.632243</td>\n","      <td>0.571577</td>\n","      <td>0.707317</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.283900</td>\n","      <td>0.552262</td>\n","      <td>0.811118</td>\n","      <td>0.769713</td>\n","      <td>0.768770</td>\n","      <td>0.772498</td>\n","      <td>0.784822</td>\n","      <td>0.821932</td>\n","      <td>0.750919</td>\n","      <td>0.877831</td>\n","      <td>0.875231</td>\n","      <td>0.880447</td>\n","      <td>0.646486</td>\n","      <td>0.620332</td>\n","      <td>0.674944</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660 (score: 0.48311448097229004).\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: __index_level_0__, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4658\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [292/292 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.7990553885787892,\n"," 'eval_f1': 0.7532255278894512,\n"," 'eval_hate_f1': 0.7676767676767677,\n"," 'eval_hate_precision': 0.7348066298342542,\n"," 'eval_hate_recall': 0.8036253776435045,\n"," 'eval_loss': 0.5167168974876404,\n"," 'eval_normal_f1': 0.6264367816091954,\n"," 'eval_normal_precision': 0.7032258064516129,\n"," 'eval_normal_recall': 0.5647668393782384,\n"," 'eval_offensive_f1': 0.8655630343823904,\n"," 'eval_offensive_precision': 0.8505541651769754,\n"," 'eval_offensive_recall': 0.8811111111111111,\n"," 'eval_precision': 0.7628622004876141,\n"," 'eval_recall': 0.7498344427109513,\n"," 'eval_runtime': 3.7025,\n"," 'eval_samples_per_second': 1258.077,\n"," 'eval_steps_per_second': 78.866}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["model = model_init()\n","trainer_theta = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","    #optimizers = (optimizer,scheduler)\n",")\n","trainer_theta.create_optimizer()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.95)\n","trainer_theta.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)\n","trainer_theta.train()\n","trainer_theta.evaluate(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJaSiZfcEsXa"},"outputs":[],"source":["model = model_init()\n","trainer_one = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n","    #optimizers = (optimizer,scheduler)\n",")\n","trainer_one.create_optimizer()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.9)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1644757531631,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"gMg0LA1c9S5U","outputId":"93813b90-78c0-47e7-9491-c3348653a968"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["trainer_one.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":814},"executionInfo":{"elapsed":333415,"status":"ok","timestamp":1644757865030,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"UbOulfz660XD","outputId":"6edd0844-46f4-43a2-ebfa-5def1dc98c2a"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:33, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.559100</td>\n","      <td>0.534393</td>\n","      <td>0.781713</td>\n","      <td>0.740093</td>\n","      <td>0.735141</td>\n","      <td>0.751714</td>\n","      <td>0.724572</td>\n","      <td>0.809859</td>\n","      <td>0.655537</td>\n","      <td>0.856053</td>\n","      <td>0.831174</td>\n","      <td>0.882469</td>\n","      <td>0.639654</td>\n","      <td>0.614108</td>\n","      <td>0.667418</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.437800</td>\n","      <td>0.480476</td>\n","      <td>0.805538</td>\n","      <td>0.755238</td>\n","      <td>0.768154</td>\n","      <td>0.750819</td>\n","      <td>0.763045</td>\n","      <td>0.801811</td>\n","      <td>0.727854</td>\n","      <td>0.877377</td>\n","      <td>0.896705</td>\n","      <td>0.858865</td>\n","      <td>0.625293</td>\n","      <td>0.553942</td>\n","      <td>0.717742</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.296300</td>\n","      <td>0.542798</td>\n","      <td>0.807040</td>\n","      <td>0.763870</td>\n","      <td>0.762546</td>\n","      <td>0.767495</td>\n","      <td>0.778255</td>\n","      <td>0.820926</td>\n","      <td>0.739801</td>\n","      <td>0.876697</td>\n","      <td>0.872640</td>\n","      <td>0.880792</td>\n","      <td>0.636659</td>\n","      <td>0.608921</td>\n","      <td>0.667045</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660 (score: 0.4804760217666626).\n"]},{"data":{"text/plain":["TrainOutput(global_step=6990, training_loss=0.46294733574120955, metrics={'train_runtime': 333.3997, 'train_samples_per_second': 335.318, 'train_steps_per_second': 20.966, 'total_flos': 1851182116709760.0, 'train_loss': 0.46294733574120955, 'epoch': 3.0})"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["trainer_one.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1644757865031,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"f6VXroG67pHI","outputId":"3738ada5-eda7-4259-8288-c8b0bbde4dc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["13-Feb-2022 (13:11:04.588831)\n"]}],"source":["timestamp()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":5747,"status":"ok","timestamp":1644757870768,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"1G57aLkv7OhH","outputId":"7a44c0de-9e48-4540-9fc5-4e3da7bba525"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4658\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [292/292 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.7945470158866467,\n"," 'eval_f1': 0.7443414496025772,\n"," 'eval_hate_f1': 0.7698945349952061,\n"," 'eval_hate_precision': 0.7346752058554438,\n"," 'eval_hate_recall': 0.8086606243705942,\n"," 'eval_loss': 0.5204463005065918,\n"," 'eval_normal_f1': 0.5984804208065458,\n"," 'eval_normal_precision': 0.6863270777479893,\n"," 'eval_normal_recall': 0.5305699481865285,\n"," 'eval_offensive_f1': 0.8646493930059794,\n"," 'eval_offensive_precision': 0.846399432422845,\n"," 'eval_offensive_recall': 0.8837037037037037,\n"," 'eval_precision': 0.7558005720087593,\n"," 'eval_recall': 0.7409780920869421,\n"," 'eval_runtime': 5.9248,\n"," 'eval_samples_per_second': 786.186,\n"," 'eval_steps_per_second': 49.284}"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["trainer_one.evaluate(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1644757870768,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"f5hWnjoncQKw","outputId":"0e10c774-a2e0-4619-a672-8bc9c3c93681"},"outputs":[{"name":"stdout","output_type":"stream","text":["13-Feb-2022 (13:11:10.533979)\n"]}],"source":["timestamp()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1588,"status":"ok","timestamp":1644757872353,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"Zkk9DWHntNkD","outputId":"d60fb512-73e1-4245-f53d-c28b200d2e3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["model =model_init()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.8)\n","trainer_two = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",")\n","trainer_two.create_optimizer()\n","trainer_two.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":814},"executionInfo":{"elapsed":332926,"status":"ok","timestamp":1644758205265,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"uRRdsV6RtQ7b","outputId":"699a9edb-716c-48bc-a9ad-a012dc8194f9"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:33, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.558300</td>\n","      <td>0.529969</td>\n","      <td>0.778064</td>\n","      <td>0.734772</td>\n","      <td>0.728670</td>\n","      <td>0.746141</td>\n","      <td>0.725535</td>\n","      <td>0.801811</td>\n","      <td>0.662510</td>\n","      <td>0.855183</td>\n","      <td>0.830803</td>\n","      <td>0.881037</td>\n","      <td>0.623599</td>\n","      <td>0.605809</td>\n","      <td>0.642464</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.450300</td>\n","      <td>0.486710</td>\n","      <td>0.801889</td>\n","      <td>0.754699</td>\n","      <td>0.764162</td>\n","      <td>0.750584</td>\n","      <td>0.764165</td>\n","      <td>0.793763</td>\n","      <td>0.736695</td>\n","      <td>0.871003</td>\n","      <td>0.887449</td>\n","      <td>0.855155</td>\n","      <td>0.628931</td>\n","      <td>0.570539</td>\n","      <td>0.700637</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.329500</td>\n","      <td>0.515521</td>\n","      <td>0.803821</td>\n","      <td>0.760911</td>\n","      <td>0.759127</td>\n","      <td>0.765432</td>\n","      <td>0.771780</td>\n","      <td>0.819920</td>\n","      <td>0.728980</td>\n","      <td>0.873602</td>\n","      <td>0.867456</td>\n","      <td>0.879835</td>\n","      <td>0.637351</td>\n","      <td>0.608921</td>\n","      <td>0.668565</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660 (score: 0.48670971393585205).\n"]},{"data":{"text/plain":["TrainOutput(global_step=6990, training_loss=0.47832156987660945, metrics={'train_runtime': 333.1142, 'train_samples_per_second': 335.606, 'train_steps_per_second': 20.984, 'total_flos': 1851182116709760.0, 'train_loss': 0.47832156987660945, 'epoch': 3.0})"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["trainer_two.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":3999,"status":"ok","timestamp":1644758209249,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"PLDtxXOutVn6","outputId":"d0ce1f63-98d3-4576-915a-b44f41ba3757"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4658\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [292/292 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.7934735938170889,\n"," 'eval_f1': 0.7467309178764472,\n"," 'eval_hate_f1': 0.7621776504297995,\n"," 'eval_hate_precision': 0.7247956403269755,\n"," 'eval_hate_recall': 0.8036253776435045,\n"," 'eval_loss': 0.5242156386375427,\n"," 'eval_normal_f1': 0.6161790017211703,\n"," 'eval_normal_precision': 0.6902313624678663,\n"," 'eval_normal_recall': 0.5564766839378238,\n"," 'eval_offensive_f1': 0.8618361014783721,\n"," 'eval_offensive_precision': 0.8495861820798849,\n"," 'eval_offensive_recall': 0.8744444444444445,\n"," 'eval_precision': 0.7548710616249089,\n"," 'eval_recall': 0.7448488353419243,\n"," 'eval_runtime': 3.7187,\n"," 'eval_samples_per_second': 1252.603,\n"," 'eval_steps_per_second': 78.523}"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["trainer_two.evaluate(test_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1146,"status":"ok","timestamp":1644758210391,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"yNg68g8QtYf4","outputId":"57bc08a8-9e5a-4bac-fab4-96567434c736"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["model = model_init()\n","parameters = get_optimizer_parameters_with_llrd(model, LEARNING_RATE, 0.7)\n","trainer_three = Trainer(\n","    model =model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=eval_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",")\n","\n","trainer_three.create_optimizer()\n","trainer_three.optimizer = AdamW(parameters, lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":814},"executionInfo":{"elapsed":332167,"status":"ok","timestamp":1644758542553,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"l1N3dwMztdXt","outputId":"82f04c23-e1e3-498f-c651-f4d1ac13506a"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence.\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:32, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.562000</td>\n","      <td>0.544932</td>\n","      <td>0.772483</td>\n","      <td>0.728405</td>\n","      <td>0.723399</td>\n","      <td>0.740677</td>\n","      <td>0.715884</td>\n","      <td>0.804829</td>\n","      <td>0.644641</td>\n","      <td>0.851031</td>\n","      <td>0.824880</td>\n","      <td>0.878895</td>\n","      <td>0.618300</td>\n","      <td>0.592324</td>\n","      <td>0.646659</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.467400</td>\n","      <td>0.495549</td>\n","      <td>0.793303</td>\n","      <td>0.743340</td>\n","      <td>0.754154</td>\n","      <td>0.738063</td>\n","      <td>0.750853</td>\n","      <td>0.774648</td>\n","      <td>0.728477</td>\n","      <td>0.865726</td>\n","      <td>0.885598</td>\n","      <td>0.846726</td>\n","      <td>0.613441</td>\n","      <td>0.553942</td>\n","      <td>0.687259</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.375400</td>\n","      <td>0.501198</td>\n","      <td>0.799742</td>\n","      <td>0.755580</td>\n","      <td>0.754741</td>\n","      <td>0.760058</td>\n","      <td>0.758102</td>\n","      <td>0.811871</td>\n","      <td>0.711013</td>\n","      <td>0.871575</td>\n","      <td>0.865605</td>\n","      <td>0.877628</td>\n","      <td>0.637061</td>\n","      <td>0.602697</td>\n","      <td>0.675581</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_llrd/results/checkpoint-4660 (score: 0.4955490827560425).\n"]},{"data":{"text/plain":["TrainOutput(global_step=6990, training_loss=0.49917930635771524, metrics={'train_runtime': 332.078, 'train_samples_per_second': 336.653, 'train_steps_per_second': 21.049, 'total_flos': 1851182116709760.0, 'train_loss': 0.49917930635771524, 'epoch': 3.0})"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["trainer_three.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":4079,"status":"ok","timestamp":1644758546600,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"JR7VmTmNtexn","outputId":"98445890-9820-4fa7-9222-be3ce4fdfe6e"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: sentence, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 4658\n","  Batch size = 16\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='292' max='292' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [292/292 00:03]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'epoch': 3.0,\n"," 'eval_accuracy': 0.7887505367110348,\n"," 'eval_f1': 0.7395305948926852,\n"," 'eval_hate_f1': 0.7405614714424009,\n"," 'eval_hate_precision': 0.7129543336439889,\n"," 'eval_hate_recall': 0.770392749244713,\n"," 'eval_loss': 0.5288717746734619,\n"," 'eval_normal_f1': 0.6171560161197467,\n"," 'eval_normal_precision': 0.694300518134715,\n"," 'eval_normal_recall': 0.555440414507772,\n"," 'eval_offensive_f1': 0.8608742971159079,\n"," 'eval_offensive_precision': 0.8435833629576964,\n"," 'eval_offensive_recall': 0.8788888888888889,\n"," 'eval_precision': 0.7502794049121334,\n"," 'eval_recall': 0.7349073508804581,\n"," 'eval_runtime': 3.779,\n"," 'eval_samples_per_second': 1232.608,\n"," 'eval_steps_per_second': 77.27}"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["trainer_three.evaluate(test_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Distilbert Experiment  HateTwit LLRD.ipynb","provenance":[{"file_id":"1-rZqxPTOtm21puGsZKK_FiKXeyCg7py7","timestamp":1644163561234},{"file_id":"15wylWRQJ1vxlP58OhBHUnY5fdagThMvJ","timestamp":1643406846637},{"file_id":"1B35hajBJY3fRQV7LSjqoqQ3xjdcyCzE3","timestamp":1642365829686},{"file_id":"1b0lCW2Cj6AULiE-Axh2OeZwURHR6pfcD","timestamp":1642333651961},{"file_id":"1RAjFaq-k9CLJQP5eO668UXLc0q-wjuve","timestamp":1642280213414},{"file_id":"1LzZr7GRN1SwVrNyjVX5t5PpCvio8w_kE","timestamp":1642109182548},{"file_id":"1t-qzEyxZtGn_Cnlfg2WtN-0dlgUmxfzx","timestamp":1641764210714},{"file_id":"14bEU8hCGPF8cVUA_BonJQA6Brv89DKUm","timestamp":1641252935713},{"file_id":"18Il3CpGf89tF1Z10iYX8OdfeHxxAX1Ui","timestamp":1639243141142},{"file_id":"1SHCgoRQVGtl9OiWEJGK3JgKkuxxSPy_5","timestamp":1639231968300},{"file_id":"1D-3yvF0nGnaWEZGxQj21R6fsGnyv5a5g","timestamp":1637526185003},{"file_id":"1glXr2JglKPUpN_3AGyk3GjfCtZSDNfsZ","timestamp":1637408594851},{"file_id":"1rc5hX8PBIv7GKvlxLQ35Vwf2jxLsv9f4","timestamp":1634501666668}],"mount_file_id":"16nhJIX25usU3-INiry6hlKgeD9v2SqdP","authorship_tag":"ABX9TyNG4dbQmD0urvEsAzsdO2sn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8fb553b7f63344fc94dd1c9304089f8e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc7e6178a88f4ff6bd35de44390453fd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_e40c6af4815b438fbb7328353b547ecf","IPY_MODEL_d195565a1dd941ee986a7a867adcd254","IPY_MODEL_43fde59d4bb74d6a8e4574728574c029"]}},"dc7e6178a88f4ff6bd35de44390453fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e40c6af4815b438fbb7328353b547ecf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a55fe51a347c4a8c8d53a9203f1844f9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e1d3c007790c4a6c8c07b3ac6a9d10a9"}},"d195565a1dd941ee986a7a867adcd254":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_56de65883e904e60afa954be7643083e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":483,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":483,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_27a037d5015b402eb9ce50a964571fd0"}},"43fde59d4bb74d6a8e4574728574c029":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bb32e9435cc748bc804eca25d5b2b7bd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483/483 [00:00&lt;00:00, 17.0kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b87cc21082c646cd915c58966bc29309"}},"a55fe51a347c4a8c8d53a9203f1844f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e1d3c007790c4a6c8c07b3ac6a9d10a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"56de65883e904e60afa954be7643083e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"27a037d5015b402eb9ce50a964571fd0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bb32e9435cc748bc804eca25d5b2b7bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b87cc21082c646cd915c58966bc29309":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8632ecb9c6f24afca89709e369d93c9e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_6a37765a5b9846d5960b78a68f006202","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_45e76b14591d4646b1cc59c86295b3ba","IPY_MODEL_9615769f26b64013be04827ff0a322bf","IPY_MODEL_7d23b088dd5f428eb8c882b5dc188c8e"]}},"6a37765a5b9846d5960b78a68f006202":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"45e76b14591d4646b1cc59c86295b3ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0dbfba224a574ec4b276c8c6b52c3fa7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45c46751417e4ba6b716944d0aae158a"}},"9615769f26b64013be04827ff0a322bf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_607fb991c6424d00ad3060b3ac9c55ec","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_125bddd8dd0942f996e94cbd541533ad"}},"7d23b088dd5f428eb8c882b5dc188c8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5cdacb8019cd4dd99fb866e268d4f8fd","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 256M/256M [00:05&lt;00:00, 46.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ac400bbdea64f54924eb2fcb4a74d9c"}},"0dbfba224a574ec4b276c8c6b52c3fa7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"45c46751417e4ba6b716944d0aae158a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"607fb991c6424d00ad3060b3ac9c55ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"125bddd8dd0942f996e94cbd541533ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5cdacb8019cd4dd99fb866e268d4f8fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3ac400bbdea64f54924eb2fcb4a74d9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}