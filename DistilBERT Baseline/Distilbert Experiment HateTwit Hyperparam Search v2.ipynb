{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":24169,"status":"ok","timestamp":1644669799651,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"F-o6bXOJ18j4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"665322e4-ef10-4731-c3b9-26d1fc1dfb66"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.5 MB 14.4 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 53.3 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 57.9 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n","\u001b[K     |████████████████████████████████| 6.8 MB 75.5 MB/s \n","\u001b[K     |████████████████████████████████| 308 kB 13.2 MB/s \n","\u001b[K     |████████████████████████████████| 210 kB 69.4 MB/s \n","\u001b[K     |████████████████████████████████| 80 kB 10.8 MB/s \n","\u001b[K     |████████████████████████████████| 75 kB 5.0 MB/s \n","\u001b[K     |████████████████████████████████| 149 kB 87.0 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 7.8 MB/s \n","\u001b[K     |████████████████████████████████| 113 kB 83.6 MB/s \n","\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.2 MB 14.4 MB/s \n","\u001b[K     |████████████████████████████████| 311 kB 14.1 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 71.4 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 63.4 MB/s \n","\u001b[K     |████████████████████████████████| 133 kB 82.5 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 83.9 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 95.8 MB/s \n","\u001b[?25h"]}],"source":["!pip install -qq transformers\n","!pip install -qq optuna\n","!pip install -qq sentencepiece\n","!pip install -qq datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rz6wNlu92ge_"},"outputs":[],"source":["import transformers\n","import datasets\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification,AdamW, get_linear_schedule_with_warmup,Trainer, TrainingArguments\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","import random\n","from textwrap import wrap\n","from datetime import datetime\n","from datasets import load_from_disk\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7IFr4-3TKaA"},"outputs":[],"source":["# the model we gonna train, base uncased BERT\n","# check text classification models here: https://huggingface.co/models?filter=text-classification\n","MODEL_NAME = \"distilbert-base-uncased\"\n","# max sequence length for each document/sentence sample\n","MAX_LENGTH = 64\n","BATCH_SIZE = 16\n","EPOCHS = 5\n","LEARNING_RATE= 1e-5\n","WEIGHT_DECAY = 1e-16\n","RANDOM_SEED=5\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoKXcvyo_X47"},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n","       When seed is None, disables deterministic mode.\n","    :param seed: an integer to your choosing\n","    \"\"\"\n","    if seed is not None:\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        np.random.seed(seed)\n","        random.seed(seed)\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","  acc = accuracy_score(labels, preds)\n","  confusion_matrix = classification_report(labels, preds, digits=4,output_dict=True)\n","  return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'hate_f1': confusion_matrix[\"0\"][\"f1-score\"],\n","        'hate_recall': confusion_matrix[\"0\"][\"recall\"],\n","        'hate_precision': confusion_matrix[\"0\"][\"precision\"],\n","        'offensive_f1': confusion_matrix[\"1\"][\"f1-score\"],\n","        'offensive_recall': confusion_matrix[\"1\"][\"recall\"],\n","        'offensive_precision': confusion_matrix[\"1\"][\"precision\"],\n","        'normal_f1': confusion_matrix[\"2\"][\"f1-score\"],\n","        'normal_recall': confusion_matrix[\"2\"][\"recall\"],\n","        'normal_precision': confusion_matrix[\"2\"][\"precision\"],    \n","  }\n","\n","def model_init():\n","  temp_model =  AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,num_labels=3).to(device)\n","  return temp_model\n","\n","def timestamp():\n","    dateTimeObj = datetime.now()\n","    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n","    print(timestampStr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqKiS7jbkC4x"},"outputs":[],"source":["set_seed(RANDOM_SEED)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["07c939dd2f90489599626a9353eb99dc","fcd9fca81fec444090f5d98e258e24ac","25956e8d5602402cb6bf2459102805f4","b1134b067026461cb0afca0966f946a9","2f32feb7f4304f60bf8a8a9bad4efbc6","b7eb6c1f35284f12bf233339a68a22e3","5b3309376c124e15a7bf6e8e3629d980","6b88e578081e465ba39ebaf827575fe9","4992fe294d39452193e94b4771cabca1","dcad80517c7947f28c57f29f87758db8","7dc0f6f7b0a143a5a0e4b4fe0bf27779","0395716dba054989b8c36c800ebb93bf","5c2b51a6b8964500a9a1079f5736979a","2424545ef5e84c4e88ef675411302ec8","ea12fce919374940b22958ffa2cc998a","de035539dbb64c15bb5fcf85caa4bdad","e05916329ab049428ba11b82ccf2242d","19dd7a617c014eb18d471421390b5395","725f5646dcc8462883e5f88c8ab5bad5","f4e4276d1a9c4d4297497dcc75ebf204","1ce07ed770fb4f509845d9714c513531","4cb0c1b4667f408e8f3b902c46237fcb","febec3a100c84748bc1d27bb73356c22","b20347d73b1d4e159de10d8f77b2c0c4","a51691f046bb49e68af5edcb547e84b0","51e1fadeb0b341438a7f7c7cc6074a07","7f031938f0af4954b018ba5d5b8ec6b4","576ce9b33e314368b03c7e9b86a8e38a","68a93365e47a46dc9d0080e6451b1167","78ffa19b935e4c4284091a87dbb2f8f9","5ad82b4c2c7e4a4287e1581ab660b6ad","331f5ef209e14e928ec6f2be11137e29","a1a155b54b0949318d4084b6fe39ebe8","d6328975758d4e3a9638bb27e7c1f42b","7a4b66dad21c4cc1b7381ed213e54d78","6f4b8751dd7a43f5aaa1d97ec21d2166","dd2af74042e04d4389716c5e4828bfaf","a1bb2d2c2cec401787684fd94f85dc91","4cf68ef0813d438c8295edc2e3da6554","15e40bd1507a4c1e853c62a2500f56ce","7669282abf174f0197ede44cbed55c35","1d0cfef1495e4332bfde6cea5a2553c7","a5f740c35b8e4422b0ae4206d321e36f","f9a5c6d4b69e4963934e93441c4a0a0b"]},"executionInfo":{"elapsed":5806,"status":"ok","timestamp":1644669814407,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"AMEUIo294iAd","outputId":"71cb73d3-233b-4f91-9a18-89b3c79375c0"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07c939dd2f90489599626a9353eb99dc","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0395716dba054989b8c36c800ebb93bf","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"febec3a100c84748bc1d27bb73356c22","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d6328975758d4e3a9638bb27e7c1f42b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{}}],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgpuNCCzJtP2"},"outputs":[],"source":["dataset_dfs = load_from_disk('/content/drive/MyDrive/Dissertation/datasets/hatetwit_'+str(1))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aum4jWZzXdgX"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results',          # output directory\n","    num_train_epochs=EPOCHS,              # total number of training epochs\n","    save_strategy =\"epoch\" ,\n","    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n","    weight_decay= WEIGHT_DECAY,               # strength of weight decay\n","    learning_rate= LEARNING_RATE, \n","    logging_dir='./disbert_hate/hyper/logs',     # directory for storing logs\n","    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","    evaluation_strategy=\"epoch\",\n","    #eval_steps = 500     # evaluate each `logging_steps`\n",")"]},{"cell_type":"markdown","metadata":{"id":"ovv4Qtmf53q3"},"source":["Each argument is explained in the code comments, I've specified 16 as training batch size, that's because it's the maximum I can get to fit in a Google Colab environment's memory.\n","\n","You can also tweak other parameters, such as increasing the number of epochs for better training.\n","\n","***Note that load best model at end set to False as issue with saving steps value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":833,"referenced_widgets":["2710d26d035d4786b012865fe0fc7e5e","2340e2e266504a02ac967839db4f5fa8","d5541251391a4a3a99e690f24a5b602e","8ed3695997a441048aff7ead4fd84206","6bf31739e793411eb0f84d605eca14da","aafb830784d549c3b618554f45b68387","45cba0faf8fd4deabcdcd754ec5fab32","d88985d11a744949b0cc32136f21a22d","79069103dd4644a084f2c4ce629afc08","82fbf863605440099d0c8fd3a7a68c5f","7b440548641145acb0018d05cde23aa9"]},"executionInfo":{"elapsed":19430,"status":"ok","timestamp":1644672756774,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"n1mglCo6lEK5","outputId":"a0642e61-9907-4081-bf0a-53b3f9105f8e"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptt0_bj7g\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2710d26d035d4786b012865fe0fc7e5e","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["storing https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","creating metadata file for /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["hyper_trainer = Trainer(\n","    model_init=model_init,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=dataset_dfs['train'],         # training dataset\n","    eval_dataset=dataset_dfs['validation'],          # evaluation dataset\n","    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",")"]},{"cell_type":"code","source":["def hp_space_optuna(trial) :\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n","        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n","        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.3),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [ 8, 16, 32, 64]),\n","    }"],"metadata":{"id":"GVeNVUt0g5dN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":8261189,"status":"ok","timestamp":1644681017957,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"UvkgPJczDv8F","outputId":"26b90274-50ae-42d9-b9c2-b2fee3cc05b0"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 13:32:41,386]\u001b[0m A new study created in memory with name: no-name-60724ab9-d73e-4387-8c41-db9ac9239eee\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3495' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3495/3495 04:17, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.655000</td>\n","      <td>0.595284</td>\n","      <td>0.761322</td>\n","      <td>0.757030</td>\n","      <td>0.756396</td>\n","      <td>0.761322</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.574900</td>\n","      <td>0.561695</td>\n","      <td>0.772269</td>\n","      <td>0.770109</td>\n","      <td>0.770437</td>\n","      <td>0.772269</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.544600</td>\n","      <td>0.552900</td>\n","      <td>0.775274</td>\n","      <td>0.771375</td>\n","      <td>0.770792</td>\n","      <td>0.775274</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6783    0.7254    0.7010       994\n","           1     0.8235    0.8601    0.8414      2701\n","           2     0.6490    0.5218    0.5785       964\n","\n","    accuracy                         0.7613      4659\n","   macro avg     0.7169    0.7024    0.7070      4659\n","weighted avg     0.7564    0.7613    0.7570      4659\n","\n","0.7570295544860335\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6781    0.7565    0.7152       994\n","           1     0.8478    0.8560    0.8519      2701\n","           2     0.6488    0.5539    0.5976       964\n","\n","    accuracy                         0.7723      4659\n","   macro avg     0.7249    0.7222    0.7216      4659\n","weighted avg     0.7704    0.7723    0.7701      4659\n","\n","0.7701089650230784\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6907    0.7435    0.7161       994\n","           1     0.8402    0.8719    0.8557      2701\n","           2     0.6590    0.5373    0.5920       964\n","\n","    accuracy                         0.7753      4659\n","   macro avg     0.7300    0.7176    0.7213      4659\n","weighted avg     0.7708    0.7753    0.7714      4659\n","\n","0.771374534118344\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-3495\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-3495/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-3495/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-0/checkpoint-3495 (score: 0.5529000163078308).\n","\u001b[32m[I 2022-02-12 13:37:00,939]\u001b[0m Trial 0 finished with value: 3.0927140278755667 and parameters: {'learning_rate': 5.163945255711368e-06, 'num_train_epochs': 3, 'seed': 33, 'warmup_steps': 172, 'weight_decay': 0.1163929664963968, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 3.0927140278755667.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 13977\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='13977' max='13977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13977/13977 08:16, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.592900</td>\n","      <td>0.558939</td>\n","      <td>0.770337</td>\n","      <td>0.772311</td>\n","      <td>0.777818</td>\n","      <td>0.770337</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.478700</td>\n","      <td>0.553991</td>\n","      <td>0.796737</td>\n","      <td>0.794592</td>\n","      <td>0.793188</td>\n","      <td>0.796737</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.358900</td>\n","      <td>0.634564</td>\n","      <td>0.802533</td>\n","      <td>0.802054</td>\n","      <td>0.801775</td>\n","      <td>0.802533</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6457    0.7757    0.7048       994\n","           1     0.8732    0.8238    0.8478      2701\n","           2     0.6467    0.6151    0.6305       964\n","\n","    accuracy                         0.7703      4659\n","   macro avg     0.7219    0.7382    0.7277      4659\n","weighted avg     0.7778    0.7703    0.7723      4659\n","\n","0.7723110129090739\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7487    0.7404    0.7446       994\n","           1     0.8570    0.8852    0.8709      2701\n","           2     0.6603    0.6068    0.6324       964\n","\n","    accuracy                         0.7967      4659\n","   macro avg     0.7553    0.7442    0.7493      4659\n","weighted avg     0.7932    0.7967    0.7946      4659\n","\n","0.7945923621587604\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-9318/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7565    0.7847    0.7704       994\n","           1     0.8729    0.8723    0.8726      2701\n","           2     0.6491    0.6255    0.6371       964\n","\n","    accuracy                         0.8025      4659\n","   macro avg     0.7595    0.7608    0.7600      4659\n","weighted avg     0.8018    0.8025    0.8021      4659\n","\n","0.8020540267503339\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-13977\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-13977/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-13977/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-1/checkpoint-9318 (score: 0.5539913773536682).\n","\u001b[32m[I 2022-02-12 13:45:19,021]\u001b[0m Trial 1 finished with value: 3.208894886927413 and parameters: {'learning_rate': 7.113930006942247e-05, 'num_train_epochs': 3, 'seed': 21, 'warmup_steps': 499, 'weight_decay': 0.005885721438242597, 'per_device_train_batch_size': 8}. Best is trial 1 with value: 3.208894886927413.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 05:44, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.762400</td>\n","      <td>0.669116</td>\n","      <td>0.718180</td>\n","      <td>0.706584</td>\n","      <td>0.704843</td>\n","      <td>0.718180</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.663600</td>\n","      <td>0.617361</td>\n","      <td>0.746941</td>\n","      <td>0.742998</td>\n","      <td>0.741183</td>\n","      <td>0.746941</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.621500</td>\n","      <td>0.601205</td>\n","      <td>0.753381</td>\n","      <td>0.748042</td>\n","      <td>0.747657</td>\n","      <td>0.753381</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.590600</td>\n","      <td>0.595284</td>\n","      <td>0.756600</td>\n","      <td>0.751303</td>\n","      <td>0.750373</td>\n","      <td>0.756600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6432    0.6258    0.6344       994\n","           1     0.7733    0.8663    0.8172      2701\n","           2     0.5766    0.3983    0.4712       964\n","\n","    accuracy                         0.7182      4659\n","   macro avg     0.6644    0.6301    0.6409      4659\n","weighted avg     0.7048    0.7182    0.7066      4659\n","\n","0.7065841685395443\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6657    0.6871    0.6762       994\n","           1     0.8176    0.8530    0.8349      2701\n","           2     0.6049    0.5114    0.5542       964\n","\n","    accuracy                         0.7469      4659\n","   macro avg     0.6961    0.6839    0.6885      4659\n","weighted avg     0.7412    0.7469    0.7430      4659\n","\n","0.7429980128234656\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6648    0.7203    0.6915       994\n","           1     0.8171    0.8586    0.8373      2701\n","           2     0.6384    0.4927    0.5562       964\n","\n","    accuracy                         0.7534      4659\n","   macro avg     0.7068    0.6905    0.6950      4659\n","weighted avg     0.7477    0.7534    0.7480      4659\n","\n","0.7480424744087703\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-3495\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-3495/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-3495/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6739    0.7193    0.6959       994\n","           1     0.8202    0.8630    0.8411      2701\n","           2     0.6336    0.4969    0.5570       964\n","\n","    accuracy                         0.7566      4659\n","   macro avg     0.7092    0.6931    0.6980      4659\n","weighted avg     0.7504    0.7566    0.7513      4659\n","\n","0.7513027172473458\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-2/checkpoint-4660 (score: 0.5952842235565186).\n","\u001b[32m[I 2022-02-12 13:51:05,263]\u001b[0m Trial 2 finished with value: 3.0148763973087584 and parameters: {'learning_rate': 1.9885527540532752e-06, 'num_train_epochs': 4, 'seed': 7, 'warmup_steps': 398, 'weight_decay': 0.2021313515600552, 'per_device_train_batch_size': 32}. Best is trial 1 with value: 3.208894886927413.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 03:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.794200</td>\n","      <td>0.593869</td>\n","      <td>0.761966</td>\n","      <td>0.755312</td>\n","      <td>0.757017</td>\n","      <td>0.761966</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.569800</td>\n","      <td>0.544258</td>\n","      <td>0.778064</td>\n","      <td>0.776214</td>\n","      <td>0.775259</td>\n","      <td>0.778064</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.525800</td>\n","      <td>0.535419</td>\n","      <td>0.782357</td>\n","      <td>0.778210</td>\n","      <td>0.778070</td>\n","      <td>0.782357</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6646    0.7555    0.7072       994\n","           1     0.8250    0.8675    0.8457      2701\n","           2     0.6618    0.4730    0.5517       964\n","\n","    accuracy                         0.7620      4659\n","   macro avg     0.7171    0.6987    0.7015      4659\n","weighted avg     0.7570    0.7620    0.7553      4659\n","\n","0.7553122101801825\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7064    0.7384    0.7221       994\n","           1     0.8484    0.8638    0.8560      2701\n","           2     0.6414    0.5788    0.6085       964\n","\n","    accuracy                         0.7781      4659\n","   macro avg     0.7321    0.7270    0.7289      4659\n","weighted avg     0.7753    0.7781    0.7762      4659\n","\n","0.7762140175722042\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7035    0.7686    0.7346       994\n","           1     0.8433    0.8745    0.8586      2701\n","           2     0.6723    0.5384    0.5979       964\n","\n","    accuracy                         0.7824      4659\n","   macro avg     0.7397    0.7272    0.7304      4659\n","weighted avg     0.7781    0.7824    0.7782      4659\n","\n","0.7782096523445967\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-3/checkpoint-1749 (score: 0.5354192852973938).\n","\u001b[32m[I 2022-02-12 13:54:55,504]\u001b[0m Trial 3 finished with value: 3.1209932494383255 and parameters: {'learning_rate': 1.112960505325453e-05, 'num_train_epochs': 3, 'seed': 29, 'warmup_steps': 356, 'weight_decay': 0.28109091759167143, 'per_device_train_batch_size': 64}. Best is trial 1 with value: 3.208894886927413.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9318' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9318/9318 05:33, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.559100</td>\n","      <td>0.531514</td>\n","      <td>0.781284</td>\n","      <td>0.780291</td>\n","      <td>0.779593</td>\n","      <td>0.781284</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.432000</td>\n","      <td>0.540284</td>\n","      <td>0.804679</td>\n","      <td>0.802990</td>\n","      <td>0.802461</td>\n","      <td>0.804679</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7286    0.6942    0.7110       994\n","           1     0.8503    0.8682    0.8591      2701\n","           2     0.6342    0.6276    0.6309       964\n","\n","    accuracy                         0.7813      4659\n","   macro avg     0.7377    0.7300    0.7337      4659\n","weighted avg     0.7796    0.7813    0.7803      4659\n","\n","0.7802912548292993\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7443    0.7938    0.7683       994\n","           1     0.8672    0.8778    0.8725      2701\n","           2     0.6809    0.6110    0.6441       964\n","\n","    accuracy                         0.8047      4659\n","   macro avg     0.7642    0.7609    0.7616      4659\n","weighted avg     0.8025    0.8047    0.8030      4659\n","\n","0.802990499424248\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-9318/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-4/checkpoint-4659 (score: 0.5315142869949341).\n","\u001b[32m[I 2022-02-12 14:00:30,973]\u001b[0m Trial 4 finished with value: 3.2148099108683876 and parameters: {'learning_rate': 4.202148090930707e-05, 'num_train_epochs': 2, 'seed': 38, 'warmup_steps': 152, 'weight_decay': 0.14589392232755702, 'per_device_train_batch_size': 8}. Best is trial 4 with value: 3.2148099108683876.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.553500</td>\n","      <td>0.529251</td>\n","      <td>0.776991</td>\n","      <td>0.780073</td>\n","      <td>0.785671</td>\n","      <td>0.776991</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.441400</td>\n","      <td>0.495635</td>\n","      <td>0.805538</td>\n","      <td>0.801923</td>\n","      <td>0.800715</td>\n","      <td>0.805538</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.298000</td>\n","      <td>0.539576</td>\n","      <td>0.801460</td>\n","      <td>0.800795</td>\n","      <td>0.801043</td>\n","      <td>0.801460</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6771    0.7636    0.7177       994\n","           1     0.8833    0.8238    0.8525      2701\n","           2     0.6241    0.6598    0.6415       964\n","\n","    accuracy                         0.7770      4659\n","   macro avg     0.7282    0.7490    0.7372      4659\n","weighted avg     0.7857    0.7770    0.7801      4659\n","\n","0.7800731492470574\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7581    0.7726    0.7653       994\n","           1     0.8555    0.8963    0.8754      2701\n","           2     0.6912    0.5851    0.6337       964\n","\n","    accuracy                         0.8055      4659\n","   macro avg     0.7683    0.7513    0.7582      4659\n","weighted avg     0.8007    0.8055    0.8019      4659\n","\n","0.8019228660208497\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7384    0.8038    0.7697       994\n","           1     0.8771    0.8689    0.8730      2701\n","           2     0.6526    0.6100    0.6306       964\n","\n","    accuracy                         0.8015      4659\n","   macro avg     0.7560    0.7609    0.7578      4659\n","weighted avg     0.8010    0.8015    0.8008      4659\n","\n","0.8007954441915375\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-5/checkpoint-4660 (score: 0.49563461542129517).\n","\u001b[32m[I 2022-02-12 14:05:58,861]\u001b[0m Trial 5 finished with value: 3.204757096036416 and parameters: {'learning_rate': 3.099273387921209e-05, 'num_train_epochs': 3, 'seed': 21, 'warmup_steps': 277, 'weight_decay': 0.2566556426664683, 'per_device_train_batch_size': 16}. Best is trial 4 with value: 3.2148099108683876.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1166 01:09 < 01:09, 8.33 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.729000</td>\n","      <td>0.590566</td>\n","      <td>0.762610</td>\n","      <td>0.754862</td>\n","      <td>0.755621</td>\n","      <td>0.762610</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6813    0.7183    0.6993       994\n","           1     0.8133    0.8808    0.8457      2701\n","           2     0.6706    0.4772    0.5576       964\n","\n","    accuracy                         0.7626      4659\n","   macro avg     0.7217    0.6921    0.7009      4659\n","weighted avg     0.7556    0.7626    0.7549      4659\n","\n","0.7548618403201093\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:07:14,190]\u001b[0m Trial 6 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:33 < 01:46, 21.86 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.600900</td>\n","      <td>0.558845</td>\n","      <td>0.779566</td>\n","      <td>0.774559</td>\n","      <td>0.773395</td>\n","      <td>0.779566</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.512400</td>\n","      <td>0.533036</td>\n","      <td>0.784503</td>\n","      <td>0.778503</td>\n","      <td>0.781240</td>\n","      <td>0.784503</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7316    0.6911    0.7108       994\n","           1     0.8234    0.8893    0.8551      2701\n","           2     0.6762    0.5633    0.6146       964\n","\n","    accuracy                         0.7796      4659\n","   macro avg     0.7438    0.7146    0.7268      4659\n","weighted avg     0.7734    0.7796    0.7746      4659\n","\n","0.7745591309632635\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-7/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-7/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-7/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6877    0.7797    0.7308       994\n","           1     0.8409    0.8845    0.8621      2701\n","           2     0.7106    0.5093    0.5934       964\n","\n","    accuracy                         0.7845      4659\n","   macro avg     0.7464    0.7245    0.7288      4659\n","weighted avg     0.7812    0.7845    0.7785      4659\n","\n","0.7785029696022018\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:10:52,499]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 03:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.698100</td>\n","      <td>0.544126</td>\n","      <td>0.778279</td>\n","      <td>0.771479</td>\n","      <td>0.771929</td>\n","      <td>0.778279</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.499300</td>\n","      <td>0.496555</td>\n","      <td>0.797167</td>\n","      <td>0.798736</td>\n","      <td>0.800924</td>\n","      <td>0.797167</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.398400</td>\n","      <td>0.510567</td>\n","      <td>0.804250</td>\n","      <td>0.802714</td>\n","      <td>0.802306</td>\n","      <td>0.804250</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7317    0.6831    0.7066       994\n","           1     0.8116    0.8982    0.8527      2701\n","           2     0.7022    0.5405    0.6108       964\n","\n","    accuracy                         0.7783      4659\n","   macro avg     0.7485    0.7072    0.7234      4659\n","weighted avg     0.7719    0.7783    0.7715      4659\n","\n","0.7714791277987585\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7356    0.7726    0.7537       994\n","           1     0.8823    0.8523    0.8670      2701\n","           2     0.6402    0.6680    0.6538       964\n","\n","    accuracy                         0.7972      4659\n","   macro avg     0.7527    0.7643    0.7582      4659\n","weighted avg     0.8009    0.7972    0.7987      4659\n","\n","0.7987362183442277\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7385    0.7897    0.7632       994\n","           1     0.8691    0.8778    0.8735      2701\n","           2     0.6809    0.6131    0.6452       964\n","\n","    accuracy                         0.8042      4659\n","   macro avg     0.7628    0.7602    0.7606      4659\n","weighted avg     0.8023    0.8042    0.8027      4659\n","\n","0.802714256498369\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-8/checkpoint-1166 (score: 0.4965546727180481).\n","\u001b[32m[I 2022-02-12 14:14:43,481]\u001b[0m Trial 8 finished with value: 3.2135197015062835 and parameters: {'learning_rate': 3.8013548517568867e-05, 'num_train_epochs': 3, 'seed': 39, 'warmup_steps': 274, 'weight_decay': 0.038291400492122485, 'per_device_train_batch_size': 64}. Best is trial 4 with value: 3.2148099108683876.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:27, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.563200</td>\n","      <td>0.542018</td>\n","      <td>0.770766</td>\n","      <td>0.772611</td>\n","      <td>0.780993</td>\n","      <td>0.770766</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.430500</td>\n","      <td>0.512798</td>\n","      <td>0.804464</td>\n","      <td>0.800182</td>\n","      <td>0.800715</td>\n","      <td>0.804464</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.268800</td>\n","      <td>0.552453</td>\n","      <td>0.812835</td>\n","      <td>0.811793</td>\n","      <td>0.811225</td>\n","      <td>0.812835</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6329    0.8099    0.7105       994\n","           1     0.8804    0.8204    0.8494      2701\n","           2     0.6552    0.5913    0.6216       964\n","\n","    accuracy                         0.7708      4659\n","   macro avg     0.7228    0.7405    0.7272      4659\n","weighted avg     0.7810    0.7708    0.7726      4659\n","\n","0.7726114441656149\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7307    0.7998    0.7637       994\n","           1     0.8574    0.8926    0.8747      2701\n","           2     0.7141    0.5622    0.6291       964\n","\n","    accuracy                         0.8045      4659\n","   macro avg     0.7674    0.7516    0.7558      4659\n","weighted avg     0.8007    0.8045    0.8002      4659\n","\n","0.8001823063059155\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7699    0.8078    0.7884       994\n","           1     0.8787    0.8823    0.8805      2701\n","           2     0.6648    0.6234    0.6435       964\n","\n","    accuracy                         0.8128      4659\n","   macro avg     0.7711    0.7712    0.7708      4659\n","weighted avg     0.8112    0.8128    0.8118      4659\n","\n","0.811793371147101\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-9/checkpoint-4660 (score: 0.5127978324890137).\n","\u001b[32m[I 2022-02-12 14:20:12,090]\u001b[0m Trial 9 finished with value: 3.2486894835841547 and parameters: {'learning_rate': 6.579941859647633e-05, 'num_train_epochs': 3, 'seed': 22, 'warmup_steps': 464, 'weight_decay': 0.2894897525763174, 'per_device_train_batch_size': 16}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 11650\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='11650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 2331/11650 01:42 < 06:50, 22.69 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.575800</td>\n","      <td>0.574004</td>\n","      <td>0.769908</td>\n","      <td>0.769804</td>\n","      <td>0.773055</td>\n","      <td>0.769908</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6556    0.7777    0.7115       994\n","           1     0.8595    0.8360    0.8476      2701\n","           2     0.6518    0.5768    0.6120       964\n","\n","    accuracy                         0.7699      4659\n","   macro avg     0.7223    0.7301    0.7237      4659\n","weighted avg     0.7731    0.7699    0.7698      4659\n","\n","0.7698040456657698\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:21:59,972]\u001b[0m Trial 10 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9318' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9318/9318 05:35, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.563900</td>\n","      <td>0.533038</td>\n","      <td>0.776776</td>\n","      <td>0.777170</td>\n","      <td>0.779199</td>\n","      <td>0.776776</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.425000</td>\n","      <td>0.531701</td>\n","      <td>0.801460</td>\n","      <td>0.799700</td>\n","      <td>0.799411</td>\n","      <td>0.801460</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6812    0.7716    0.7236       994\n","           1     0.8667    0.8427    0.8545      2701\n","           2     0.6351    0.5975    0.6157       964\n","\n","    accuracy                         0.7768      4659\n","   macro avg     0.7277    0.7373    0.7313      4659\n","weighted avg     0.7792    0.7768    0.7772      4659\n","\n","0.777170108224493\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7295    0.7867    0.7570       994\n","           1     0.8668    0.8771    0.8719      2701\n","           2     0.6827    0.6048    0.6414       964\n","\n","    accuracy                         0.8015      4659\n","   macro avg     0.7597    0.7562    0.7568      4659\n","weighted avg     0.7994    0.8015    0.7997      4659\n","\n","0.7996996413986973\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-9318/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-11/checkpoint-9318 (score: 0.5317009687423706).\n","\u001b[32m[I 2022-02-12 14:27:36,513]\u001b[0m Trial 11 finished with value: 3.202029842354337 and parameters: {'learning_rate': 2.8241349884339874e-05, 'num_train_epochs': 2, 'seed': 35, 'warmup_steps': 150, 'weight_decay': 0.09097200032265645, 'per_device_train_batch_size': 8}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4657/9318 02:41 < 02:41, 28.81 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.522700</td>\n","      <td>0.557774</td>\n","      <td>0.765186</td>\n","      <td>0.762487</td>\n","      <td>0.771341</td>\n","      <td>0.765186</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6116    0.7938    0.6909       994\n","           1     0.8579    0.8493    0.8536      2701\n","           2     0.6935    0.5000    0.5811       964\n","\n","    accuracy                         0.7652      4659\n","   macro avg     0.7210    0.7144    0.7085      4659\n","weighted avg     0.7713    0.7652    0.7625      4659\n","\n","0.7624866858812082\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:30:23,411]\u001b[0m Trial 12 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9320\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='9320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/9320 03:33 < 03:33, 21.78 it/s, Epoch 2.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.557600</td>\n","      <td>0.537427</td>\n","      <td>0.784288</td>\n","      <td>0.776393</td>\n","      <td>0.778494</td>\n","      <td>0.784288</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.455100</td>\n","      <td>0.495857</td>\n","      <td>0.790084</td>\n","      <td>0.792429</td>\n","      <td>0.796479</td>\n","      <td>0.790084</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7340    0.7022    0.7177       994\n","           1     0.8135    0.9078    0.8581      2701\n","           2     0.7262    0.5228    0.6080       964\n","\n","    accuracy                         0.7843      4659\n","   macro avg     0.7579    0.7109    0.7279      4659\n","weighted avg     0.7785    0.7843    0.7764      4659\n","\n","0.7763929250503012\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-13/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-13/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-13/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7166    0.7887    0.7510       994\n","           1     0.8868    0.8378    0.8616      2701\n","           2     0.6259    0.6577    0.6414       964\n","\n","    accuracy                         0.7901      4659\n","   macro avg     0.7431    0.7614    0.7513      4659\n","weighted avg     0.7965    0.7901    0.7924      4659\n","\n","0.7924292370066351\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:34:02,478]\u001b[0m Trial 13 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4657/9318 02:38 < 02:38, 29.42 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.613000</td>\n","      <td>0.590818</td>\n","      <td>0.768835</td>\n","      <td>0.766465</td>\n","      <td>0.767431</td>\n","      <td>0.768835</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6628    0.7475    0.7026       994\n","           1     0.8426    0.8545    0.8485      2701\n","           2     0.6646    0.5508    0.6024       964\n","\n","    accuracy                         0.7688      4659\n","   macro avg     0.7233    0.7176    0.7178      4659\n","weighted avg     0.7674    0.7688    0.7665      4659\n","\n","0.7664647673624138\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:36:46,012]\u001b[0m Trial 14 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18636\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9319' max='18636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9316/18636 05:25 < 05:26, 28.58 it/s, Epoch 2.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.565600</td>\n","      <td>0.524527</td>\n","      <td>0.786435</td>\n","      <td>0.780204</td>\n","      <td>0.782134</td>\n","      <td>0.786435</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.444500</td>\n","      <td>0.514944</td>\n","      <td>0.799528</td>\n","      <td>0.796322</td>\n","      <td>0.796884</td>\n","      <td>0.799528</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7009    0.7545    0.7267       994\n","           1     0.8340    0.8926    0.8623      2701\n","           2     0.7206    0.5218    0.6053       964\n","\n","    accuracy                         0.7864      4659\n","   macro avg     0.7518    0.7230    0.7314      4659\n","weighted avg     0.7821    0.7864    0.7802      4659\n","\n","0.780203717564679\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-15/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-15/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-15/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7180    0.7968    0.7554       994\n","           1     0.8592    0.8812    0.8700      2701\n","           2     0.7036    0.5737    0.6320       964\n","\n","    accuracy                         0.7995      4659\n","   macro avg     0.7603    0.7505    0.7525      4659\n","weighted avg     0.7969    0.7995    0.7963      4659\n","\n","0.7963219934859517\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:42:17,138]\u001b[0m Trial 15 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/4660 01:41 < 01:41, 22.85 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.745200</td>\n","      <td>0.698688</td>\n","      <td>0.704014</td>\n","      <td>0.693615</td>\n","      <td>0.689865</td>\n","      <td>0.704014</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6378    0.6378    0.6378       994\n","           1     0.7699    0.8449    0.8056      2701\n","           2     0.5193    0.3776    0.4372       964\n","\n","    accuracy                         0.7040      4659\n","   macro avg     0.6423    0.6201    0.6269      4659\n","weighted avg     0.6899    0.7040    0.6936      4659\n","\n","0.6936153444921317\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:44:04,332]\u001b[0m Trial 16 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5825\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='5825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/5825 02:45 < 04:08, 14.03 it/s, Epoch 2/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.578600</td>\n","      <td>0.525789</td>\n","      <td>0.784503</td>\n","      <td>0.778800</td>\n","      <td>0.778129</td>\n","      <td>0.784503</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.445100</td>\n","      <td>0.497069</td>\n","      <td>0.797811</td>\n","      <td>0.797332</td>\n","      <td>0.798636</td>\n","      <td>0.797811</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7236    0.7163    0.7199       994\n","           1     0.8298    0.8956    0.8615      2701\n","           2     0.6895    0.5436    0.6079       964\n","\n","    accuracy                         0.7845      4659\n","   macro avg     0.7476    0.7185    0.7298      4659\n","weighted avg     0.7781    0.7845    0.7788      4659\n","\n","0.7787995796566424\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-17/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-17/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-17/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7189    0.8129    0.7630       994\n","           1     0.8776    0.8604    0.8689      2701\n","           2     0.6595    0.6068    0.6321       964\n","\n","    accuracy                         0.7978      4659\n","   macro avg     0.7520    0.7600    0.7547      4659\n","weighted avg     0.7986    0.7978    0.7973      4659\n","\n","0.7973317548047276\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:46:55,531]\u001b[0m Trial 17 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/4660 01:42 < 01:42, 22.79 it/s, Epoch 1/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.625900</td>\n","      <td>0.600340</td>\n","      <td>0.758103</td>\n","      <td>0.754806</td>\n","      <td>0.754185</td>\n","      <td>0.758103</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6691    0.7264    0.6966       994\n","           1     0.8293    0.8526    0.8408      2701\n","           2     0.6314    0.5259    0.5739       964\n","\n","    accuracy                         0.7581      4659\n","   macro avg     0.7099    0.7016    0.7037      4659\n","weighted avg     0.7542    0.7581    0.7548      4659\n","\n","0.7548056945979844\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:48:42,963]\u001b[0m Trial 18 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18636\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9319' max='18636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9316/18636 05:26 < 05:27, 28.48 it/s, Epoch 2.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.556000</td>\n","      <td>0.540625</td>\n","      <td>0.786864</td>\n","      <td>0.784192</td>\n","      <td>0.784337</td>\n","      <td>0.786864</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.461200</td>\n","      <td>0.555088</td>\n","      <td>0.796952</td>\n","      <td>0.790412</td>\n","      <td>0.792449</td>\n","      <td>0.796952</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6994    0.7656    0.7310       994\n","           1     0.8509    0.8708    0.8608      2701\n","           2     0.6853    0.5737    0.6245       964\n","\n","    accuracy                         0.7869      4659\n","   macro avg     0.7452    0.7367    0.7388      4659\n","weighted avg     0.7843    0.7869    0.7842      4659\n","\n","0.7841922674162086\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-19/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-19/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-19/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7245    0.7857    0.7539       994\n","           1     0.8407    0.8989    0.8688      2701\n","           2     0.7273    0.5228    0.6083       964\n","\n","    accuracy                         0.7970      4659\n","   macro avg     0.7642    0.7358    0.7437      4659\n","weighted avg     0.7924    0.7970    0.7904      4659\n","\n","0.790411867865357\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 14:54:15,260]\u001b[0m Trial 19 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 03:39, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.567400</td>\n","      <td>0.517422</td>\n","      <td>0.788367</td>\n","      <td>0.782804</td>\n","      <td>0.783161</td>\n","      <td>0.788367</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.415400</td>\n","      <td>0.491683</td>\n","      <td>0.805967</td>\n","      <td>0.803451</td>\n","      <td>0.803904</td>\n","      <td>0.805967</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7211    0.7596    0.7398       994\n","           1     0.8346    0.8893    0.8611      2701\n","           2     0.7030    0.5353    0.6078       964\n","\n","    accuracy                         0.7884      4659\n","   macro avg     0.7529    0.7280    0.7362      4659\n","weighted avg     0.7832    0.7884    0.7828      4659\n","\n","0.7828036654442388\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7300    0.8189    0.7719       994\n","           1     0.8710    0.8800    0.8755      2701\n","           2     0.6920    0.5851    0.6341       964\n","\n","    accuracy                         0.8060      4659\n","   macro avg     0.7644    0.7613    0.7605      4659\n","weighted avg     0.8039    0.8060    0.8035      4659\n","\n","0.8034511546265247\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-20/checkpoint-4660 (score: 0.49168333411216736).\n","\u001b[32m[I 2022-02-12 14:57:56,564]\u001b[0m Trial 20 finished with value: 3.2192888472141696 and parameters: {'learning_rate': 4.591455580099434e-05, 'num_train_epochs': 2, 'seed': 25, 'warmup_steps': 225, 'weight_decay': 0.1661852327680115, 'per_device_train_batch_size': 16}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 03:34, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.541900</td>\n","      <td>0.523503</td>\n","      <td>0.784288</td>\n","      <td>0.779088</td>\n","      <td>0.778618</td>\n","      <td>0.784288</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.418700</td>\n","      <td>0.494062</td>\n","      <td>0.799313</td>\n","      <td>0.796393</td>\n","      <td>0.795709</td>\n","      <td>0.799313</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7204    0.7233    0.7219       994\n","           1     0.8295    0.8900    0.8587      2701\n","           2     0.6959    0.5508    0.6149       964\n","\n","    accuracy                         0.7843      4659\n","   macro avg     0.7486    0.7214    0.7319      4659\n","weighted avg     0.7786    0.7843    0.7791      4659\n","\n","0.7790884791850295\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-21/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-21/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-21/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7358    0.7817    0.7580       994\n","           1     0.8563    0.8823    0.8691      2701\n","           2     0.6878    0.5851    0.6323       964\n","\n","    accuracy                         0.7993      4659\n","   macro avg     0.7600    0.7497    0.7531      4659\n","weighted avg     0.7957    0.7993    0.7964      4659\n","\n","0.7963926016439064\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:01:36,014]\u001b[0m Trial 21 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 03:37, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.563100</td>\n","      <td>0.523726</td>\n","      <td>0.786006</td>\n","      <td>0.777954</td>\n","      <td>0.779057</td>\n","      <td>0.786006</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.414600</td>\n","      <td>0.491583</td>\n","      <td>0.802962</td>\n","      <td>0.802376</td>\n","      <td>0.802794</td>\n","      <td>0.802962</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7468    0.7093    0.7276       994\n","           1     0.8172    0.9104    0.8613      2701\n","           2     0.7054    0.5166    0.5964       964\n","\n","    accuracy                         0.7860      4659\n","   macro avg     0.7565    0.7121    0.7284      4659\n","weighted avg     0.7791    0.7860    0.7780      4659\n","\n","0.7779535955678275\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7311    0.7988    0.7635       994\n","           1     0.8786    0.8704    0.8745      2701\n","           2     0.6644    0.6183    0.6405       964\n","\n","    accuracy                         0.8030      4659\n","   macro avg     0.7580    0.7625    0.7595      4659\n","weighted avg     0.8028    0.8030    0.8024      4659\n","\n","0.8023758031128712\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-4660/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-22/checkpoint-4660 (score: 0.49158304929733276).\n","\u001b[32m[I 2022-02-12 15:05:14,860]\u001b[0m Trial 22 finished with value: 3.2110939142827677 and parameters: {'learning_rate': 5.736610942081283e-05, 'num_train_epochs': 2, 'seed': 29, 'warmup_steps': 323, 'weight_decay': 0.21003181844745372, 'per_device_train_batch_size': 16}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6990' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6990/6990 05:26, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.567900</td>\n","      <td>0.527350</td>\n","      <td>0.789440</td>\n","      <td>0.785098</td>\n","      <td>0.783995</td>\n","      <td>0.789440</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.421100</td>\n","      <td>0.499829</td>\n","      <td>0.800816</td>\n","      <td>0.801204</td>\n","      <td>0.802821</td>\n","      <td>0.800816</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.314300</td>\n","      <td>0.543686</td>\n","      <td>0.801460</td>\n","      <td>0.800922</td>\n","      <td>0.801504</td>\n","      <td>0.801460</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7342    0.7223    0.7282       994\n","           1     0.8361    0.8912    0.8627      2701\n","           2     0.6895    0.5737    0.6263       964\n","\n","    accuracy                         0.7894      4659\n","   macro avg     0.7532    0.7290    0.7391      4659\n","weighted avg     0.7840    0.7894    0.7851      4659\n","\n","0.7850979437333856\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7206    0.8018    0.7590       994\n","           1     0.8840    0.8604    0.8720      2701\n","           2     0.6602    0.6328    0.6462       964\n","\n","    accuracy                         0.8008      4659\n","   macro avg     0.7549    0.7650    0.7591      4659\n","weighted avg     0.8028    0.8008    0.8012      4659\n","\n","0.8012042676883507\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7294    0.8028    0.7644       994\n","           1     0.8804    0.8693    0.8748      2701\n","           2     0.6548    0.6100    0.6316       964\n","\n","    accuracy                         0.8015      4659\n","   macro avg     0.7549    0.7607    0.7569      4659\n","weighted avg     0.8015    0.8015    0.8009      4659\n","\n","0.8009219891701026\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-6990/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-23/checkpoint-4660 (score: 0.4998287558555603).\n","\u001b[32m[I 2022-02-12 15:10:42,532]\u001b[0m Trial 23 finished with value: 3.205344819549575 and parameters: {'learning_rate': 2.7213255631668047e-05, 'num_train_epochs': 3, 'seed': 18, 'warmup_steps': 233, 'weight_decay': 0.2986372926158696, 'per_device_train_batch_size': 16}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/4660 03:30, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.589500</td>\n","      <td>0.526113</td>\n","      <td>0.788796</td>\n","      <td>0.783521</td>\n","      <td>0.782786</td>\n","      <td>0.788796</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.419500</td>\n","      <td>0.512061</td>\n","      <td>0.795879</td>\n","      <td>0.793904</td>\n","      <td>0.793803</td>\n","      <td>0.795879</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7375    0.7264    0.7319       994\n","           1     0.8315    0.8952    0.8622      2701\n","           2     0.6930    0.5550    0.6164       964\n","\n","    accuracy                         0.7888      4659\n","   macro avg     0.7540    0.7255    0.7368      4659\n","weighted avg     0.7828    0.7888    0.7835      4659\n","\n","0.783520794657633\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-24/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-24/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-24/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7220    0.7918    0.7553       994\n","           1     0.8639    0.8719    0.8679      2701\n","           2     0.6714    0.5871    0.6265       964\n","\n","    accuracy                         0.7959      4659\n","   macro avg     0.7524    0.7503    0.7499      4659\n","weighted avg     0.7938    0.7959    0.7939      4659\n","\n","0.7939039479190795\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:14:18,078]\u001b[0m Trial 24 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 13977\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9319' max='13977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9316/13977 05:24 < 02:42, 28.74 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.575900</td>\n","      <td>0.541958</td>\n","      <td>0.784718</td>\n","      <td>0.785323</td>\n","      <td>0.786025</td>\n","      <td>0.784718</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.410300</td>\n","      <td>0.522692</td>\n","      <td>0.801245</td>\n","      <td>0.798572</td>\n","      <td>0.798943</td>\n","      <td>0.801245</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7164    0.7344    0.7253       994\n","           1     0.8665    0.8552    0.8608      2701\n","           2     0.6324    0.6390    0.6357       964\n","\n","    accuracy                         0.7847      4659\n","   macro avg     0.7384    0.7429    0.7406      4659\n","weighted avg     0.7860    0.7847    0.7853      4659\n","\n","0.7853229612810536\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-25/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-25/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-25/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7201    0.7998    0.7579       994\n","           1     0.8650    0.8800    0.8725      2701\n","           2     0.6952    0.5820    0.6335       964\n","\n","    accuracy                         0.8012      4659\n","   macro avg     0.7601    0.7539    0.7546      4659\n","weighted avg     0.7989    0.8012    0.7986      4659\n","\n","0.79857216213819\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:19:47,485]\u001b[0m Trial 25 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1167' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 02:26, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.641600</td>\n","      <td>0.519838</td>\n","      <td>0.786220</td>\n","      <td>0.781371</td>\n","      <td>0.783060</td>\n","      <td>0.786220</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.470200</td>\n","      <td>0.490654</td>\n","      <td>0.801674</td>\n","      <td>0.800365</td>\n","      <td>0.800011</td>\n","      <td>0.801674</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6955    0.7837    0.7370       994\n","           1     0.8439    0.8786    0.8609      2701\n","           2     0.7029    0.5301    0.6044       964\n","\n","    accuracy                         0.7862      4659\n","   macro avg     0.7474    0.7308    0.7341      4659\n","weighted avg     0.7831    0.7862    0.7814      4659\n","\n","0.7813714492011276\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-26/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-26/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-26/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7357    0.7867    0.7603       994\n","           1     0.8694    0.8749    0.8721      2701\n","           2     0.6720    0.6120    0.6406       964\n","\n","    accuracy                         0.8017      4659\n","   macro avg     0.7590    0.7579    0.7577      4659\n","weighted avg     0.8000    0.8017    0.8004      4659\n","\n","0.8003652606156981\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:22:18,825]\u001b[0m Trial 26 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/2330 01:19 < 01:19, 14.70 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.594000</td>\n","      <td>0.552673</td>\n","      <td>0.777849</td>\n","      <td>0.772213</td>\n","      <td>0.771763</td>\n","      <td>0.777849</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7112    0.7062    0.7087       994\n","           1     0.8230    0.8886    0.8545      2701\n","           2     0.6905    0.5415    0.6070       964\n","\n","    accuracy                         0.7778      4659\n","   macro avg     0.7416    0.7121    0.7234      4659\n","weighted avg     0.7718    0.7778    0.7722      4659\n","\n","0.7722133853687699\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:23:43,195]\u001b[0m Trial 27 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4661' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4660/6990 03:30 < 01:45, 22.15 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.558400</td>\n","      <td>0.511172</td>\n","      <td>0.793089</td>\n","      <td>0.790092</td>\n","      <td>0.790592</td>\n","      <td>0.793089</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.424900</td>\n","      <td>0.508188</td>\n","      <td>0.794376</td>\n","      <td>0.792376</td>\n","      <td>0.799604</td>\n","      <td>0.794376</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7053    0.7827    0.7420       994\n","           1     0.8567    0.8767    0.8666      2701\n","           2     0.6932    0.5695    0.6253       964\n","\n","    accuracy                         0.7931      4659\n","   macro avg     0.7518    0.7430    0.7446      4659\n","weighted avg     0.7906    0.7931    0.7901      4659\n","\n","0.7900915932736149\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-28/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-28/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-28/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6669    0.8561    0.7498       994\n","           1     0.8796    0.8571    0.8682      2701\n","           2     0.7124    0.5550    0.6239       964\n","\n","    accuracy                         0.7944      4659\n","   macro avg     0.7530    0.7561    0.7473      4659\n","weighted avg     0.7996    0.7944    0.7924      4659\n","\n","0.7923758973857472\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:27:18,739]\u001b[0m Trial 28 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 4660\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='4660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/4660 01:19 < 03:58, 14.65 it/s, Epoch 1.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.627000</td>\n","      <td>0.580313</td>\n","      <td>0.766044</td>\n","      <td>0.760567</td>\n","      <td>0.759322</td>\n","      <td>0.766044</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6902    0.6992    0.6947       994\n","           1     0.8222    0.8782    0.8493      2701\n","           2     0.6545    0.5207    0.5800       964\n","\n","    accuracy                         0.7660      4659\n","   macro avg     0.7223    0.6994    0.7080      4659\n","weighted avg     0.7593    0.7660    0.7606      4659\n","\n","0.7605673796199659\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:28:43,390]\u001b[0m Trial 29 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 13977\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='13977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/13977 02:38 < 05:17, 29.34 it/s, Epoch 1.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.610000</td>\n","      <td>0.591354</td>\n","      <td>0.764112</td>\n","      <td>0.759556</td>\n","      <td>0.758956</td>\n","      <td>0.764112</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6804    0.7304    0.7045       994\n","           1     0.8268    0.8645    0.8452      2701\n","           2     0.6497    0.5176    0.5762       964\n","\n","    accuracy                         0.7641      4659\n","   macro avg     0.7190    0.7042    0.7087      4659\n","weighted avg     0.7590    0.7641    0.7596      4659\n","\n","0.7595559809272101\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:31:27,322]\u001b[0m Trial 30 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1167' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1167/1749 02:26 < 01:12, 7.98 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.700400</td>\n","      <td>0.539908</td>\n","      <td>0.780854</td>\n","      <td>0.775506</td>\n","      <td>0.774802</td>\n","      <td>0.780854</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.496800</td>\n","      <td>0.491065</td>\n","      <td>0.800816</td>\n","      <td>0.797448</td>\n","      <td>0.797026</td>\n","      <td>0.800816</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7218    0.6942    0.7077       994\n","           1     0.8236    0.8919    0.8564      2701\n","           2     0.6928    0.5591    0.6188       964\n","\n","    accuracy                         0.7809      4659\n","   macro avg     0.7460    0.7151    0.7276      4659\n","weighted avg     0.7748    0.7809    0.7755      4659\n","\n","0.7755060233337965\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-31/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-31/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-31/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7335    0.7837    0.7578       994\n","           1     0.8559    0.8863    0.8709      2701\n","           2     0.6975    0.5788    0.6327       964\n","\n","    accuracy                         0.8008      4659\n","   macro avg     0.7623    0.7496    0.7538      4659\n","weighted avg     0.7970    0.8008    0.7974      4659\n","\n","0.7974482887443389\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:33:58,647]\u001b[0m Trial 31 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:09 < 02:19, 8.36 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.698900</td>\n","      <td>0.558880</td>\n","      <td>0.770122</td>\n","      <td>0.768219</td>\n","      <td>0.771498</td>\n","      <td>0.770122</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7580    0.5956    0.6670       994\n","           1     0.8414    0.8778    0.8592      2701\n","           2     0.5896    0.6483    0.6176       964\n","\n","    accuracy                         0.7701      4659\n","   macro avg     0.7297    0.7072    0.7146      4659\n","weighted avg     0.7715    0.7701    0.7682      4659\n","\n","0.7682188740339928\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:35:13,667]\u001b[0m Trial 32 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:09 < 02:19, 8.34 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.720500</td>\n","      <td>0.551510</td>\n","      <td>0.774844</td>\n","      <td>0.769574</td>\n","      <td>0.767744</td>\n","      <td>0.774844</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7226    0.6841    0.7028       994\n","           1     0.8270    0.8904    0.8576      2701\n","           2     0.6481    0.5446    0.5919       964\n","\n","    accuracy                         0.7748      4659\n","   macro avg     0.7326    0.7064    0.7174      4659\n","weighted avg     0.7677    0.7748    0.7696      4659\n","\n","0.7695736922053926\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:36:28,814]\u001b[0m Trial 33 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2332 01:10 < 03:30, 8.29 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.726000</td>\n","      <td>0.580225</td>\n","      <td>0.761966</td>\n","      <td>0.761854</td>\n","      <td>0.775023</td>\n","      <td>0.761966</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.5974    0.8209    0.6915       994\n","           1     0.8661    0.8238    0.8444      2701\n","           2     0.7030    0.5280    0.6031       964\n","\n","    accuracy                         0.7620      4659\n","   macro avg     0.7222    0.7242    0.7130      4659\n","weighted avg     0.7750    0.7620    0.7619      4659\n","\n","0.7618537217680869\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:37:44,531]\u001b[0m Trial 34 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1749' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1749/1749 03:49, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.669700</td>\n","      <td>0.536138</td>\n","      <td>0.779566</td>\n","      <td>0.778558</td>\n","      <td>0.779357</td>\n","      <td>0.779566</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.469700</td>\n","      <td>0.481131</td>\n","      <td>0.806825</td>\n","      <td>0.806504</td>\n","      <td>0.807258</td>\n","      <td>0.806825</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.348200</td>\n","      <td>0.513604</td>\n","      <td>0.808113</td>\n","      <td>0.807210</td>\n","      <td>0.807051</td>\n","      <td>0.808113</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7644    0.6660    0.7118       994\n","           1     0.8437    0.8712    0.8572      2701\n","           2     0.6145    0.6400    0.6270       964\n","\n","    accuracy                         0.7796      4659\n","   macro avg     0.7409    0.7257    0.7320      4659\n","weighted avg     0.7794    0.7796    0.7786      4659\n","\n","0.7785575795769097\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7308    0.8028    0.7651       994\n","           1     0.8811    0.8697    0.8753      2701\n","           2     0.6792    0.6349    0.6563       964\n","\n","    accuracy                         0.8068      4659\n","   macro avg     0.7637    0.7691    0.7656      4659\n","weighted avg     0.8073    0.8068    0.8065      4659\n","\n","0.806504000710417\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7542    0.8089    0.7806       994\n","           1     0.8761    0.8741    0.8751      2701\n","           2     0.6682    0.6224    0.6445       964\n","\n","    accuracy                         0.8081      4659\n","   macro avg     0.7661    0.7685    0.7667      4659\n","weighted avg     0.8071    0.8081    0.8072      4659\n","\n","0.8072100797295699\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1749/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-35/checkpoint-1166 (score: 0.4811306893825531).\n","\u001b[32m[I 2022-02-12 15:41:35,402]\u001b[0m Trial 35 finished with value: 3.2304877050408587 and parameters: {'learning_rate': 6.387782087336463e-05, 'num_train_epochs': 3, 'seed': 27, 'warmup_steps': 195, 'weight_decay': 0.030071139791874396, 'per_device_train_batch_size': 64}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2330\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2330' max='2330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2330/2330 02:51, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.574800</td>\n","      <td>0.515685</td>\n","      <td>0.791372</td>\n","      <td>0.789652</td>\n","      <td>0.788450</td>\n","      <td>0.791372</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.415600</td>\n","      <td>0.486278</td>\n","      <td>0.802318</td>\n","      <td>0.800883</td>\n","      <td>0.800775</td>\n","      <td>0.802318</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7332    0.7243    0.7287       994\n","           1     0.8534    0.8775    0.8653      2701\n","           2     0.6633    0.6193    0.6406       964\n","\n","    accuracy                         0.7914      4659\n","   macro avg     0.7500    0.7404    0.7449      4659\n","weighted avg     0.7885    0.7914    0.7897      4659\n","\n","0.7896522237283408\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7311    0.7958    0.7620       994\n","           1     0.8731    0.8760    0.8745      2701\n","           2     0.6701    0.6027    0.6346       964\n","\n","    accuracy                         0.8023      4659\n","   macro avg     0.7581    0.7581    0.7571      4659\n","weighted avg     0.8008    0.8023    0.8009      4659\n","\n","0.8008830593413219\n"]},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-2330/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_twit/hyper/results/run-36/checkpoint-2330 (score: 0.48627781867980957).\n","\u001b[32m[I 2022-02-12 15:44:28,725]\u001b[0m Trial 36 finished with value: 3.2062945264027176 and parameters: {'learning_rate': 6.60022542733528e-05, 'num_train_epochs': 2, 'seed': 27, 'warmup_steps': 193, 'weight_decay': 0.03362492622452753, 'per_device_train_batch_size': 32}. Best is trial 9 with value: 3.2486894835841547.\u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2329/6990 01:43 < 03:26, 22.53 it/s, Epoch 1.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.551400</td>\n","      <td>0.540260</td>\n","      <td>0.770981</td>\n","      <td>0.774159</td>\n","      <td>0.781431</td>\n","      <td>0.770981</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.6566    0.7847    0.7149       994\n","           1     0.8835    0.8141    0.8474      2701\n","           2     0.6242    0.6359    0.6300       964\n","\n","    accuracy                         0.7710      4659\n","   macro avg     0.7214    0.7449    0.7308      4659\n","weighted avg     0.7814    0.7710    0.7742      4659\n","\n","0.7741586100152303\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:46:17,430]\u001b[0m Trial 37 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/1749 01:09 < 02:19, 8.33 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.708000</td>\n","      <td>0.562966</td>\n","      <td>0.775274</td>\n","      <td>0.775353</td>\n","      <td>0.775439</td>\n","      <td>0.775274</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7136    0.7093    0.7114       994\n","           1     0.8526    0.8523    0.8524      2701\n","           2     0.6231    0.6276    0.6253       964\n","\n","    accuracy                         0.7753      4659\n","   macro avg     0.7297    0.7297    0.7297      4659\n","weighted avg     0.7754    0.7753    0.7754      4659\n","\n","0.7753534652816131\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:47:32,568]\u001b[0m Trial 38 pruned. \u001b[0m\n","Trial:\n","loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"distilbert-base-uncased\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n","Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, sentence, attention_mask_bert, input_ids_bert.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4657/9318 02:39 < 02:39, 29.18 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.606900</td>\n","      <td>0.571793</td>\n","      <td>0.776132</td>\n","      <td>0.772837</td>\n","      <td>0.771481</td>\n","      <td>0.776132</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: input_ids_bert, sentence, token_type_ids_bert, __index_level_0__, attention_mask_bert.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n"]},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0     0.7067    0.7103    0.7085       994\n","           1     0.8332    0.8726    0.8524      2701\n","           2     0.6655    0.5737    0.6162       964\n","\n","    accuracy                         0.7761      4659\n","   macro avg     0.7351    0.7189    0.7257      4659\n","weighted avg     0.7715    0.7761    0.7728      4659\n","\n","0.7728368609924559\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[32m[I 2022-02-12 15:50:17,389]\u001b[0m Trial 39 pruned. \u001b[0m\n"]}],"source":["best_trial = hyper_trainer.hyperparameter_search(n_trials=40, direction=\"maximize\", backend=\"optuna\", hp_space=hp_space_optuna)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1644681017957,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"},"user_tz":0},"id":"_SuDrqW_srbP","outputId":"45adc7c7-fe2f-41f1-a565-285b9d806b48"},"outputs":[{"output_type":"stream","name":"stdout","text":["12-Feb-2022 (15:50:17.397670)\n"]}],"source":["timestamp()"]},{"cell_type":"code","source":["best_trial"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qEi1M72IIi8T","executionInfo":{"status":"ok","timestamp":1644681017958,"user_tz":0,"elapsed":12,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"ee8a845c-4bb6-4c29-cb17-33cd9ace5969"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='9', objective=3.2486894835841547, hyperparameters={'learning_rate': 6.579941859647633e-05, 'num_train_epochs': 3, 'seed': 22, 'warmup_steps': 464, 'weight_decay': 0.2894897525763174, 'per_device_train_batch_size': 16})"]},"metadata":{},"execution_count":14}]}],"metadata":{"accelerator":"GPU","colab":{"background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"Distilbert Experiment HateTwit Hyperparam Search v2.ipynb","provenance":[{"file_id":"1hyoAQszZMukh6-H5eZiWKIPQp3c2GcoV","timestamp":1644755985401},{"file_id":"15wylWRQJ1vxlP58OhBHUnY5fdagThMvJ","timestamp":1643746015678},{"file_id":"1B35hajBJY3fRQV7LSjqoqQ3xjdcyCzE3","timestamp":1642365829686},{"file_id":"1b0lCW2Cj6AULiE-Axh2OeZwURHR6pfcD","timestamp":1642333651961},{"file_id":"1RAjFaq-k9CLJQP5eO668UXLc0q-wjuve","timestamp":1642280213414},{"file_id":"1LzZr7GRN1SwVrNyjVX5t5PpCvio8w_kE","timestamp":1642109182548},{"file_id":"1t-qzEyxZtGn_Cnlfg2WtN-0dlgUmxfzx","timestamp":1641764210714},{"file_id":"14bEU8hCGPF8cVUA_BonJQA6Brv89DKUm","timestamp":1641252935713},{"file_id":"18Il3CpGf89tF1Z10iYX8OdfeHxxAX1Ui","timestamp":1639243141142},{"file_id":"1SHCgoRQVGtl9OiWEJGK3JgKkuxxSPy_5","timestamp":1639231968300},{"file_id":"1D-3yvF0nGnaWEZGxQj21R6fsGnyv5a5g","timestamp":1637526185003},{"file_id":"1glXr2JglKPUpN_3AGyk3GjfCtZSDNfsZ","timestamp":1637408594851},{"file_id":"1rc5hX8PBIv7GKvlxLQ35Vwf2jxLsv9f4","timestamp":1634501666668}],"mount_file_id":"1hyoAQszZMukh6-H5eZiWKIPQp3c2GcoV","authorship_tag":"ABX9TyMbZwcays63UGUVgrJxf8hM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"07c939dd2f90489599626a9353eb99dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fcd9fca81fec444090f5d98e258e24ac","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_25956e8d5602402cb6bf2459102805f4","IPY_MODEL_b1134b067026461cb0afca0966f946a9","IPY_MODEL_2f32feb7f4304f60bf8a8a9bad4efbc6"]}},"fcd9fca81fec444090f5d98e258e24ac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"25956e8d5602402cb6bf2459102805f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b7eb6c1f35284f12bf233339a68a22e3","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5b3309376c124e15a7bf6e8e3629d980"}},"b1134b067026461cb0afca0966f946a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6b88e578081e465ba39ebaf827575fe9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4992fe294d39452193e94b4771cabca1"}},"2f32feb7f4304f60bf8a8a9bad4efbc6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dcad80517c7947f28c57f29f87758db8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 869B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7dc0f6f7b0a143a5a0e4b4fe0bf27779"}},"b7eb6c1f35284f12bf233339a68a22e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5b3309376c124e15a7bf6e8e3629d980":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6b88e578081e465ba39ebaf827575fe9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4992fe294d39452193e94b4771cabca1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dcad80517c7947f28c57f29f87758db8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7dc0f6f7b0a143a5a0e4b4fe0bf27779":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0395716dba054989b8c36c800ebb93bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5c2b51a6b8964500a9a1079f5736979a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2424545ef5e84c4e88ef675411302ec8","IPY_MODEL_ea12fce919374940b22958ffa2cc998a","IPY_MODEL_de035539dbb64c15bb5fcf85caa4bdad"]}},"5c2b51a6b8964500a9a1079f5736979a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2424545ef5e84c4e88ef675411302ec8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e05916329ab049428ba11b82ccf2242d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19dd7a617c014eb18d471421390b5395"}},"ea12fce919374940b22958ffa2cc998a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_725f5646dcc8462883e5f88c8ab5bad5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":483,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":483,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f4e4276d1a9c4d4297497dcc75ebf204"}},"de035539dbb64c15bb5fcf85caa4bdad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1ce07ed770fb4f509845d9714c513531","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 483/483 [00:00&lt;00:00, 18.4kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4cb0c1b4667f408e8f3b902c46237fcb"}},"e05916329ab049428ba11b82ccf2242d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"19dd7a617c014eb18d471421390b5395":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"725f5646dcc8462883e5f88c8ab5bad5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f4e4276d1a9c4d4297497dcc75ebf204":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ce07ed770fb4f509845d9714c513531":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"4cb0c1b4667f408e8f3b902c46237fcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"febec3a100c84748bc1d27bb73356c22":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b20347d73b1d4e159de10d8f77b2c0c4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a51691f046bb49e68af5edcb547e84b0","IPY_MODEL_51e1fadeb0b341438a7f7c7cc6074a07","IPY_MODEL_7f031938f0af4954b018ba5d5b8ec6b4"]}},"b20347d73b1d4e159de10d8f77b2c0c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a51691f046bb49e68af5edcb547e84b0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_576ce9b33e314368b03c7e9b86a8e38a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_68a93365e47a46dc9d0080e6451b1167"}},"51e1fadeb0b341438a7f7c7cc6074a07":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_78ffa19b935e4c4284091a87dbb2f8f9","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ad82b4c2c7e4a4287e1581ab660b6ad"}},"7f031938f0af4954b018ba5d5b8ec6b4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_331f5ef209e14e928ec6f2be11137e29","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 592kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a1a155b54b0949318d4084b6fe39ebe8"}},"576ce9b33e314368b03c7e9b86a8e38a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"68a93365e47a46dc9d0080e6451b1167":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"78ffa19b935e4c4284091a87dbb2f8f9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5ad82b4c2c7e4a4287e1581ab660b6ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"331f5ef209e14e928ec6f2be11137e29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a1a155b54b0949318d4084b6fe39ebe8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d6328975758d4e3a9638bb27e7c1f42b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a4b66dad21c4cc1b7381ed213e54d78","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_6f4b8751dd7a43f5aaa1d97ec21d2166","IPY_MODEL_dd2af74042e04d4389716c5e4828bfaf","IPY_MODEL_a1bb2d2c2cec401787684fd94f85dc91"]}},"7a4b66dad21c4cc1b7381ed213e54d78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6f4b8751dd7a43f5aaa1d97ec21d2166":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4cf68ef0813d438c8295edc2e3da6554","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_15e40bd1507a4c1e853c62a2500f56ce"}},"dd2af74042e04d4389716c5e4828bfaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7669282abf174f0197ede44cbed55c35","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d0cfef1495e4332bfde6cea5a2553c7"}},"a1bb2d2c2cec401787684fd94f85dc91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a5f740c35b8e4422b0ae4206d321e36f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 455k/455k [00:00&lt;00:00, 593kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f9a5c6d4b69e4963934e93441c4a0a0b"}},"4cf68ef0813d438c8295edc2e3da6554":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"15e40bd1507a4c1e853c62a2500f56ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7669282abf174f0197ede44cbed55c35":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1d0cfef1495e4332bfde6cea5a2553c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a5f740c35b8e4422b0ae4206d321e36f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f9a5c6d4b69e4963934e93441c4a0a0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2710d26d035d4786b012865fe0fc7e5e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2340e2e266504a02ac967839db4f5fa8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d5541251391a4a3a99e690f24a5b602e","IPY_MODEL_8ed3695997a441048aff7ead4fd84206","IPY_MODEL_6bf31739e793411eb0f84d605eca14da"]}},"2340e2e266504a02ac967839db4f5fa8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5541251391a4a3a99e690f24a5b602e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_aafb830784d549c3b618554f45b68387","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45cba0faf8fd4deabcdcd754ec5fab32"}},"8ed3695997a441048aff7ead4fd84206":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d88985d11a744949b0cc32136f21a22d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":267967963,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":267967963,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79069103dd4644a084f2c4ce629afc08"}},"6bf31739e793411eb0f84d605eca14da":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_82fbf863605440099d0c8fd3a7a68c5f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 256M/256M [00:04&lt;00:00, 59.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b440548641145acb0018d05cde23aa9"}},"aafb830784d549c3b618554f45b68387":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"45cba0faf8fd4deabcdcd754ec5fab32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d88985d11a744949b0cc32136f21a22d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"79069103dd4644a084f2c4ce629afc08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82fbf863605440099d0c8fd3a7a68c5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7b440548641145acb0018d05cde23aa9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}