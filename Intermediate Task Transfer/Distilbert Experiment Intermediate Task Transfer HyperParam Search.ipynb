{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"F-o6bXOJ18j4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644756614244,"user_tz":0,"elapsed":20809,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"dd8564ba-5734-44bd-9c8b-ac8be7803d09"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.5 MB 14.4 MB/s \n","\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n","\u001b[K     |████████████████████████████████| 6.8 MB 59.4 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 61.3 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 59.7 MB/s \n","\u001b[K     |████████████████████████████████| 308 kB 15.3 MB/s \n","\u001b[K     |████████████████████████████████| 210 kB 87.8 MB/s \n","\u001b[K     |████████████████████████████████| 80 kB 11.3 MB/s \n","\u001b[K     |████████████████████████████████| 75 kB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 49 kB 6.4 MB/s \n","\u001b[K     |████████████████████████████████| 113 kB 67.9 MB/s \n","\u001b[K     |████████████████████████████████| 149 kB 68.1 MB/s \n","\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 311 kB 13.6 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 69.6 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 79.3 MB/s \n","\u001b[K     |████████████████████████████████| 133 kB 96.2 MB/s \n","\u001b[K     |████████████████████████████████| 144 kB 82.8 MB/s \n","\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 65.2 MB/s \n","\u001b[?25h"]}],"source":["!pip install -qq transformers\n","!pip install -qq optuna\n","!pip install -qq datasets"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"Rz6wNlu92ge_","executionInfo":{"status":"ok","timestamp":1644756623316,"user_tz":0,"elapsed":9075,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["import transformers\n","import datasets\n","from transformers import AutoTokenizer,AutoModelForQuestionAnswering, AutoModelForSequenceClassification,AdamW, get_linear_schedule_with_warmup,Trainer, TrainingArguments\n","from transformers import DataCollator, DataCollatorForLanguageModeling,default_data_collator\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","import torch\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from pylab import rcParams\n","import matplotlib.pyplot as plt\n","from matplotlib import rc\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, classification_report\n","from collections import defaultdict\n","import random\n","from textwrap import wrap\n","from datetime import datetime\n","from datasets import load_from_disk\n","from datasets import load_dataset\n","from datasets import Dataset\n","from sklearn.metrics import accuracy_score,classification_report, confusion_matrix\n","from sklearn.metrics import precision_recall_fscore_support"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"T7IFr4-3TKaA","executionInfo":{"status":"ok","timestamp":1644756623317,"user_tz":0,"elapsed":22,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["# the model we gonna train, base uncased BERT\n","# check text classification models here: https://huggingface.co/models?filter=text-classification\n","MODEL_NAME = \"distilbert-base-uncased\"\n","# max sequence length for each document/sentence sample\n","BATCH_SIZE = 16\n","EPOCHS = 3\n","LEARNING_RATE= 6.58e-5\n","WEIGHT_DECAY = 0.289\n","WARMUP_STEPS = 464\n","RANDOM_SEED=22\n","\n","\n","\n","\n","QA_OUTPUT_PATH= \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\"\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YoKXcvyo_X47","executionInfo":{"status":"ok","timestamp":1644756822959,"user_tz":0,"elapsed":342,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["def set_seed(seed):\n","    \"\"\"Set all seeds to make results reproducible (deterministic mode).\n","       When seed is None, disables deterministic mode.\n","    :param seed: an integer to your choosing\n","    \"\"\"\n","    if seed is not None:\n","        torch.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","        np.random.seed(seed)\n","        random.seed(seed)\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","  # calculate accuracy using sklearn's function\n","  acc = accuracy_score(labels, preds)\n","  precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n","  acc = accuracy_score(labels, preds)\n","  confusion_matrix = classification_report(labels, preds, digits=4,output_dict=True)\n","  return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","        'hate_f1': confusion_matrix[\"0\"][\"f1-score\"],\n","        'hate_recall': confusion_matrix[\"0\"][\"recall\"],\n","        'hate_precision': confusion_matrix[\"0\"][\"precision\"],\n","        'offensive_f1': confusion_matrix[\"1\"][\"f1-score\"],\n","        'offensive_recall': confusion_matrix[\"1\"][\"recall\"],\n","        'offensive_precision': confusion_matrix[\"1\"][\"precision\"],\n","        'normal_f1': confusion_matrix[\"2\"][\"f1-score\"],\n","        'normal_recall': confusion_matrix[\"2\"][\"recall\"],\n","        'normal_precision': confusion_matrix[\"2\"][\"precision\"],    \n","  }\n","\n","\n","def seq_model_init():\n","  temp_model =  AutoModelForSequenceClassification.from_pretrained(QA_OUTPUT_PATH,num_labels=3).to(device)\n","  return temp_model\n","\n","def qa_model_init():\n","  temp_model =  AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME).to(device)\n","  return temp_model\n","\n","\n","\n","def timestamp():\n","    dateTimeObj = datetime.now()\n","    timestampStr = dateTimeObj.strftime(\"%d-%b-%Y (%H:%M:%S.%f)\")\n","    print(timestampStr)\n","\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lqKiS7jbkC4x","executionInfo":{"status":"ok","timestamp":1644756623319,"user_tz":0,"elapsed":20,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["set_seed(RANDOM_SEED)\n","\n","\n"]},{"cell_type":"code","source":["dataset_dfs = load_from_disk('/content/drive/MyDrive/Dissertation/datasets/hatetwit_'+str(1))"],"metadata":{"id":"0DtMt9ngFcUt","executionInfo":{"status":"ok","timestamp":1644756629544,"user_tz":0,"elapsed":6244,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir='/content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results',          # output directory\n","    num_train_epochs=EPOCHS,              # total number of training epochs\n","    save_strategy =\"epoch\" ,\n","    per_device_train_batch_size=BATCH_SIZE,  # batch size per device during training\n","    per_device_eval_batch_size=BATCH_SIZE,   # batch size for evaluation\n","    weight_decay= WEIGHT_DECAY,               # strength of weight decay\n","    learning_rate= LEARNING_RATE, \n","    logging_dir='./disbert_hate_task/hyper/logs',     # directory for storing logs\n","    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","    evaluation_strategy=\"epoch\",\n","    #eval_steps = 500     # evaluate each `logging_steps`\n",")"],"metadata":{"id":"qO86KFanHLcD","executionInfo":{"status":"ok","timestamp":1644756629546,"user_tz":0,"elapsed":7,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jo4n81tx6lbr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644756835551,"user_tz":0,"elapsed":1261,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"ab753663-e24d-4dc1-b903-1c7eaf610def"},"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["hyper_trainer = Trainer(\n","    model_init=seq_model_init,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=dataset_dfs['train'],         # training dataset\n","    eval_dataset=dataset_dfs['validation'],          # evaluation dataset\n","    compute_metrics=compute_metrics     # the callback that computes metrics of interest\n",")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"khwJ-nkJrtka","executionInfo":{"status":"ok","timestamp":1644756840330,"user_tz":0,"elapsed":337,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}}},"outputs":[],"source":["def hp_space_optuna(trial) :\n","    return {\n","        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n","        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 5),\n","        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n","        \"warmup_steps\": trial.suggest_int(\"warmup_steps\", 0, 500),\n","        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0, 0.3),\n","        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [ 8, 16, 32, 64]),\n","    }"]},{"cell_type":"code","source":["best_trial = hyper_trainer.hyperparameter_search(n_trials=40, direction=\"maximize\", backend=\"optuna\", hp_space=hp_space_optuna)"],"metadata":{"id":"PON0aGtf78Aw","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1644766897025,"user_tz":0,"elapsed":2336415,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"d9ab5611-98b7-4626-ad0b-26edc3ef2cd5"},"execution_count":13,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[32m[I 2022-02-13 12:54:05,467]\u001b[0m A new study created in memory with name: no-name-18c76278-872b-4904-bc44-8d11353d1bf1\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3495' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3495/3495 04:19, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.797000</td>\n","      <td>0.700663</td>\n","      <td>0.703799</td>\n","      <td>0.636078</td>\n","      <td>0.639799</td>\n","      <td>0.632761</td>\n","      <td>0.621901</td>\n","      <td>0.605634</td>\n","      <td>0.639066</td>\n","      <td>0.813528</td>\n","      <td>0.823769</td>\n","      <td>0.803539</td>\n","      <td>0.472803</td>\n","      <td>0.468880</td>\n","      <td>0.476793</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.679800</td>\n","      <td>0.647603</td>\n","      <td>0.732561</td>\n","      <td>0.675370</td>\n","      <td>0.677571</td>\n","      <td>0.673289</td>\n","      <td>0.659195</td>\n","      <td>0.650905</td>\n","      <td>0.667699</td>\n","      <td>0.826686</td>\n","      <td>0.832655</td>\n","      <td>0.820803</td>\n","      <td>0.540230</td>\n","      <td>0.536307</td>\n","      <td>0.544211</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.649900</td>\n","      <td>0.637425</td>\n","      <td>0.739429</td>\n","      <td>0.676274</td>\n","      <td>0.687565</td>\n","      <td>0.668291</td>\n","      <td>0.665986</td>\n","      <td>0.655936</td>\n","      <td>0.676349</td>\n","      <td>0.831304</td>\n","      <td>0.859311</td>\n","      <td>0.805064</td>\n","      <td>0.531532</td>\n","      <td>0.489627</td>\n","      <td>0.581281</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-1165\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-1165/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-1165/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-0/checkpoint-3495 (score: 0.6374251246452332).\n","\u001b[32m[I 2022-02-13 12:58:26,580]\u001b[0m Trial 0 finished with value: 8.867946391598666 and parameters: {'learning_rate': 2.5571096321455773e-06, 'num_train_epochs': 3, 'seed': 34, 'warmup_steps': 430, 'weight_decay': 0.011109926323566022, 'per_device_train_batch_size': 32}. Best is trial 0 with value: 8.867946391598666.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 11650\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11650' max='11650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11650/11650 09:06, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.580300</td>\n","      <td>0.556196</td>\n","      <td>0.778064</td>\n","      <td>0.730080</td>\n","      <td>0.731921</td>\n","      <td>0.728721</td>\n","      <td>0.714002</td>\n","      <td>0.723340</td>\n","      <td>0.704902</td>\n","      <td>0.856249</td>\n","      <td>0.861163</td>\n","      <td>0.851391</td>\n","      <td>0.619989</td>\n","      <td>0.601660</td>\n","      <td>0.639471</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.489800</td>\n","      <td>0.513696</td>\n","      <td>0.797167</td>\n","      <td>0.748144</td>\n","      <td>0.762716</td>\n","      <td>0.738745</td>\n","      <td>0.748122</td>\n","      <td>0.751509</td>\n","      <td>0.744766</td>\n","      <td>0.867444</td>\n","      <td>0.895224</td>\n","      <td>0.841336</td>\n","      <td>0.628866</td>\n","      <td>0.569502</td>\n","      <td>0.702046</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.399800</td>\n","      <td>0.519182</td>\n","      <td>0.804894</td>\n","      <td>0.756247</td>\n","      <td>0.765966</td>\n","      <td>0.750074</td>\n","      <td>0.762141</td>\n","      <td>0.773642</td>\n","      <td>0.750977</td>\n","      <td>0.876742</td>\n","      <td>0.896705</td>\n","      <td>0.857649</td>\n","      <td>0.629859</td>\n","      <td>0.579876</td>\n","      <td>0.689273</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.320400</td>\n","      <td>0.557025</td>\n","      <td>0.797811</td>\n","      <td>0.758508</td>\n","      <td>0.751173</td>\n","      <td>0.767863</td>\n","      <td>0.763395</td>\n","      <td>0.809859</td>\n","      <td>0.721973</td>\n","      <td>0.867867</td>\n","      <td>0.847464</td>\n","      <td>0.889277</td>\n","      <td>0.644261</td>\n","      <td>0.646266</td>\n","      <td>0.642268</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.257700</td>\n","      <td>0.596510</td>\n","      <td>0.801460</td>\n","      <td>0.757741</td>\n","      <td>0.755783</td>\n","      <td>0.761129</td>\n","      <td>0.769009</td>\n","      <td>0.803823</td>\n","      <td>0.737085</td>\n","      <td>0.873255</td>\n","      <td>0.868567</td>\n","      <td>0.877994</td>\n","      <td>0.630959</td>\n","      <td>0.610996</td>\n","      <td>0.652270</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-6990/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-9320\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-9320/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-9320/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-11650\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-11650/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-11650/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-1/checkpoint-4660 (score: 0.5136959552764893).\n","\u001b[32m[I 2022-02-13 13:07:34,446]\u001b[0m Trial 1 finished with value: 9.900069758099775 and parameters: {'learning_rate': 1.4623052568622517e-05, 'num_train_epochs': 5, 'seed': 3, 'warmup_steps': 110, 'weight_decay': 0.28523918628219885, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 9.900069758099775.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9320\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9320' max='9320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [9320/9320 07:14, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.669400</td>\n","      <td>0.637030</td>\n","      <td>0.745010</td>\n","      <td>0.676227</td>\n","      <td>0.697930</td>\n","      <td>0.666750</td>\n","      <td>0.677612</td>\n","      <td>0.685111</td>\n","      <td>0.670276</td>\n","      <td>0.835215</td>\n","      <td>0.876342</td>\n","      <td>0.797776</td>\n","      <td>0.515854</td>\n","      <td>0.438797</td>\n","      <td>0.625740</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.615100</td>\n","      <td>0.596233</td>\n","      <td>0.763039</td>\n","      <td>0.704120</td>\n","      <td>0.725571</td>\n","      <td>0.689396</td>\n","      <td>0.692023</td>\n","      <td>0.658954</td>\n","      <td>0.728587</td>\n","      <td>0.844163</td>\n","      <td>0.887449</td>\n","      <td>0.804903</td>\n","      <td>0.576174</td>\n","      <td>0.521784</td>\n","      <td>0.643223</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.561100</td>\n","      <td>0.566537</td>\n","      <td>0.770552</td>\n","      <td>0.718846</td>\n","      <td>0.726113</td>\n","      <td>0.713377</td>\n","      <td>0.712851</td>\n","      <td>0.714286</td>\n","      <td>0.711423</td>\n","      <td>0.849419</td>\n","      <td>0.866716</td>\n","      <td>0.832800</td>\n","      <td>0.594267</td>\n","      <td>0.559129</td>\n","      <td>0.634118</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.548200</td>\n","      <td>0.561396</td>\n","      <td>0.771410</td>\n","      <td>0.720427</td>\n","      <td>0.725389</td>\n","      <td>0.716890</td>\n","      <td>0.714712</td>\n","      <td>0.723340</td>\n","      <td>0.706287</td>\n","      <td>0.850730</td>\n","      <td>0.863014</td>\n","      <td>0.838791</td>\n","      <td>0.595838</td>\n","      <td>0.564315</td>\n","      <td>0.631090</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-6990/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-9320\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-9320/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-9320/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-2/checkpoint-9320 (score: 0.5613958239555359).\n","\u001b[32m[I 2022-02-13 13:14:50,322]\u001b[0m Trial 2 finished with value: 9.422232709063763 and parameters: {'learning_rate': 3.46963520537546e-06, 'num_train_epochs': 4, 'seed': 23, 'warmup_steps': 234, 'weight_decay': 0.18635067151502518, 'per_device_train_batch_size': 16}. Best is trial 1 with value: 9.900069758099775.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2915\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2915' max='2915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2915/2915 06:21, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.679400</td>\n","      <td>0.552414</td>\n","      <td>0.778493</td>\n","      <td>0.716595</td>\n","      <td>0.752796</td>\n","      <td>0.700470</td>\n","      <td>0.703815</td>\n","      <td>0.705231</td>\n","      <td>0.702405</td>\n","      <td>0.857143</td>\n","      <td>0.909663</td>\n","      <td>0.810356</td>\n","      <td>0.588826</td>\n","      <td>0.486515</td>\n","      <td>0.745628</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.489300</td>\n","      <td>0.502466</td>\n","      <td>0.795450</td>\n","      <td>0.739146</td>\n","      <td>0.769999</td>\n","      <td>0.732674</td>\n","      <td>0.755847</td>\n","      <td>0.812877</td>\n","      <td>0.706294</td>\n","      <td>0.866928</td>\n","      <td>0.899667</td>\n","      <td>0.836489</td>\n","      <td>0.594663</td>\n","      <td>0.485477</td>\n","      <td>0.767213</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.349500</td>\n","      <td>0.543707</td>\n","      <td>0.810474</td>\n","      <td>0.768567</td>\n","      <td>0.766471</td>\n","      <td>0.770992</td>\n","      <td>0.784661</td>\n","      <td>0.802817</td>\n","      <td>0.767308</td>\n","      <td>0.880580</td>\n","      <td>0.876342</td>\n","      <td>0.884860</td>\n","      <td>0.640461</td>\n","      <td>0.633817</td>\n","      <td>0.647246</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.221000</td>\n","      <td>0.661457</td>\n","      <td>0.806611</td>\n","      <td>0.765572</td>\n","      <td>0.762249</td>\n","      <td>0.770511</td>\n","      <td>0.787937</td>\n","      <td>0.827968</td>\n","      <td>0.751598</td>\n","      <td>0.874113</td>\n","      <td>0.866346</td>\n","      <td>0.882020</td>\n","      <td>0.634667</td>\n","      <td>0.617220</td>\n","      <td>0.653128</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.125700</td>\n","      <td>0.850227</td>\n","      <td>0.805323</td>\n","      <td>0.766595</td>\n","      <td>0.761754</td>\n","      <td>0.771939</td>\n","      <td>0.791953</td>\n","      <td>0.811871</td>\n","      <td>0.772989</td>\n","      <td>0.872912</td>\n","      <td>0.860792</td>\n","      <td>0.885377</td>\n","      <td>0.634921</td>\n","      <td>0.643154</td>\n","      <td>0.626896</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1749/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2332\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2332/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2332/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2915\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2915/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-2915/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-3/checkpoint-1166 (score: 0.5024662613868713).\n","\u001b[32m[I 2022-02-13 13:21:14,388]\u001b[0m Trial 3 finished with value: 10.006474397743213 and parameters: {'learning_rate': 9.974369890954737e-05, 'num_train_epochs': 5, 'seed': 33, 'warmup_steps': 182, 'weight_decay': 0.16387717915373545, 'per_device_train_batch_size': 64}. Best is trial 3 with value: 10.006474397743213.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='23295' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23295/23295 13:39, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.571700</td>\n","      <td>0.551664</td>\n","      <td>0.781069</td>\n","      <td>0.733040</td>\n","      <td>0.737610</td>\n","      <td>0.733738</td>\n","      <td>0.721281</td>\n","      <td>0.770624</td>\n","      <td>0.677876</td>\n","      <td>0.856721</td>\n","      <td>0.860052</td>\n","      <td>0.853417</td>\n","      <td>0.621118</td>\n","      <td>0.570539</td>\n","      <td>0.681537</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.467000</td>\n","      <td>0.556067</td>\n","      <td>0.793518</td>\n","      <td>0.739675</td>\n","      <td>0.766980</td>\n","      <td>0.727658</td>\n","      <td>0.741904</td>\n","      <td>0.760563</td>\n","      <td>0.724138</td>\n","      <td>0.864377</td>\n","      <td>0.903739</td>\n","      <td>0.828300</td>\n","      <td>0.612745</td>\n","      <td>0.518672</td>\n","      <td>0.748503</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.397100</td>\n","      <td>0.617469</td>\n","      <td>0.802103</td>\n","      <td>0.755061</td>\n","      <td>0.763743</td>\n","      <td>0.752837</td>\n","      <td>0.760362</td>\n","      <td>0.802817</td>\n","      <td>0.722172</td>\n","      <td>0.871851</td>\n","      <td>0.884117</td>\n","      <td>0.859921</td>\n","      <td>0.632970</td>\n","      <td>0.571577</td>\n","      <td>0.709138</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.338700</td>\n","      <td>0.711903</td>\n","      <td>0.804250</td>\n","      <td>0.762613</td>\n","      <td>0.762570</td>\n","      <td>0.763515</td>\n","      <td>0.770283</td>\n","      <td>0.792757</td>\n","      <td>0.749049</td>\n","      <td>0.871947</td>\n","      <td>0.872270</td>\n","      <td>0.871624</td>\n","      <td>0.645610</td>\n","      <td>0.625519</td>\n","      <td>0.667035</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.253400</td>\n","      <td>0.834390</td>\n","      <td>0.803391</td>\n","      <td>0.761441</td>\n","      <td>0.759088</td>\n","      <td>0.765756</td>\n","      <td>0.765856</td>\n","      <td>0.807847</td>\n","      <td>0.728015</td>\n","      <td>0.872924</td>\n","      <td>0.865976</td>\n","      <td>0.879985</td>\n","      <td>0.645542</td>\n","      <td>0.623444</td>\n","      <td>0.669265</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-9318/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-13977\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-13977/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-13977/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-18636\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-18636/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-18636/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-23295\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-23295/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-23295/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-4/checkpoint-4659 (score: 0.5516642332077026).\n","\u001b[32m[I 2022-02-13 13:34:54,683]\u001b[0m Trial 4 finished with value: 9.948529400519863 and parameters: {'learning_rate': 1.39758486966422e-05, 'num_train_epochs': 5, 'seed': 11, 'warmup_steps': 125, 'weight_decay': 0.27164161637192047, 'per_device_train_batch_size': 8}. Best is trial 3 with value: 10.006474397743213.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1749\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1167' max='1749' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1167/1749 02:28 < 01:14, 7.83 it/s, Epoch 2/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.747300</td>\n","      <td>0.563203</td>\n","      <td>0.773342</td>\n","      <td>0.723126</td>\n","      <td>0.727291</td>\n","      <td>0.726724</td>\n","      <td>0.714745</td>\n","      <td>0.782696</td>\n","      <td>0.657650</td>\n","      <td>0.852690</td>\n","      <td>0.850796</td>\n","      <td>0.854593</td>\n","      <td>0.601942</td>\n","      <td>0.546680</td>\n","      <td>0.669632</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.536200</td>\n","      <td>0.512179</td>\n","      <td>0.785791</td>\n","      <td>0.737764</td>\n","      <td>0.741421</td>\n","      <td>0.736113</td>\n","      <td>0.738281</td>\n","      <td>0.760563</td>\n","      <td>0.717268</td>\n","      <td>0.860811</td>\n","      <td>0.868937</td>\n","      <td>0.852834</td>\n","      <td>0.614199</td>\n","      <td>0.578838</td>\n","      <td>0.654162</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-5/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 13:37:28,307]\u001b[0m Trial 5 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 13977\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='9319' max='13977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 9316/13977 05:17 < 02:38, 29.37 it/s, Epoch 2.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.578000</td>\n","      <td>0.554058</td>\n","      <td>0.777849</td>\n","      <td>0.731805</td>\n","      <td>0.729799</td>\n","      <td>0.735199</td>\n","      <td>0.717235</td>\n","      <td>0.751509</td>\n","      <td>0.685950</td>\n","      <td>0.855651</td>\n","      <td>0.849315</td>\n","      <td>0.862082</td>\n","      <td>0.622531</td>\n","      <td>0.604772</td>\n","      <td>0.641364</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.493000</td>\n","      <td>0.530972</td>\n","      <td>0.786649</td>\n","      <td>0.745075</td>\n","      <td>0.740090</td>\n","      <td>0.750668</td>\n","      <td>0.734834</td>\n","      <td>0.755533</td>\n","      <td>0.715238</td>\n","      <td>0.860636</td>\n","      <td>0.847094</td>\n","      <td>0.874618</td>\n","      <td>0.639755</td>\n","      <td>0.649378</td>\n","      <td>0.630413</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-6/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 13:42:50,179]\u001b[0m Trial 6 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='13978' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13975/23295 08:04 < 05:22, 28.86 it/s, Epoch 3.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.579100</td>\n","      <td>0.565941</td>\n","      <td>0.778279</td>\n","      <td>0.715634</td>\n","      <td>0.758472</td>\n","      <td>0.696344</td>\n","      <td>0.719713</td>\n","      <td>0.705231</td>\n","      <td>0.734801</td>\n","      <td>0.853398</td>\n","      <td>0.915957</td>\n","      <td>0.798838</td>\n","      <td>0.573791</td>\n","      <td>0.467842</td>\n","      <td>0.741776</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.492500</td>\n","      <td>0.521914</td>\n","      <td>0.791801</td>\n","      <td>0.749753</td>\n","      <td>0.745061</td>\n","      <td>0.756538</td>\n","      <td>0.749645</td>\n","      <td>0.796781</td>\n","      <td>0.707775</td>\n","      <td>0.864021</td>\n","      <td>0.850426</td>\n","      <td>0.878058</td>\n","      <td>0.635593</td>\n","      <td>0.622407</td>\n","      <td>0.649351</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.419700</td>\n","      <td>0.585296</td>\n","      <td>0.793303</td>\n","      <td>0.744340</td>\n","      <td>0.751940</td>\n","      <td>0.738685</td>\n","      <td>0.755266</td>\n","      <td>0.757545</td>\n","      <td>0.753000</td>\n","      <td>0.867042</td>\n","      <td>0.884857</td>\n","      <td>0.849929</td>\n","      <td>0.610712</td>\n","      <td>0.573651</td>\n","      <td>0.652893</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-7/checkpoint-9318/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 13:50:58,989]\u001b[0m Trial 7 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4655/9318 02:34 < 02:34, 30.10 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.738500</td>\n","      <td>0.710771</td>\n","      <td>0.692638</td>\n","      <td>0.597764</td>\n","      <td>0.619012</td>\n","      <td>0.592334</td>\n","      <td>0.631472</td>\n","      <td>0.625755</td>\n","      <td>0.637295</td>\n","      <td>0.804220</td>\n","      <td>0.860792</td>\n","      <td>0.754625</td>\n","      <td>0.357599</td>\n","      <td>0.290456</td>\n","      <td>0.465116</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 13:53:38,309]\u001b[0m Trial 8 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/3495 01:19 < 02:38, 14.67 it/s, Epoch 1.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.630400</td>\n","      <td>0.562458</td>\n","      <td>0.772483</td>\n","      <td>0.722209</td>\n","      <td>0.725708</td>\n","      <td>0.721068</td>\n","      <td>0.718447</td>\n","      <td>0.744467</td>\n","      <td>0.694184</td>\n","      <td>0.851165</td>\n","      <td>0.858571</td>\n","      <td>0.843886</td>\n","      <td>0.597015</td>\n","      <td>0.560166</td>\n","      <td>0.639053</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 13:55:02,153]\u001b[0m Trial 9 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2332' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2332/2332 05:07, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.650200</td>\n","      <td>0.530578</td>\n","      <td>0.782786</td>\n","      <td>0.727768</td>\n","      <td>0.747281</td>\n","      <td>0.719400</td>\n","      <td>0.721359</td>\n","      <td>0.747485</td>\n","      <td>0.696998</td>\n","      <td>0.859188</td>\n","      <td>0.888930</td>\n","      <td>0.831371</td>\n","      <td>0.602756</td>\n","      <td>0.521784</td>\n","      <td>0.713475</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.480000</td>\n","      <td>0.474851</td>\n","      <td>0.807040</td>\n","      <td>0.762352</td>\n","      <td>0.764342</td>\n","      <td>0.762781</td>\n","      <td>0.770120</td>\n","      <td>0.803823</td>\n","      <td>0.739130</td>\n","      <td>0.877374</td>\n","      <td>0.880785</td>\n","      <td>0.873990</td>\n","      <td>0.639560</td>\n","      <td>0.603734</td>\n","      <td>0.679907</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.348900</td>\n","      <td>0.531634</td>\n","      <td>0.808328</td>\n","      <td>0.764564</td>\n","      <td>0.766608</td>\n","      <td>0.765696</td>\n","      <td>0.790230</td>\n","      <td>0.829980</td>\n","      <td>0.754113</td>\n","      <td>0.875207</td>\n","      <td>0.878934</td>\n","      <td>0.871512</td>\n","      <td>0.628255</td>\n","      <td>0.588174</td>\n","      <td>0.674197</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.220200</td>\n","      <td>0.632754</td>\n","      <td>0.808113</td>\n","      <td>0.769909</td>\n","      <td>0.763140</td>\n","      <td>0.778025</td>\n","      <td>0.792127</td>\n","      <td>0.829980</td>\n","      <td>0.757576</td>\n","      <td>0.875000</td>\n","      <td>0.857830</td>\n","      <td>0.892871</td>\n","      <td>0.642599</td>\n","      <td>0.646266</td>\n","      <td>0.638974</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1749/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-2332\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-2332/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-2332/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-10/checkpoint-1166 (score: 0.47485125064849854).\n","\u001b[32m[I 2022-02-13 14:00:10,868]\u001b[0m Trial 10 finished with value: 10.05241054638603 and parameters: {'learning_rate': 8.584684132528283e-05, 'num_train_epochs': 4, 'seed': 32, 'warmup_steps': 50, 'weight_decay': 0.10919253165395515, 'per_device_train_batch_size': 64}. Best is trial 10 with value: 10.05241054638603.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2333' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2332/2332 04:58, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.642000</td>\n","      <td>0.538342</td>\n","      <td>0.784718</td>\n","      <td>0.728843</td>\n","      <td>0.752795</td>\n","      <td>0.715052</td>\n","      <td>0.722110</td>\n","      <td>0.716298</td>\n","      <td>0.728016</td>\n","      <td>0.860018</td>\n","      <td>0.901888</td>\n","      <td>0.821862</td>\n","      <td>0.604402</td>\n","      <td>0.526971</td>\n","      <td>0.708508</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.470600</td>\n","      <td>0.499259</td>\n","      <td>0.785147</td>\n","      <td>0.747685</td>\n","      <td>0.736190</td>\n","      <td>0.763802</td>\n","      <td>0.759777</td>\n","      <td>0.820926</td>\n","      <td>0.707106</td>\n","      <td>0.857530</td>\n","      <td>0.820067</td>\n","      <td>0.898580</td>\n","      <td>0.625749</td>\n","      <td>0.650415</td>\n","      <td>0.602885</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.330000</td>\n","      <td>0.570377</td>\n","      <td>0.803391</td>\n","      <td>0.762339</td>\n","      <td>0.764553</td>\n","      <td>0.761138</td>\n","      <td>0.786594</td>\n","      <td>0.802817</td>\n","      <td>0.771014</td>\n","      <td>0.867647</td>\n","      <td>0.873750</td>\n","      <td>0.861628</td>\n","      <td>0.632774</td>\n","      <td>0.606846</td>\n","      <td>0.661017</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.203200</td>\n","      <td>0.704024</td>\n","      <td>0.798669</td>\n","      <td>0.760887</td>\n","      <td>0.754323</td>\n","      <td>0.768495</td>\n","      <td>0.787409</td>\n","      <td>0.817907</td>\n","      <td>0.759104</td>\n","      <td>0.865396</td>\n","      <td>0.848575</td>\n","      <td>0.882897</td>\n","      <td>0.629857</td>\n","      <td>0.639004</td>\n","      <td>0.620968</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-11/checkpoint-1749/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:05:13,860]\u001b[0m Trial 11 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2332' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2332/2332 05:07, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.639200</td>\n","      <td>0.539692</td>\n","      <td>0.787293</td>\n","      <td>0.731363</td>\n","      <td>0.754401</td>\n","      <td>0.719170</td>\n","      <td>0.711443</td>\n","      <td>0.719316</td>\n","      <td>0.703740</td>\n","      <td>0.864596</td>\n","      <td>0.901888</td>\n","      <td>0.830266</td>\n","      <td>0.618051</td>\n","      <td>0.536307</td>\n","      <td>0.729196</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.475500</td>\n","      <td>0.498909</td>\n","      <td>0.798669</td>\n","      <td>0.749612</td>\n","      <td>0.761380</td>\n","      <td>0.743170</td>\n","      <td>0.754050</td>\n","      <td>0.772636</td>\n","      <td>0.736337</td>\n","      <td>0.869785</td>\n","      <td>0.891522</td>\n","      <td>0.849083</td>\n","      <td>0.625000</td>\n","      <td>0.565353</td>\n","      <td>0.698718</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.343900</td>\n","      <td>0.575402</td>\n","      <td>0.804679</td>\n","      <td>0.767068</td>\n","      <td>0.759775</td>\n","      <td>0.775613</td>\n","      <td>0.781128</td>\n","      <td>0.807847</td>\n","      <td>0.756121</td>\n","      <td>0.873223</td>\n","      <td>0.853017</td>\n","      <td>0.894410</td>\n","      <td>0.646851</td>\n","      <td>0.665975</td>\n","      <td>0.628795</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.225000</td>\n","      <td>0.668695</td>\n","      <td>0.800386</td>\n","      <td>0.760363</td>\n","      <td>0.756580</td>\n","      <td>0.764546</td>\n","      <td>0.780392</td>\n","      <td>0.800805</td>\n","      <td>0.760994</td>\n","      <td>0.868898</td>\n","      <td>0.860052</td>\n","      <td>0.877929</td>\n","      <td>0.631797</td>\n","      <td>0.632780</td>\n","      <td>0.630817</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1749\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1749/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1749/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-2332\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-2332/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-2332/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-12/checkpoint-1166 (score: 0.4989085793495178).\n","\u001b[32m[I 2022-02-13 14:10:22,319]\u001b[0m Trial 12 finished with value: 9.926339040295636 and parameters: {'learning_rate': 9.81523990875988e-05, 'num_train_epochs': 4, 'seed': 33, 'warmup_steps': 0, 'weight_decay': 0.11097824132996897, 'per_device_train_batch_size': 64}. Best is trial 10 with value: 10.05241054638603.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2915\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2915 01:09 < 04:37, 8.39 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.685300</td>\n","      <td>0.547675</td>\n","      <td>0.775059</td>\n","      <td>0.724466</td>\n","      <td>0.728140</td>\n","      <td>0.731516</td>\n","      <td>0.712243</td>\n","      <td>0.801811</td>\n","      <td>0.640675</td>\n","      <td>0.857250</td>\n","      <td>0.847094</td>\n","      <td>0.867653</td>\n","      <td>0.603904</td>\n","      <td>0.545643</td>\n","      <td>0.676093</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:11:36,345]\u001b[0m Trial 13 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2332 01:09 < 03:27, 8.43 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.684700</td>\n","      <td>0.544548</td>\n","      <td>0.780640</td>\n","      <td>0.729243</td>\n","      <td>0.739614</td>\n","      <td>0.720717</td>\n","      <td>0.709062</td>\n","      <td>0.673038</td>\n","      <td>0.749160</td>\n","      <td>0.860599</td>\n","      <td>0.882266</td>\n","      <td>0.839972</td>\n","      <td>0.618067</td>\n","      <td>0.606846</td>\n","      <td>0.629709</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:12:49,965]\u001b[0m Trial 14 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2915\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2915 01:09 < 04:36, 8.42 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.690200</td>\n","      <td>0.535196</td>\n","      <td>0.773342</td>\n","      <td>0.715458</td>\n","      <td>0.735029</td>\n","      <td>0.704307</td>\n","      <td>0.714645</td>\n","      <td>0.714286</td>\n","      <td>0.715005</td>\n","      <td>0.852378</td>\n","      <td>0.889300</td>\n","      <td>0.818399</td>\n","      <td>0.579351</td>\n","      <td>0.509336</td>\n","      <td>0.671683</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:14:03,716]\u001b[0m Trial 15 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2332 01:09 < 03:28, 8.38 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.709600</td>\n","      <td>0.545459</td>\n","      <td>0.777420</td>\n","      <td>0.716524</td>\n","      <td>0.744671</td>\n","      <td>0.705915</td>\n","      <td>0.719649</td>\n","      <td>0.742455</td>\n","      <td>0.698202</td>\n","      <td>0.856184</td>\n","      <td>0.897075</td>\n","      <td>0.818858</td>\n","      <td>0.573740</td>\n","      <td>0.478216</td>\n","      <td>0.716952</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:15:17,771]\u001b[0m Trial 16 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1166\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='1166' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1166/1166 02:32, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.720700</td>\n","      <td>0.544033</td>\n","      <td>0.780854</td>\n","      <td>0.729183</td>\n","      <td>0.743043</td>\n","      <td>0.724432</td>\n","      <td>0.715579</td>\n","      <td>0.755533</td>\n","      <td>0.679638</td>\n","      <td>0.856677</td>\n","      <td>0.875231</td>\n","      <td>0.838893</td>\n","      <td>0.615294</td>\n","      <td>0.542531</td>\n","      <td>0.710598</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.504500</td>\n","      <td>0.505471</td>\n","      <td>0.794591</td>\n","      <td>0.745887</td>\n","      <td>0.754155</td>\n","      <td>0.742149</td>\n","      <td>0.745878</td>\n","      <td>0.773642</td>\n","      <td>0.720037</td>\n","      <td>0.867492</td>\n","      <td>0.882266</td>\n","      <td>0.853204</td>\n","      <td>0.624291</td>\n","      <td>0.570539</td>\n","      <td>0.689223</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-1166\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-1166/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-1166/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-17/checkpoint-1166 (score: 0.505470871925354).\n","\u001b[32m[I 2022-02-13 14:17:51,583]\u001b[0m Trial 17 finished with value: 9.763354080442554 and parameters: {'learning_rate': 6.411683282434578e-05, 'num_train_epochs': 2, 'seed': 40, 'warmup_steps': 324, 'weight_decay': 0.215574021763272, 'per_device_train_batch_size': 64}. Best is trial 10 with value: 10.05241054638603.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5825\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='5825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/5825 01:20 < 05:23, 14.40 it/s, Epoch 1.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.669800</td>\n","      <td>0.618035</td>\n","      <td>0.749732</td>\n","      <td>0.686398</td>\n","      <td>0.700624</td>\n","      <td>0.683227</td>\n","      <td>0.689786</td>\n","      <td>0.730382</td>\n","      <td>0.653465</td>\n","      <td>0.837838</td>\n","      <td>0.860792</td>\n","      <td>0.816076</td>\n","      <td>0.531569</td>\n","      <td>0.458506</td>\n","      <td>0.632332</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:19:17,084]\u001b[0m Trial 18 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9320\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='9320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/9320 01:42 < 05:08, 22.64 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.565300</td>\n","      <td>0.555174</td>\n","      <td>0.781927</td>\n","      <td>0.724957</td>\n","      <td>0.756655</td>\n","      <td>0.709998</td>\n","      <td>0.724086</td>\n","      <td>0.727364</td>\n","      <td>0.720837</td>\n","      <td>0.854989</td>\n","      <td>0.902629</td>\n","      <td>0.812125</td>\n","      <td>0.595797</td>\n","      <td>0.500000</td>\n","      <td>0.737003</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:21:04,580]\u001b[0m Trial 19 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2915\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1167' max='2915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1167/2915 02:26 < 03:40, 7.93 it/s, Epoch 2/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.659200</td>\n","      <td>0.536042</td>\n","      <td>0.781927</td>\n","      <td>0.739250</td>\n","      <td>0.735443</td>\n","      <td>0.744162</td>\n","      <td>0.723077</td>\n","      <td>0.756539</td>\n","      <td>0.692449</td>\n","      <td>0.856018</td>\n","      <td>0.845243</td>\n","      <td>0.867072</td>\n","      <td>0.638655</td>\n","      <td>0.630705</td>\n","      <td>0.646809</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.478900</td>\n","      <td>0.496441</td>\n","      <td>0.798669</td>\n","      <td>0.743381</td>\n","      <td>0.766429</td>\n","      <td>0.729050</td>\n","      <td>0.756035</td>\n","      <td>0.740443</td>\n","      <td>0.772298</td>\n","      <td>0.872464</td>\n","      <td>0.915587</td>\n","      <td>0.833221</td>\n","      <td>0.601645</td>\n","      <td>0.531120</td>\n","      <td>0.693767</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-20/checkpoint-583\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-20/checkpoint-583/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-20/checkpoint-583/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:23:36,230]\u001b[0m Trial 20 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='23295' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23295/23295 13:53, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.562400</td>\n","      <td>0.531966</td>\n","      <td>0.786006</td>\n","      <td>0.733762</td>\n","      <td>0.746591</td>\n","      <td>0.723913</td>\n","      <td>0.720452</td>\n","      <td>0.705231</td>\n","      <td>0.736345</td>\n","      <td>0.863139</td>\n","      <td>0.890781</td>\n","      <td>0.837161</td>\n","      <td>0.617696</td>\n","      <td>0.575726</td>\n","      <td>0.666267</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.442700</td>\n","      <td>0.546651</td>\n","      <td>0.804035</td>\n","      <td>0.761893</td>\n","      <td>0.760046</td>\n","      <td>0.766126</td>\n","      <td>0.762583</td>\n","      <td>0.807847</td>\n","      <td>0.722122</td>\n","      <td>0.873718</td>\n","      <td>0.867086</td>\n","      <td>0.880451</td>\n","      <td>0.649379</td>\n","      <td>0.623444</td>\n","      <td>0.677565</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.357800</td>\n","      <td>0.642173</td>\n","      <td>0.799099</td>\n","      <td>0.759906</td>\n","      <td>0.750809</td>\n","      <td>0.773109</td>\n","      <td>0.777829</td>\n","      <td>0.847082</td>\n","      <td>0.719044</td>\n","      <td>0.868884</td>\n","      <td>0.841540</td>\n","      <td>0.898064</td>\n","      <td>0.633004</td>\n","      <td>0.630705</td>\n","      <td>0.635319</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.246000</td>\n","      <td>0.861196</td>\n","      <td>0.801889</td>\n","      <td>0.757699</td>\n","      <td>0.756433</td>\n","      <td>0.763637</td>\n","      <td>0.780624</td>\n","      <td>0.843058</td>\n","      <td>0.726800</td>\n","      <td>0.871479</td>\n","      <td>0.864865</td>\n","      <td>0.878195</td>\n","      <td>0.620994</td>\n","      <td>0.582988</td>\n","      <td>0.664303</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.158000</td>\n","      <td>0.984117</td>\n","      <td>0.803821</td>\n","      <td>0.763701</td>\n","      <td>0.758433</td>\n","      <td>0.769901</td>\n","      <td>0.785887</td>\n","      <td>0.817907</td>\n","      <td>0.756279</td>\n","      <td>0.872816</td>\n","      <td>0.860052</td>\n","      <td>0.885965</td>\n","      <td>0.632399</td>\n","      <td>0.631743</td>\n","      <td>0.633056</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-9318/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-13977\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-13977/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-13977/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-18636\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-18636/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-18636/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-23295\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-23295/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-23295/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-21/checkpoint-4659 (score: 0.5319660305976868).\n","\u001b[32m[I 2022-02-13 14:37:31,100]\u001b[0m Trial 21 finished with value: 9.971959052955585 and parameters: {'learning_rate': 2.0552769902311645e-05, 'num_train_epochs': 5, 'seed': 14, 'warmup_steps': 163, 'weight_decay': 0.13687466660718214, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 10.05241054638603.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='23295' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [23295/23295 14:00, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.568100</td>\n","      <td>0.548467</td>\n","      <td>0.783215</td>\n","      <td>0.736354</td>\n","      <td>0.742267</td>\n","      <td>0.732023</td>\n","      <td>0.725373</td>\n","      <td>0.733400</td>\n","      <td>0.717520</td>\n","      <td>0.856153</td>\n","      <td>0.869308</td>\n","      <td>0.843391</td>\n","      <td>0.627537</td>\n","      <td>0.593361</td>\n","      <td>0.665891</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.474000</td>\n","      <td>0.525101</td>\n","      <td>0.799957</td>\n","      <td>0.746459</td>\n","      <td>0.773459</td>\n","      <td>0.738198</td>\n","      <td>0.767908</td>\n","      <td>0.808853</td>\n","      <td>0.730909</td>\n","      <td>0.868234</td>\n","      <td>0.902629</td>\n","      <td>0.836364</td>\n","      <td>0.603234</td>\n","      <td>0.503112</td>\n","      <td>0.753106</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.353000</td>\n","      <td>0.587205</td>\n","      <td>0.803821</td>\n","      <td>0.761745</td>\n","      <td>0.756719</td>\n","      <td>0.769977</td>\n","      <td>0.772897</td>\n","      <td>0.831992</td>\n","      <td>0.721640</td>\n","      <td>0.874906</td>\n","      <td>0.859682</td>\n","      <td>0.890679</td>\n","      <td>0.637433</td>\n","      <td>0.618257</td>\n","      <td>0.657837</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.270500</td>\n","      <td>0.835007</td>\n","      <td>0.804035</td>\n","      <td>0.762613</td>\n","      <td>0.758885</td>\n","      <td>0.770441</td>\n","      <td>0.777520</td>\n","      <td>0.842052</td>\n","      <td>0.722174</td>\n","      <td>0.872228</td>\n","      <td>0.859311</td>\n","      <td>0.885540</td>\n","      <td>0.638090</td>\n","      <td>0.609959</td>\n","      <td>0.668942</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.207100</td>\n","      <td>0.984792</td>\n","      <td>0.804894</td>\n","      <td>0.766149</td>\n","      <td>0.760552</td>\n","      <td>0.773124</td>\n","      <td>0.784483</td>\n","      <td>0.823944</td>\n","      <td>0.748629</td>\n","      <td>0.871332</td>\n","      <td>0.857460</td>\n","      <td>0.885660</td>\n","      <td>0.642633</td>\n","      <td>0.637967</td>\n","      <td>0.647368</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-9318/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-13977\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-13977/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-13977/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-18636\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-18636/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-18636/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-23295\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-23295/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-23295/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-22/checkpoint-9318 (score: 0.5251005291938782).\n","\u001b[32m[I 2022-02-13 14:51:32,143]\u001b[0m Trial 22 finished with value: 10.004194346679117 and parameters: {'learning_rate': 2.1524649232663933e-05, 'num_train_epochs': 5, 'seed': 15, 'warmup_steps': 184, 'weight_decay': 0.13047598924144294, 'per_device_train_batch_size': 8}. Best is trial 10 with value: 10.05241054638603.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18636\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='18636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4659/18636 02:41 < 08:04, 28.84 it/s, Epoch 1.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.575600</td>\n","      <td>0.574471</td>\n","      <td>0.777420</td>\n","      <td>0.730961</td>\n","      <td>0.735846</td>\n","      <td>0.728427</td>\n","      <td>0.715122</td>\n","      <td>0.737425</td>\n","      <td>0.694129</td>\n","      <td>0.850549</td>\n","      <td>0.859682</td>\n","      <td>0.841609</td>\n","      <td>0.627212</td>\n","      <td>0.588174</td>\n","      <td>0.671801</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:54:18,419]\u001b[0m Trial 23 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2915\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2915 01:10 < 04:41, 8.29 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.835500</td>\n","      <td>0.630461</td>\n","      <td>0.744795</td>\n","      <td>0.680312</td>\n","      <td>0.698397</td>\n","      <td>0.670299</td>\n","      <td>0.676426</td>\n","      <td>0.674044</td>\n","      <td>0.678825</td>\n","      <td>0.832595</td>\n","      <td>0.870048</td>\n","      <td>0.798234</td>\n","      <td>0.531915</td>\n","      <td>0.466805</td>\n","      <td>0.618132</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:55:33,414]\u001b[0m Trial 24 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18636\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='18636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/18636 02:40 < 08:01, 29.03 it/s, Epoch 1.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.612400</td>\n","      <td>0.582801</td>\n","      <td>0.767332</td>\n","      <td>0.709711</td>\n","      <td>0.730167</td>\n","      <td>0.704468</td>\n","      <td>0.704385</td>\n","      <td>0.751509</td>\n","      <td>0.662822</td>\n","      <td>0.847025</td>\n","      <td>0.872270</td>\n","      <td>0.823201</td>\n","      <td>0.577723</td>\n","      <td>0.489627</td>\n","      <td>0.704478</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 14:58:18,525]\u001b[0m Trial 25 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 11650\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='11650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 2331/11650 01:43 < 06:53, 22.54 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.609000</td>\n","      <td>0.635831</td>\n","      <td>0.764756</td>\n","      <td>0.709369</td>\n","      <td>0.716365</td>\n","      <td>0.709023</td>\n","      <td>0.701225</td>\n","      <td>0.748491</td>\n","      <td>0.659574</td>\n","      <td>0.848873</td>\n","      <td>0.857830</td>\n","      <td>0.840102</td>\n","      <td>0.578008</td>\n","      <td>0.520747</td>\n","      <td>0.649418</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:00:06,519]\u001b[0m Trial 26 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 5825\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='5825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/5825 01:20 < 05:22, 14.43 it/s, Epoch 1.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.586400</td>\n","      <td>0.543169</td>\n","      <td>0.776991</td>\n","      <td>0.722862</td>\n","      <td>0.736203</td>\n","      <td>0.723211</td>\n","      <td>0.716705</td>\n","      <td>0.787726</td>\n","      <td>0.657431</td>\n","      <td>0.856672</td>\n","      <td>0.866346</td>\n","      <td>0.847212</td>\n","      <td>0.595210</td>\n","      <td>0.515560</td>\n","      <td>0.703966</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:01:31,826]\u001b[0m Trial 27 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2332\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='556' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 556/2332 01:07 < 03:34, 8.26 it/s, Epoch 0.95/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2332' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2332 01:10 < 03:31, 8.27 it/s, Epoch 1/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.774300</td>\n","      <td>0.575539</td>\n","      <td>0.767547</td>\n","      <td>0.715631</td>\n","      <td>0.727958</td>\n","      <td>0.706031</td>\n","      <td>0.693457</td>\n","      <td>0.677062</td>\n","      <td>0.710665</td>\n","      <td>0.844787</td>\n","      <td>0.871529</td>\n","      <td>0.819638</td>\n","      <td>0.608647</td>\n","      <td>0.569502</td>\n","      <td>0.653571</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:02:46,924]\u001b[0m Trial 28 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 3495\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1166' max='3495' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1165/3495 01:20 < 02:40, 14.50 it/s, Epoch 1.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.813100</td>\n","      <td>0.732258</td>\n","      <td>0.676325</td>\n","      <td>0.582075</td>\n","      <td>0.597942</td>\n","      <td>0.573679</td>\n","      <td>0.588800</td>\n","      <td>0.555332</td>\n","      <td>0.626561</td>\n","      <td>0.799442</td>\n","      <td>0.849315</td>\n","      <td>0.755102</td>\n","      <td>0.357981</td>\n","      <td>0.316390</td>\n","      <td>0.412162</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:04:11,809]\u001b[0m Trial 29 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 9318\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='9318' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [4657/9318 02:39 < 02:39, 29.20 it/s, Epoch 1.00/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.577400</td>\n","      <td>0.575310</td>\n","      <td>0.767976</td>\n","      <td>0.712640</td>\n","      <td>0.745415</td>\n","      <td>0.692148</td>\n","      <td>0.662464</td>\n","      <td>0.581489</td>\n","      <td>0.769640</td>\n","      <td>0.842837</td>\n","      <td>0.897445</td>\n","      <td>0.794494</td>\n","      <td>0.632619</td>\n","      <td>0.597510</td>\n","      <td>0.672112</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:06:55,913]\u001b[0m Trial 30 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='13978' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13975/23295 08:20 < 05:33, 27.93 it/s, Epoch 3.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.563900</td>\n","      <td>0.527737</td>\n","      <td>0.787293</td>\n","      <td>0.735721</td>\n","      <td>0.744133</td>\n","      <td>0.730886</td>\n","      <td>0.730221</td>\n","      <td>0.747485</td>\n","      <td>0.713737</td>\n","      <td>0.864767</td>\n","      <td>0.881896</td>\n","      <td>0.848291</td>\n","      <td>0.612176</td>\n","      <td>0.563278</td>\n","      <td>0.670370</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.440900</td>\n","      <td>0.546158</td>\n","      <td>0.801245</td>\n","      <td>0.759729</td>\n","      <td>0.756425</td>\n","      <td>0.766568</td>\n","      <td>0.765997</td>\n","      <td>0.824950</td>\n","      <td>0.714908</td>\n","      <td>0.870332</td>\n","      <td>0.858571</td>\n","      <td>0.882420</td>\n","      <td>0.642857</td>\n","      <td>0.616183</td>\n","      <td>0.671946</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.355400</td>\n","      <td>0.622417</td>\n","      <td>0.803821</td>\n","      <td>0.758604</td>\n","      <td>0.758650</td>\n","      <td>0.760804</td>\n","      <td>0.778311</td>\n","      <td>0.815895</td>\n","      <td>0.744037</td>\n","      <td>0.875231</td>\n","      <td>0.875231</td>\n","      <td>0.875231</td>\n","      <td>0.622271</td>\n","      <td>0.591286</td>\n","      <td>0.656682</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-4659\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-4659/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-4659/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-9318\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-9318/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-31/checkpoint-9318/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:15:20,865]\u001b[0m Trial 31 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/23295 02:41 < 10:46, 28.81 it/s, Epoch 1.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.614200</td>\n","      <td>0.575101</td>\n","      <td>0.768191</td>\n","      <td>0.715590</td>\n","      <td>0.722463</td>\n","      <td>0.714545</td>\n","      <td>0.706831</td>\n","      <td>0.749497</td>\n","      <td>0.668761</td>\n","      <td>0.848407</td>\n","      <td>0.857830</td>\n","      <td>0.839189</td>\n","      <td>0.591533</td>\n","      <td>0.536307</td>\n","      <td>0.659439</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:18:07,251]\u001b[0m Trial 32 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/23295 02:41 < 10:47, 28.76 it/s, Epoch 1.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.569200</td>\n","      <td>0.541281</td>\n","      <td>0.779137</td>\n","      <td>0.719038</td>\n","      <td>0.747694</td>\n","      <td>0.714914</td>\n","      <td>0.717241</td>\n","      <td>0.784708</td>\n","      <td>0.660457</td>\n","      <td>0.859555</td>\n","      <td>0.885968</td>\n","      <td>0.834670</td>\n","      <td>0.580317</td>\n","      <td>0.474066</td>\n","      <td>0.747954</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:20:53,990]\u001b[0m Trial 33 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 11650\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='11650' max='11650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [11650/11650 09:14, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.587900</td>\n","      <td>0.533245</td>\n","      <td>0.786649</td>\n","      <td>0.731903</td>\n","      <td>0.748940</td>\n","      <td>0.726506</td>\n","      <td>0.728225</td>\n","      <td>0.769618</td>\n","      <td>0.691057</td>\n","      <td>0.863890</td>\n","      <td>0.887079</td>\n","      <td>0.841883</td>\n","      <td>0.603593</td>\n","      <td>0.522822</td>\n","      <td>0.713881</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.453100</td>\n","      <td>0.505583</td>\n","      <td>0.798455</td>\n","      <td>0.750964</td>\n","      <td>0.757638</td>\n","      <td>0.747484</td>\n","      <td>0.750977</td>\n","      <td>0.773642</td>\n","      <td>0.729602</td>\n","      <td>0.870691</td>\n","      <td>0.883747</td>\n","      <td>0.858016</td>\n","      <td>0.631226</td>\n","      <td>0.585062</td>\n","      <td>0.685298</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.311400</td>\n","      <td>0.553460</td>\n","      <td>0.810474</td>\n","      <td>0.764521</td>\n","      <td>0.777580</td>\n","      <td>0.757386</td>\n","      <td>0.789370</td>\n","      <td>0.806841</td>\n","      <td>0.772640</td>\n","      <td>0.874797</td>\n","      <td>0.898926</td>\n","      <td>0.851930</td>\n","      <td>0.629395</td>\n","      <td>0.566390</td>\n","      <td>0.708171</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.203400</td>\n","      <td>0.706059</td>\n","      <td>0.807469</td>\n","      <td>0.766893</td>\n","      <td>0.764222</td>\n","      <td>0.769900</td>\n","      <td>0.786837</td>\n","      <td>0.805835</td>\n","      <td>0.768714</td>\n","      <td>0.875722</td>\n","      <td>0.870048</td>\n","      <td>0.881470</td>\n","      <td>0.638120</td>\n","      <td>0.633817</td>\n","      <td>0.642482</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.132600</td>\n","      <td>0.925541</td>\n","      <td>0.800386</td>\n","      <td>0.758649</td>\n","      <td>0.754455</td>\n","      <td>0.763965</td>\n","      <td>0.779008</td>\n","      <td>0.813883</td>\n","      <td>0.746999</td>\n","      <td>0.870623</td>\n","      <td>0.860792</td>\n","      <td>0.880682</td>\n","      <td>0.626316</td>\n","      <td>0.617220</td>\n","      <td>0.635684</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-2330\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-2330/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-2330/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-4660\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-4660/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-4660/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-6990\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-6990/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-6990/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-9320\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-9320/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-9320/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","Saving model checkpoint to /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-11650\n","Configuration saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-11650/config.json\n","Model weights saved in /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-11650/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from /content/drive/MyDrive/Dissertation/disbert_hate_task/hyper/results/run-34/checkpoint-4660 (score: 0.5055827498435974).\n","\u001b[32m[I 2022-02-13 15:30:09,375]\u001b[0m Trial 34 finished with value: 9.908663181760964 and parameters: {'learning_rate': 3.44028696540716e-05, 'num_train_epochs': 5, 'seed': 16, 'warmup_steps': 90, 'weight_decay': 0.135460063536684, 'per_device_train_batch_size': 16}. Best is trial 10 with value: 10.05241054638603.\u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18636\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='18636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/18636 02:42 < 08:07, 28.67 it/s, Epoch 1.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.623600</td>\n","      <td>0.584020</td>\n","      <td>0.770337</td>\n","      <td>0.724613</td>\n","      <td>0.725182</td>\n","      <td>0.724691</td>\n","      <td>0.709234</td>\n","      <td>0.726358</td>\n","      <td>0.692898</td>\n","      <td>0.845685</td>\n","      <td>0.847094</td>\n","      <td>0.844280</td>\n","      <td>0.618920</td>\n","      <td>0.600622</td>\n","      <td>0.638368</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:32:56,565]\u001b[0m Trial 35 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 23295\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='23295' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/23295 02:44 < 10:58, 28.31 it/s, Epoch 1.00/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.580900</td>\n","      <td>0.548740</td>\n","      <td>0.772483</td>\n","      <td>0.721360</td>\n","      <td>0.723407</td>\n","      <td>0.725828</td>\n","      <td>0.712707</td>\n","      <td>0.778672</td>\n","      <td>0.657046</td>\n","      <td>0.854644</td>\n","      <td>0.850056</td>\n","      <td>0.859281</td>\n","      <td>0.596729</td>\n","      <td>0.548755</td>\n","      <td>0.653894</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:35:45,829]\u001b[0m Trial 36 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 5\n","  Instantaneous batch size per device = 64\n","  Total train batch size (w. parallel, distributed & accumulation) = 64\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2915\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='584' max='2915' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 584/2915 01:10 < 04:44, 8.20 it/s, Epoch 1/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.734000</td>\n","      <td>0.561537</td>\n","      <td>0.776776</td>\n","      <td>0.719744</td>\n","      <td>0.743230</td>\n","      <td>0.705750</td>\n","      <td>0.712315</td>\n","      <td>0.701207</td>\n","      <td>0.723780</td>\n","      <td>0.853667</td>\n","      <td>0.896335</td>\n","      <td>0.814877</td>\n","      <td>0.593250</td>\n","      <td>0.519710</td>\n","      <td>0.691034</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:37:01,643]\u001b[0m Trial 37 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 18636\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4660' max='18636' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4657/18636 02:43 < 08:09, 28.54 it/s, Epoch 1.00/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.558300</td>\n","      <td>0.560672</td>\n","      <td>0.771625</td>\n","      <td>0.721089</td>\n","      <td>0.723260</td>\n","      <td>0.727613</td>\n","      <td>0.714286</td>\n","      <td>0.794769</td>\n","      <td>0.648604</td>\n","      <td>0.853189</td>\n","      <td>0.844502</td>\n","      <td>0.862056</td>\n","      <td>0.595793</td>\n","      <td>0.543568</td>\n","      <td>0.659119</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:39:49,473]\u001b[0m Trial 38 pruned. \u001b[0m\n","Trial:\n","loading configuration file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForQuestionAnswering\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.16.2\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad/pytorch_model.bin\n","Some weights of the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad were not used when initializing DistilBertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/Dissertation/disbert_hate_ml/results/best_model_squad and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, input_ids_bert, attention_mask_bert, sentence.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 37265\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 6990\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2331' max='6990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2331/6990 01:42 < 03:25, 22.72 it/s, Epoch 1/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>Hate F1</th>\n","      <th>Hate Recall</th>\n","      <th>Hate Precision</th>\n","      <th>Offensive F1</th>\n","      <th>Offensive Recall</th>\n","      <th>Offensive Precision</th>\n","      <th>Normal F1</th>\n","      <th>Normal Recall</th>\n","      <th>Normal Precision</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.695800</td>\n","      <td>0.650311</td>\n","      <td>0.735780</td>\n","      <td>0.678781</td>\n","      <td>0.683367</td>\n","      <td>0.675231</td>\n","      <td>0.648791</td>\n","      <td>0.620724</td>\n","      <td>0.679515</td>\n","      <td>0.830126</td>\n","      <td>0.838578</td>\n","      <td>0.821843</td>\n","      <td>0.557427</td>\n","      <td>0.566390</td>\n","      <td>0.548744</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: token_type_ids_bert, __index_level_0__, input_ids_bert, attention_mask_bert, sentence.\n","***** Running Evaluation *****\n","  Num examples = 4659\n","  Batch size = 16\n","\u001b[32m[I 2022-02-13 15:41:36,670]\u001b[0m Trial 39 pruned. \u001b[0m\n"]}]},{"cell_type":"code","source":["best_trial"],"metadata":{"id":"KXAQOU-_79fl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644766897027,"user_tz":0,"elapsed":6,"user":{"displayName":"Aidan McGowran","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"14843729866646891917"}},"outputId":"f3d2d197-d6a9-4c5c-9da4-87e6b902b4d9"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BestRun(run_id='10', objective=10.05241054638603, hyperparameters={'learning_rate': 8.584684132528283e-05, 'num_train_epochs': 4, 'seed': 32, 'warmup_steps': 50, 'weight_decay': 0.10919253165395515, 'per_device_train_batch_size': 64})"]},"metadata":{},"execution_count":14}]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Distilbert Experiment Intermediate Task Transfer HyperParam Search.ipynb","provenance":[{"file_id":"11zAh-a_KPmflomNvD1V-XdXzenQf9Q16","timestamp":1644755707832},{"file_id":"1a2My2hhUmHWnDPSOkajnyi9ZeBYHGBAE","timestamp":1644482917164},{"file_id":"1P3VgwZUmhtAsU0t-TfIzNCI19KZV3I_q","timestamp":1643491583972},{"file_id":"1-rZqxPTOtm21puGsZKK_FiKXeyCg7py7","timestamp":1643483337240},{"file_id":"15wylWRQJ1vxlP58OhBHUnY5fdagThMvJ","timestamp":1643406846637},{"file_id":"1B35hajBJY3fRQV7LSjqoqQ3xjdcyCzE3","timestamp":1642365829686},{"file_id":"1b0lCW2Cj6AULiE-Axh2OeZwURHR6pfcD","timestamp":1642333651961},{"file_id":"1RAjFaq-k9CLJQP5eO668UXLc0q-wjuve","timestamp":1642280213414},{"file_id":"1LzZr7GRN1SwVrNyjVX5t5PpCvio8w_kE","timestamp":1642109182548},{"file_id":"1t-qzEyxZtGn_Cnlfg2WtN-0dlgUmxfzx","timestamp":1641764210714},{"file_id":"14bEU8hCGPF8cVUA_BonJQA6Brv89DKUm","timestamp":1641252935713},{"file_id":"18Il3CpGf89tF1Z10iYX8OdfeHxxAX1Ui","timestamp":1639243141142},{"file_id":"1SHCgoRQVGtl9OiWEJGK3JgKkuxxSPy_5","timestamp":1639231968300},{"file_id":"1D-3yvF0nGnaWEZGxQj21R6fsGnyv5a5g","timestamp":1637526185003},{"file_id":"1glXr2JglKPUpN_3AGyk3GjfCtZSDNfsZ","timestamp":1637408594851},{"file_id":"1rc5hX8PBIv7GKvlxLQ35Vwf2jxLsv9f4","timestamp":1634501666668}],"background_execution":"on","mount_file_id":"1rRPr2DENFy0pCRMIFdatZW-MljEG_50P","authorship_tag":"ABX9TyPc+qsePsEorbpGnEyndLL+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}